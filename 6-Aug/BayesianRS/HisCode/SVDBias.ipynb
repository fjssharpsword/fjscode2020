{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape is:2547452 rows and 3 columns\n",
      "Testset shape is:1021600 rows and 3 columns\n",
      "Dataset Statistics: Interaction = 2547452, User = 10216, Item = 96324, Sparsity = 0.0026\n",
      "  K               HR@10             NDCG@10\n",
      "  8            0.196359            0.150287\n",
      " 16            0.178837            0.135577\n",
      " 32            0.162686            0.126575\n",
      " 64            0.152408            0.119283\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.15\n",
    "@function: Implementing SVDBias with surprise lirbray\n",
    "           Dataset: KnowledgeBase-cc \n",
    "           Evaluating by hitradio,ndcg\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import surprise as sp\n",
    "\n",
    "#1. loading the KnowledgeBase dataset.\n",
    "trainset = pd.read_csv(\"/data/fjsdata/ctKngBase/kbcc_trainset.csv\", sep='|', low_memory=False)\n",
    "#trainset['num']=trainset['num'].apply(lambda x: 1 if float(x)>0.0 else 0)\n",
    "print ('Trainset shape is:%d rows and %d columns'%(trainset.shape[0],trainset.shape[1]))\n",
    "#testset includes 100 items for every user, one item is positive and other 99 is negtive items.\n",
    "testset = pd.read_csv(\"/data/fjsdata/ctKngBase/kbcc_testset.csv\", sep='|', low_memory=False)\n",
    "#testset['num']=testset['num'].apply(lambda x: 1 if float(x)>0.0 else 0)\n",
    "print ('Testset shape is:%d rows and %d columns'%(testset.shape[0],testset.shape[1]))\n",
    "csrNum = trainset['csr'].max()+1\n",
    "keNum = trainset['ke'].max()+1\n",
    "print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \n",
    "      (trainset.shape[0], csrNum, keNum, trainset.shape[0]/(csrNum*keNum)) )\n",
    "\n",
    "#2. Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 200))\n",
    "spdata = sp.Dataset.load_from_df(trainset,reader)\n",
    "trainset = spdata.build_full_trainset()\n",
    "testset = np.array(testset).tolist()\n",
    "\n",
    "#3.training and evaluating \n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "print (\"%3s%20s%20s\" % ('K','HR@10', 'NDCG@10'))\n",
    "for K in [8,16,32,64]:#iterations epoches\n",
    "    algo = sp.SVD(n_factors=K, n_epochs=20, lr_all=0.001, reg_all=0.01 )#NMF,SVDpp\n",
    "    algo.fit(trainset)\n",
    "    #print (algo.predict(str(1),str(1), r_ui=0, verbose=True)) \n",
    "    predictions = algo.test(testset)#testset include one positive and 99 negtive sample of every user.\n",
    "    user_iid_true_est = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_iid_true_est[uid].append((iid, true_r, est))\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for uid, iid_ratings in user_iid_true_est.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        #iid_ratings.sort(key=lambda x: x[2], reverse=True) #sorted by est\n",
    "        scorelist = []\n",
    "        positem = -1\n",
    "        for iid, ture_r, est in iid_ratings:\n",
    "            if positem == -1: positem=iid #one positive item in first\n",
    "            scorelist.append([iid,est])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "        hr = getHitRatio(ranklist, positem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, positem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"%3d%20.6f%20.6f\" % (K, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  K               HR@10             NDCG@10\n",
      "  8            0.496965            0.496965\n",
      " 16            0.500299            0.500299\n",
      " 32            0.495533            0.495533\n",
      " 64            0.497599            0.497599\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.15\n",
    "@function: Implementing SVDBias with surprise lirbray\n",
    "           Dataset: Pinterest-20\n",
    "           Evaluating by hitradio,ndcg\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import surprise as sp\n",
    "\n",
    "#1. loading the dataset.\n",
    "def load_dataset():\n",
    "    train_data = pd.read_csv(\"/data/fjsdata/ctKngBase/ml/pinterest-20.train.rating\", \\\n",
    "                             sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                             usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2 :np.float})\n",
    "    \n",
    "    test_data = []\n",
    "    with open(\"/data/fjsdata/ctKngBase/ml/pinterest-20.test.negative\", 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            test_data.append([u, eval(arr[0])[1], 1])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                test_data.append([u, int(i), 0]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data,test_set = load_dataset()\n",
    "#2. Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(train_data,reader)\n",
    "trainset = spdata.build_full_trainset()\n",
    "#testset = np.array(testset).tolist()\n",
    "\n",
    "#3.training and evaluating \n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "print (\"%3s%20s%20s\" % ('K','HR@10', 'NDCG@10'))\n",
    "for K in [8,16,32,64]:#iterations epoches\n",
    "    algo = sp.SVD(n_factors=K, n_epochs=20, lr_all=0.001, reg_all=0.01 )#NMF,SVDpp\n",
    "    algo.fit(trainset)\n",
    "    #print (algo.predict(str(1),str(1), r_ui=0, verbose=True)) \n",
    "    predictions = algo.test(test_set)#testset include one positive and 99 negtive sample of every user.\n",
    "    user_iid_true_est = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_iid_true_est[uid].append((iid, true_r, est))\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for uid, iid_ratings in user_iid_true_est.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        #iid_ratings.sort(key=lambda x: x[2], reverse=True) #sorted by est\n",
    "        scorelist = []\n",
    "        positem = -1\n",
    "        for iid, ture_r, est in iid_ratings:\n",
    "            if positem == -1: positem=iid #one positive item in first\n",
    "            scorelist.append([iid,est])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "        hr = getHitRatio(ranklist, positem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, positem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"%3d%20.6f%20.6f\" % (K, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  K               HR@10             NDCG@10\n",
      "  8            0.260430            0.127373\n",
      " 16            0.260430            0.127286\n",
      " 32            0.260927            0.127980\n",
      " 64            0.259603            0.126590\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.15\n",
    "@function: Implementing SVDBias with surprise lirbray\n",
    "           Dataset: Movielen-1m\n",
    "           Evaluating by hitradio,ndcg\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import surprise as sp\n",
    "\n",
    "#1. loading the dataset.\n",
    "def load_dataset():\n",
    "    train_data = pd.read_csv(\"/data/fjsdata/ctKngBase/ml/ml-1m.train.rating\", \\\n",
    "                             sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                             usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2 :np.float})\n",
    "    \n",
    "    test_data = []\n",
    "    with open(\"/data/fjsdata/ctKngBase/ml/ml-1m.test.negative\", 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            test_data.append([u, eval(arr[0])[1], 1])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                test_data.append([u, int(i), 0]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data,test_set = load_dataset()\n",
    "#2. Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 5))\n",
    "spdata = sp.Dataset.load_from_df(train_data,reader)\n",
    "trainset = spdata.build_full_trainset()\n",
    "#testset = np.array(testset).tolist()\n",
    "\n",
    "#3.training and evaluating \n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "print (\"%3s%20s%20s\" % ('K','HR@10', 'NDCG@10'))\n",
    "for K in [8,16,32,64]:#iterations epoches\n",
    "    algo = sp.SVD(n_factors=K, n_epochs=20, lr_all=0.001, reg_all=0.01 )#NMF,SVDpp\n",
    "    algo.fit(trainset)\n",
    "    #print (algo.predict(str(1),str(1), r_ui=0, verbose=True)) \n",
    "    predictions = algo.test(test_set)#testset include one positive and 99 negtive sample of every user.\n",
    "    user_iid_true_est = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_iid_true_est[uid].append((iid, true_r, est))\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for uid, iid_ratings in user_iid_true_est.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        #iid_ratings.sort(key=lambda x: x[2], reverse=True) #sorted by est\n",
    "        scorelist = []\n",
    "        positem = -1\n",
    "        for iid, ture_r, est in iid_ratings:\n",
    "            if positem == -1: positem=iid #one positive item in first\n",
    "            scorelist.append([iid,est])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "        hr = getHitRatio(ranklist, positem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, positem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"%3d%20.6f%20.6f\" % (K, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  K               HR@10             NDCG@10\n",
      "  8            0.963097            0.678305\n",
      " 16            0.959769            0.677092\n",
      " 32            0.961042            0.677676\n",
      " 64            0.958888            0.674344\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.13\n",
    "@function: Implementing SVDBias with surprise lirbray\n",
    "           Dataset: Pinterest-20\n",
    "           Evaluating by hitradio,ndcg\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import surprise as sp\n",
    "\n",
    "#1. loading the dataset.\n",
    "def load_dataset():\n",
    "    train_data = pd.read_csv(\"/data/fjsdata/ctKngBase/ml/pinterest-20.train.rating\", \\\n",
    "                             sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                             usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 1:np.float})\n",
    "    #user_num = train_data['user'].max() + 1\n",
    "    #item_num = train_data['item'].max() + 1 \n",
    "    #train_data = train_data.values.tolist()\n",
    "    \n",
    "    test_data = []\n",
    "    with open(\"/data/fjsdata/ctKngBase/ml/pinterest-20.test.negative\", 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            test_data.append([u, eval(arr[0])[1], 1])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                test_data.append([u, int(i), 0]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return train_data, test_data\n",
    "train_data,test_set = load_dataset()\n",
    "#2. Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(train_data,reader)\n",
    "trainset = spdata.build_full_trainset()\n",
    "#testset = np.array(testset).tolist()\n",
    "\n",
    "#3.training and evaluating \n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "print (\"%3s%20s%20s\" % ('K','HR@10', 'NDCG@10'))\n",
    "for K in [8,16,32,64]:#iterations epoches\n",
    "    algo = sp.SVD(n_factors=K, n_epochs=20, lr_all=0.001, reg_all=0.01 )#NMF,SVDpp\n",
    "    algo.fit(trainset)\n",
    "    #print (algo.predict(str(1),str(1), r_ui=0, verbose=True)) \n",
    "    predictions = algo.test(testset)#testset include one positive and 99 negtive sample of every user.\n",
    "    user_iid_true_est = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_iid_true_est[uid].append((iid, true_r, est))\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for uid, iid_ratings in user_iid_true_est.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        #iid_ratings.sort(key=lambda x: x[2], reverse=True) #sorted by est\n",
    "        scorelist = []\n",
    "        positem = -1\n",
    "        for iid, ture_r, est in iid_ratings:\n",
    "            #if positem == -1: positem=iid #one positive item in first\n",
    "            #scorelist.append([iid,est])\n",
    "            if (ture_r+1)>0.0:#one positive item\n",
    "                scorelist.append([iid,est])\n",
    "                positem = iid \n",
    "            else:# 99 negative items\n",
    "                scorelist.append([iid,est])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "        hr = getHitRatio(ranklist, positem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, positem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"%3d%20.6f%20.6f\" % (K, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  K               HR@10             NDCG@10\n",
      "  8            1.000000            1.000000\n",
      " 16            1.000000            1.000000\n",
      " 32            1.000000            1.000000\n",
      " 64            1.000000            1.000000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.13\n",
    "@function: Implementing SVDBias with surprise lirbray\n",
    "           Dataset: Movielen-1m\n",
    "           Evaluating by hitradio,ndcg\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import surprise as sp\n",
    "\n",
    "#1. loading the dataset.\n",
    "def load_dataset():\n",
    "    train_data = pd.read_csv(\"/data/fjsdata/ctKngBase/ml/ml-1m.train.rating\", \\\n",
    "                             sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                             usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 1:np.float})\n",
    "    #user_num = train_data['user'].max() + 1\n",
    "    #item_num = train_data['item'].max() + 1 \n",
    "    #train_data = train_data.values.tolist()\n",
    "    \n",
    "    test_data = []\n",
    "    with open(\"/data/fjsdata/ctKngBase/ml/ml-1m.test.negative\", 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            test_data.append([u, eval(arr[0])[1], 1])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                test_data.append([u, int(i), 0]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return train_data, test_data\n",
    "train_data,test_set = load_dataset()\n",
    "#2. Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(train_data,reader)\n",
    "trainset = spdata.build_full_trainset()\n",
    "#testset = np.array(testset).tolist()\n",
    "\n",
    "#3.training and evaluating \n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "print (\"%3s%20s%20s\" % ('K','HR@10', 'NDCG@10'))\n",
    "for K in [8,16,32,64]:#iterations epoches\n",
    "    algo = sp.SVD(n_factors=K, n_epochs=20, lr_all=0.001, reg_all=0.01 )#NMF,SVDpp\n",
    "    algo.fit(trainset)\n",
    "    #print (algo.predict(str(1),str(1), r_ui=0, verbose=True)) \n",
    "    predictions = algo.test(testset)#testset include one positive and 99 negtive sample of every user.\n",
    "    user_iid_true_est = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_iid_true_est[uid].append((iid, true_r, est))\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for uid, iid_ratings in user_iid_true_est.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        #iid_ratings.sort(key=lambda x: x[2], reverse=True) #sorted by est\n",
    "        scorelist = []\n",
    "        positem = -1\n",
    "        for iid, ture_r, est in iid_ratings:\n",
    "            #if positem == -1: positem=iid #one positive item in first\n",
    "            #scorelist.append([iid,est])\n",
    "            if (ture_r+1)>0.0:#one positive item\n",
    "                scorelist.append([iid,est])\n",
    "                positem = iid \n",
    "            else:# 99 negative items\n",
    "                scorelist.append([iid,est])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "        hr = getHitRatio(ranklist, positem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, positem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"%3d%20.6f%20.6f\" % (K, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape is:2547452 rows and 3 columns\n",
      "Testset shape is:1021600 rows and 3 columns\n",
      "Dataset Statistics: Interaction = 2547452, User = 10216, Item = 96324, Sparsity = 0.0026\n",
      "  K               HR@10             NDCG@10\n",
      "  8            0.500098            0.500098\n",
      " 16            0.490407            0.490407\n",
      " 32            0.501077            0.501077\n",
      " 64            0.498727            0.498727\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.13\n",
    "@function: Implementing SVDBias with surprise lirbray\n",
    "           Dataset: KnowledgeBase-cc \n",
    "           Evaluating by hitradio,ndcg\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import surprise as sp\n",
    "\n",
    "#1. loading the KnowledgeBase dataset.\n",
    "trainset = pd.read_csv(\"/data/fjsdata/ctKngBase/kbcc_trainset.csv\", sep='|', low_memory=False)\n",
    "trainset['num']=trainset['num'].apply(lambda x: 1 if float(x)>0.0 else 0)\n",
    "print ('Trainset shape is:%d rows and %d columns'%(trainset.shape[0],trainset.shape[1]))\n",
    "#testset includes 100 items for every user, one item is positive and other 99 is negtive items.\n",
    "testset = pd.read_csv(\"/data/fjsdata/ctKngBase/kbcc_testset.csv\", sep='|', low_memory=False)\n",
    "testset['num']=testset['num'].apply(lambda x: 1 if float(x)>0.0 else 0)\n",
    "print ('Testset shape is:%d rows and %d columns'%(testset.shape[0],testset.shape[1]))\n",
    "csrNum = trainset['csr'].max()+1\n",
    "keNum = trainset['ke'].max()+1\n",
    "print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \n",
    "      (trainset.shape[0], csrNum, keNum, trainset.shape[0]/(csrNum*keNum)) )\n",
    "\n",
    "#2. Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(trainset,reader)\n",
    "trainset = spdata.build_full_trainset()\n",
    "testset = np.array(testset).tolist()\n",
    "\n",
    "#3.training and evaluating \n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "print (\"%3s%20s%20s\" % ('K','HR@10', 'NDCG@10'))\n",
    "for K in [8,16,32,64]:#iterations epoches\n",
    "    algo = sp.SVD(n_factors=K, n_epochs=20, lr_all=0.001, reg_all=0.01 )#NMF,SVDpp\n",
    "    algo.fit(trainset)\n",
    "    #print (algo.predict(str(1),str(1), r_ui=0, verbose=True)) \n",
    "    predictions = algo.test(testset)#testset include one positive and 99 negtive sample of every user.\n",
    "    user_iid_true_est = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_iid_true_est[uid].append((iid, true_r, est))\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for uid, iid_ratings in user_iid_true_est.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        #iid_ratings.sort(key=lambda x: x[2], reverse=True) #sorted by est\n",
    "        scorelist = []\n",
    "        positem = -1\n",
    "        for iid, ture_r, est in iid_ratings:\n",
    "            if positem == -1: positem=iid #one positive item in first\n",
    "            scorelist.append([iid,est])\n",
    "            '''\n",
    "            if (ture_r+1)>0.0:#one positive item\n",
    "                scorelist.append([iid,est])\n",
    "                positem = iid \n",
    "            else:# 99 negative items\n",
    "                scorelist.append([iid,est])\n",
    "            '''\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "        hr = getHitRatio(ranklist, positem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, positem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"%3d%20.6f%20.6f\" % (K, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  K               HR@10             NDCG@10\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.12\n",
    "@function: Implementation: SVDBias \n",
    "           Datatset: Movielen-1m \n",
    "           Evaluation: hitradio,ndcg\n",
    "'''\n",
    "import pandas as pd\n",
    "import minpy.numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "#1.Loading the  MovienLen dataset, ml-1m\n",
    "def load_rating_file_as_list(filename):\n",
    "    ratingList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item = int(arr[0]), int(arr[1])\n",
    "            ratingList.append([user, item])\n",
    "            line = f.readline()\n",
    "    return ratingList\n",
    "def load_negative_file_as_list(filename):\n",
    "    negativeList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            negatives = []\n",
    "            for x in arr[1: ]:\n",
    "                negatives.append(int(x))\n",
    "            negativeList.append(negatives)\n",
    "            line = f.readline()\n",
    "    return negativeList\n",
    "def load_rating_file_as_matrix(filename):\n",
    "    #Read .rating file and Return dok matrix.\n",
    "    #The first line of .rating file is: num_users\\t num_items\n",
    "    # Get number of users and items\n",
    "    num_users, num_items = 0, 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            u, i = int(arr[0]), int(arr[1])\n",
    "            num_users = max(num_users, u)\n",
    "            num_items = max(num_items, i)\n",
    "            line = f.readline()\n",
    "    # Construct matrix\n",
    "    #mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "    mat = np.zeros((num_users+1, num_items+1))\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            if (rating > 0.0): mat[user, item] = 1.0\n",
    "            line = f.readline()    \n",
    "    return mat\n",
    "trainMatrix = load_rating_file_as_matrix(\"/data/fjsdata/ctKngBase/ml/ml-1m.train.rating\")\n",
    "testRatings = load_rating_file_as_list(\"/data/fjsdata/ctKngBase/ml/ml-1m.test.rating\")\n",
    "testNegatives = load_negative_file_as_list(\"/data/fjsdata/ctKngBase/ml/ml-1m.test.negative\")\n",
    "print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "      (len(trainMatrix[np.where(trainMatrix!= 0)]),trainMatrix.shape[0],trainMatrix.shape[1],\\\n",
    "       len(trainMatrix[np.where(trainMatrix!= 0)])/(trainMatrix.shape[0]*trainMatrix.shape[1]) ))\n",
    "\n",
    "#2. SVDBias class\n",
    "class SVDBias():\n",
    "    \n",
    "    def __init__(self, R, num_ng=4):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - num_ng (int)  : number of negative items\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.num_ng = num_ng\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        '''\n",
    "        #smapling the negative items\n",
    "        for x in self.samples:\n",
    "            u = x[0]\n",
    "            for t in range(self.num_ng):\n",
    "                j = np.random.randint(self.num_items)\n",
    "                #while (u, j) in self.R:\n",
    "                while self.R[u, j] > 0:\n",
    "                    j = np.random.randint(self.num_items)\n",
    "                self.samples.append([u, j, 0])\n",
    "        '''\n",
    "\n",
    "    def train(self, K, alpha=0.001, beta=0.01, epochs=20):\n",
    "        '''\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        - K (int)       : number of latent dimensions\n",
    "        -epochs(int)    : number of iterations\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "               \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.epochs):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            #if (i+1) % 10 == 0:\n",
    "            #    mse = self.mse()\n",
    "            #    print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)\n",
    "    \n",
    "#3. Training and Evaluating\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "print (\"%3s%20s%20s\" % ('K','HR@10', 'NDCG@10'))\n",
    "mdl = SVDBias(R=trainMatrix, num_ng=4)# K is latent factors\n",
    "for K in [8,16,32,64]:#latent factors\n",
    "    nR = mdl.train(K=K, alpha=0.001, beta=0.01, epochs=20)\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for u, i in testRatings:\n",
    "        scorelist= [ [ni,nR[u,ni]] for ni in testNegatives[u]]\n",
    "        scorelist.append([i,nR[u,i]])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "        hr = getHitRatio(ranklist, i)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, i)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"%3d%20.6f%20.6f\" % (K, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 1408394, User = 55187, Item = 9916, Sparsity = 0.0026\n",
      "  K               HR@10             NDCG@10\n",
      "  8            0.092232            0.041091\n",
      " 16            0.094678            0.042397\n",
      " 32            0.094080            0.042738\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.12\n",
    "@function: Implementation: SVDBias \n",
    "           Datatset: Pinterest-20 \n",
    "           Evaluation: hitradio,ndcg\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "#1.Loading the  MovienLen dataset, ml-1m\n",
    "def load_rating_file_as_list(filename):\n",
    "    ratingList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item = int(arr[0]), int(arr[1])\n",
    "            ratingList.append([user, item])\n",
    "            line = f.readline()\n",
    "    return ratingList\n",
    "def load_negative_file_as_list(filename):\n",
    "    negativeList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            negatives = []\n",
    "            for x in arr[1: ]:\n",
    "                negatives.append(int(x))\n",
    "            negativeList.append(negatives)\n",
    "            line = f.readline()\n",
    "    return negativeList\n",
    "def load_rating_file_as_matrix(filename):\n",
    "    #Read .rating file and Return dok matrix.\n",
    "    #The first line of .rating file is: num_users\\t num_items\n",
    "    # Get number of users and items\n",
    "    num_users, num_items = 0, 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            u, i = int(arr[0]), int(arr[1])\n",
    "            num_users = max(num_users, u)\n",
    "            num_items = max(num_items, i)\n",
    "            line = f.readline()\n",
    "    # Construct matrix\n",
    "    #mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "    mat = np.zeros((num_users+1, num_items+1))\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            if (rating > 0.0): mat[user, item] = 1.0\n",
    "            line = f.readline()    \n",
    "    return mat\n",
    "trainMatrix = load_rating_file_as_matrix(\"/data/fjsdata/ctKngBase/ml/pinterest-20.train.rating\")\n",
    "testRatings = load_rating_file_as_list(\"/data/fjsdata/ctKngBase/ml/pinterest-20.test.rating\")\n",
    "testNegatives = load_negative_file_as_list(\"/data/fjsdata/ctKngBase/ml/pinterest-20.test.negative\")\n",
    "print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "      (len(trainMatrix[np.where(trainMatrix!= 0)]),trainMatrix.shape[0],trainMatrix.shape[1],\\\n",
    "       len(trainMatrix[np.where(trainMatrix!= 0)])/(trainMatrix.shape[0]*trainMatrix.shape[1]) ))\n",
    "\n",
    "#2. SVDBias class\n",
    "class SVDBias():\n",
    "    \n",
    "    def __init__(self, R, K, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            #if (i+1) % 10 == 0:\n",
    "            #    mse = self.mse()\n",
    "            #    print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)\n",
    "    \n",
    "#3. Training and Evaluating\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "print (\"%3s%20s%20s\" % ('K','HR@10', 'NDCG@10'))\n",
    "for K in [8,16,32,64]:#latent factors\n",
    "    mdl = SVDBias(R=trainMatrix, K=K, alpha=0.001, beta=0.01, iterations=20)# K is latent factors\n",
    "    nR = mdl.train()\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for u, i in testRatings:\n",
    "        scorelist= [ [ni,nR[u,ni]] for ni in testNegatives[u]]\n",
    "        scorelist.append([i,nR[u,i]])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "        hr = getHitRatio(ranklist, i)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, i)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"%3d%20.6f%20.6f\" % (K, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "  K          Iterations                RMSE            HitRatio                NDCG\n",
      "  8                  20            0.958511            0.258113            0.126652\n",
      "  8                  50            0.944725            0.242715            0.120635\n",
      "  8                 100            0.916240            0.258940            0.135366\n",
      " 16                  20            0.957442            0.257616            0.126229\n",
      " 16                  50            0.942338            0.239735            0.118111\n",
      " 16                 100            0.915297            0.260265            0.137506\n",
      " 32                  20            0.957337            0.260762            0.126890\n",
      " 32                  50            0.944851            0.242715            0.117821\n",
      " 32                 100            0.908712            0.266391            0.140854\n",
      " 64                  20            0.957565            0.260927            0.127290\n",
      " 64                  50            0.946130            0.240894            0.116952\n",
      " 64                 100            0.908971            0.261258            0.137011\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.11\n",
    "@function: Implementation: SVDBias \n",
    "           Datatset: Movielen Dataset(ml-1m) \n",
    "           Evaluation: rmse,hitradio,ndcg\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "#1.Loading the  MovienLen dataset, ml-1m\n",
    "def load_rating_file_as_list(filename):\n",
    "    ratingList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            #if (rating > 0): rating=1.0\n",
    "            ratingList.append([user, item, rating])\n",
    "            line = f.readline()\n",
    "    return ratingList\n",
    "def load_negative_file_as_list(filename):\n",
    "    negativeList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            negatives = []\n",
    "            for x in arr[1: ]:\n",
    "                negatives.append(int(x))\n",
    "            negativeList.append(negatives)\n",
    "            line = f.readline()\n",
    "    return negativeList\n",
    "def load_rating_file_as_matrix(filename):\n",
    "    #Read .rating file and Return dok matrix.\n",
    "    #The first line of .rating file is: num_users\\t num_items\n",
    "    # Get number of users and items\n",
    "    num_users, num_items = 0, 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            u, i = int(arr[0]), int(arr[1])\n",
    "            num_users = max(num_users, u)\n",
    "            num_items = max(num_items, i)\n",
    "            line = f.readline()\n",
    "    # Construct matrix\n",
    "    #mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "    mat = np.zeros((num_users+1, num_items+1))\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            #if (rating > 0): mat[user, item] = 1.0\n",
    "            mat[user][item] = rating\n",
    "            line = f.readline()    \n",
    "    return mat\n",
    "trainMatrix = load_rating_file_as_matrix(\"/data/fjsdata/ctKngBase/ml/ml-1m.train.rating\")\n",
    "testRatings = load_rating_file_as_list(\"/data/fjsdata/ctKngBase/ml/ml-1m.test.rating\")\n",
    "testNegatives = load_negative_file_as_list(\"/data/fjsdata/ctKngBase/ml/ml-1m.test.negative\")\n",
    "print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "      (len(trainMatrix[np.where(trainMatrix!= 0)]),trainMatrix.shape[0],trainMatrix.shape[1],\\\n",
    "       len(trainMatrix[np.where(trainMatrix!= 0)])/(trainMatrix.shape[0]*trainMatrix.shape[1]) ))\n",
    "\n",
    "#2. SVDBias class\n",
    "class SVDBias():\n",
    "    \n",
    "    def __init__(self, R, K, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            #if (i+1) % 10 == 0:\n",
    "            #    mse = self.mse()\n",
    "            #    print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)\n",
    "    \n",
    "#3. Training and Evaluating\n",
    "def getMSE(TRating,PRating):\n",
    "    error = TRating-PRating\n",
    "    return error*error\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "print (\"%3s%20s%20s%20s%20s\" % ('K','Iterations','RMSE', 'HitRatio', 'NDCG'))\n",
    "for K in [8,16,32,64]:#latent factors\n",
    "    for iterations in [20,50,100]:#iterations epoches\n",
    "        mdl = SVDBias(R=trainMatrix, K=K, alpha=0.001, beta=0.01, iterations=iterations)# K is latent factors\n",
    "        nR = mdl.train()\n",
    "        mses = []\n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        for u, i, r in testRatings:\n",
    "            mses.append(getMSE(r,nR[u,i]))\n",
    "            scorelist= [ [ni,nR[u,ni]] for ni in testNegatives[u]]\n",
    "            scorelist.append([i,nR[u,i]])\n",
    "            map_item_score = {}\n",
    "            for item, rate in scorelist: #turn dict\n",
    "                map_item_score[item] = rate\n",
    "            ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "            hr = getHitRatio(ranklist, i)\n",
    "            hits.append(hr)\n",
    "            ndcg = getNDCG(ranklist, i)\n",
    "            ndcgs.append(ndcg)\n",
    "        rmse,hitratio,ndcg = math.sqrt(sum(mses) / len(mses)) ,np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print (\"%3d%20d%20.6f%20.6f%20.6f\" % (K, iterations, rmse, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 1408394, User = 55187, Item = 9916, Sparsity = 0.0026\n",
      "  K          Iterations                RMSE            HitRatio                NDCG\n",
      "  8                  20            0.038986            0.089623            0.040215\n",
      "  8                  50            0.033846            0.075598            0.033086\n",
      "  8                 100            0.027578            0.066827            0.028344\n",
      " 16                  20            0.014740            0.094533            0.042753\n",
      " 16                  50            0.013429            0.087140            0.038314\n",
      " 16                 100            0.011655            0.076214            0.033107\n",
      " 32                  20            0.005235            0.096309            0.043037\n",
      " 32                  50            0.004900            0.089441            0.039722\n",
      " 32                 100            0.004364            0.078406            0.034508\n",
      " 64                  20            0.001858            0.096581            0.043438\n",
      " 64                  50            0.001743            0.090474            0.039888\n",
      " 64                 100            0.001575            0.083407            0.036920\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.11\n",
    "@function: Implementation: SVDBias \n",
    "           Datatset: Pinterest (pinterest-20)\n",
    "           Evaluation: rmse,hitradio,ndcg\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "#1.Loading the  MovienLen dataset, ml-1m\n",
    "def load_rating_file_as_list(filename):\n",
    "    ratingList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            #if (rating > 0): rating=1.0\n",
    "            ratingList.append([user, item, rating])\n",
    "            line = f.readline()\n",
    "    return ratingList\n",
    "def load_negative_file_as_list(filename):\n",
    "    negativeList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            negatives = []\n",
    "            for x in arr[1: ]:\n",
    "                negatives.append(int(x))\n",
    "            negativeList.append(negatives)\n",
    "            line = f.readline()\n",
    "    return negativeList\n",
    "def load_rating_file_as_matrix(filename):\n",
    "    #Read .rating file and Return dok matrix.\n",
    "    #The first line of .rating file is: num_users\\t num_items\n",
    "    # Get number of users and items\n",
    "    num_users, num_items = 0, 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            u, i = int(arr[0]), int(arr[1])\n",
    "            num_users = max(num_users, u)\n",
    "            num_items = max(num_items, i)\n",
    "            line = f.readline()\n",
    "    # Construct matrix\n",
    "    #mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "    mat = np.zeros((num_users+1, num_items+1))\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            #if (rating > 0): mat[user, item] = 1.0\n",
    "            mat[user][item] = rating\n",
    "            line = f.readline()    \n",
    "    return mat\n",
    "trainMatrix = load_rating_file_as_matrix(\"/data/fjsdata/ctKngBase/ml/pinterest-20.train.rating\")\n",
    "testRatings = load_rating_file_as_list(\"/data/fjsdata/ctKngBase/ml/pinterest-20.test.rating\")\n",
    "testNegatives = load_negative_file_as_list(\"/data/fjsdata/ctKngBase/ml/pinterest-20.test.negative\")\n",
    "print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "      (len(trainMatrix[np.where(trainMatrix!= 0)]),trainMatrix.shape[0],trainMatrix.shape[1],\\\n",
    "       len(trainMatrix[np.where(trainMatrix!= 0)])/(trainMatrix.shape[0]*trainMatrix.shape[1]) ))\n",
    "\n",
    "#2. SVDBias class\n",
    "class SVDBias():\n",
    "    \n",
    "    def __init__(self, R, K, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            #if (i+1) % 10 == 0:\n",
    "            #    mse = self.mse()\n",
    "            #    print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)\n",
    "    \n",
    "#3. Training and Evaluating\n",
    "def getMSE(TRating,PRating):\n",
    "    error = TRating-PRating\n",
    "    return error*error\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "print (\"%3s%20s%20s%20s%20s\" % ('K','Iterations','RMSE', 'HitRatio', 'NDCG'))\n",
    "for K in [8,16,32,64]:#latent factors\n",
    "    for iterations in [20,50,100]:#iterations epoches\n",
    "        mdl = SVDBias(R=trainMatrix, K=K, alpha=0.001, beta=0.01, iterations=iterations)# K is latent factors\n",
    "        nR = mdl.train()\n",
    "        mses = []\n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        for u, i, r in testRatings:\n",
    "            mses.append(getMSE(r,nR[u,i]))\n",
    "            scorelist= [ [ni,nR[u,ni]] for ni in testNegatives[u]]\n",
    "            scorelist.append([i,nR[u,i]])\n",
    "            map_item_score = {}\n",
    "            for item, rate in scorelist: #turn dict\n",
    "                map_item_score[item] = rate\n",
    "            ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "            hr = getHitRatio(ranklist, i)\n",
    "            hits.append(hr)\n",
    "            ndcg = getNDCG(ranklist, i)\n",
    "            ndcgs.append(ndcg)\n",
    "        rmse,hitratio,ndcg = math.sqrt(sum(mses) / len(mses)) ,np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print (\"%3d%20d%20.6f%20.6f%20.6f\" % (K, iterations, rmse, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape is:2547452 rows and 3 columns\n",
      "Testset shape is:1021600 rows and 3 columns\n",
      "Dataset Statistics: Interaction = 2547452, User = 10216, Item = 96324, Sparsity = 0.0026\n",
      "  K          Iterations                RMSE            HitRatio                NDCG\n",
      "  2                   2            0.268543            0.077721            0.034852\n",
      "  2                  20            0.156521            0.073218            0.038080\n",
      "  2                  50            0.122179            0.101899            0.058513\n",
      "  2                 100            0.098171            0.126175            0.075000\n",
      " 50                   2            0.055590            0.309417            0.250908\n",
      " 50                  20            0.057026            0.385180            0.296333\n",
      " 50                  50            0.055933            0.431774            0.316457\n",
      " 50                 100            0.054602            0.462706            0.326995\n",
      "100                   2            0.055675            0.338391            0.281181\n",
      "100                  20            0.056875            0.412197            0.313898\n",
      "100                  50            0.055918            0.462901            0.332866\n",
      "100                 100            0.054668            0.487177            0.340003\n",
      "200                   2            0.055528            0.348962            0.292860\n",
      "200                  20            0.056895            0.423160            0.319722\n",
      "200                  50            0.055853            0.471222            0.337526\n",
      "200                 100            0.054718            0.493637            0.343393\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.10\n",
    "@function: Implementing SVDBias and Setting KnowledgeBase as baseline by rmse,hitradio,ndcg\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "#1. loading the KnowledgeBase dataset.\n",
    "trainset = pd.read_csv(\"/data/fjsdata/ctKngBase/trainset.csv\", sep='|', low_memory=False)\n",
    "print ('Trainset shape is:%d rows and %d columns'%(trainset.shape[0],trainset.shape[1]))\n",
    "#testset includes 100 items for every user, one item is positive and other 99 is negtive items.\n",
    "testset = pd.read_csv(\"/data/fjsdata/ctKngBase/testset.csv\", sep='|', low_memory=False)\n",
    "print ('Testset shape is:%d rows and %d columns'%(testset.shape[0],testset.shape[1]))\n",
    "csrNum = trainset['csr'].max()+1\n",
    "keNum = trainset['ke'].max()+1\n",
    "print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \n",
    "      (trainset.shape[0], csrNum, keNum, trainset.shape[0]/(csrNum*keNum)) )\n",
    "\n",
    "#2. SVDBias class\n",
    "class SVDBias():\n",
    "    \n",
    "    def __init__(self, R, K, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            #if (i+1) % 10 == 0:\n",
    "            #    mse = self.mse()\n",
    "            #    print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)\n",
    "        \n",
    "#3. training and evaluating \n",
    "def getMSE(TRating,PRating):\n",
    "    error = TRating-PRating\n",
    "    return error*error\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "R = np.zeros((csrNum, keNum))\n",
    "for index, row in trainset.iterrows(): \n",
    "    R[int(row['csr'])][int(row['ke'])] = float(row['num'])\n",
    "print (\"%3s%20s%20s%20s%20s\" % ('K','Iterations','RMSE', 'HitRatio', 'NDCG'))\n",
    "for K in [2,50,100,200]:#latent factors\n",
    "    for iterations in [2,20,50,100]:#iterations epoches\n",
    "        mdl = SVDBias(R=R, K=K, alpha=0.001, beta=0.01, iterations=iterations)# K is latent factors\n",
    "        nR = mdl.train()\n",
    "        mses = []\n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        list_csr = list(set(np.array(testset['csr']).tolist()))\n",
    "        for csr in list_csr:\n",
    "            csrset = testset[testset['csr']==csr]\n",
    "            scorelist = []\n",
    "            positem = 0\n",
    "            for u, i, r in np.array(csrset).tolist():   \n",
    "                if float(r)>0.0:#one positive item\n",
    "                    mses.append(getMSE(float(r),nR[int(u),int(i)]))\n",
    "                    scorelist.append([int(i),nR[int(u),int(i)]])\n",
    "                    positem = int(i) \n",
    "                else:# 99 negative items\n",
    "                    scorelist.append([int(i),nR[int(u),int(i)]])\n",
    "            map_item_score = {}\n",
    "            for item, rate in scorelist: #turn dict\n",
    "                map_item_score[item] = rate\n",
    "            ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "            hr = getHitRatio(ranklist, positem)\n",
    "            hits.append(hr)\n",
    "            ndcg = getNDCG(ranklist, positem)\n",
    "            ndcgs.append(ndcg)\n",
    "        rmse,hitratio,ndcg = math.sqrt(sum(mses) / len(mses)) ,np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print (\"%3d%20d%20.6f%20.6f%20.6f\" % (K, iterations, rmse, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; error = 0.4114\n",
      "Iteration: 20 ; error = 0.0391\n",
      "[[-1.19898169 -0.78560961]\n",
      " [-0.45425502 -1.14631534]\n",
      " [ 1.12139627  0.49643985]\n",
      " [ 0.61005958  0.58658564]\n",
      " [ 0.65656094 -0.7188364 ]]\n",
      "[[-1.07558661 -0.90516512]\n",
      " [-1.45245481  1.3316412 ]\n",
      " [ 0.290075   -1.12393254]\n",
      " [ 1.23244214  1.11879751]]\n",
      "[[4.98303832 2.99631132 3.90771005 1.01453877]\n",
      " [3.9922774  0.91805774 4.01290442 1.01257351]\n",
      " [1.00284911 1.00934179 2.81590131 4.98466882]\n",
      " [1.01578568 1.41662358 2.11080402 3.99987731]\n",
      " [3.54493065 1.00826738 4.98903792 3.99422334]]\n"
     ]
    }
   ],
   "source": [
    "#SVDBias Implementation\n",
    "#https://github.com/albertauyeung/matrix-factorization-in-python\n",
    "import numpy as np\n",
    "class IRTMF():\n",
    "    \n",
    "    def __init__(self, R, K, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "        \n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            mse = self.mse()\n",
    "            training_process.append((i, mse))\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return training_process\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)\n",
    "\n",
    "    \n",
    "R = np.array([\n",
    "    [5, 3, 0, 1],\n",
    "    [4, 0, 0, 1],\n",
    "    [1, 1, 0, 5],\n",
    "    [1, 0, 0, 4],\n",
    "    [0, 1, 5, 4],\n",
    "])\n",
    "# Perform training and obtain the user and item matrices \n",
    "mf = IRTMF(R, K=2, alpha=0.1, beta=0.01, iterations=20)\n",
    "training_process = mf.train()\n",
    "print(mf.P)\n",
    "print(mf.Q)\n",
    "print(mf.full_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  K               steps                RMSE\n",
      " 30                   2            2.132338\n",
      " 30                 100            0.292978\n",
      " 30                 200            0.339896\n",
      " 50                   2            2.940437\n",
      " 50                 100            0.681469\n",
      " 50                 200            0.294894\n",
      " 80                   2            2.717870\n",
      " 80                 100            0.644332\n",
      " 80                 200            0.363208\n",
      "100                   2            2.092428\n",
      "100                 100            0.379074\n",
      "100                 200            0.414947\n"
     ]
    }
   ],
   "source": [
    "#矩阵分解R=PQ，推荐SVD方法\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from math import sqrt  \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#矩阵分解函数\n",
    "# @INPUT:\n",
    "  #R     : a matrix to be factorized, dimension N x M\n",
    "  #P     : an initial matrix of dimension N x K\n",
    "  #Q     : an initial matrix of dimension M x K\n",
    "  #K     : the number of latent features\n",
    "  #steps : the maximum number of steps to perform the optimisation\n",
    "  #alpha : the learning rate\n",
    "  #beta  : the regularization parameter\n",
    "#@OUTPUT: the final matrices P and Q\n",
    "\n",
    "def matrix_factorization(R, P, Q, K, steps, alpha=0.0002, beta=0.002):\n",
    "    Q = Q.T\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - np.dot(P[i,:],Q[:,j])\n",
    "                    for k in range(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "        eR = np.dot(P,Q)\n",
    "        e = 0\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    e = e + pow(R[i][j] - np.dot(P[i,:],Q[:,j]), 2)\n",
    "                    for k in range(K):\n",
    "                        e = e + (beta/2) * ( pow(P[i][k],2) + pow(Q[k][j],2) )\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return P, Q.T\n",
    "\n",
    "#1.数据集处理\n",
    "data = pd.read_csv(\"/data/fjsdata/BayesianRS/ml-20m/ratings.csv\",sep=',',low_memory=False, iterator =True)\n",
    "data = data.get_chunk(100)\n",
    "#将userId和movieId全部标准编号\n",
    "data_rating = data[['rating']]\n",
    "le = LabelEncoder()\n",
    "data = data[['userId','movieId']].apply(le.fit_transform)\n",
    "data = pd.concat([data,data_rating],axis=1)\n",
    "#抽样10%比例测试\n",
    "test = data.sample(frac=0.1)\n",
    "#2.建立评分矩阵\n",
    "uNum = data['userId'].max()+1\n",
    "iNum = data['movieId'].max()+1\n",
    "R = np.zeros((uNum, iNum))#转成R矩阵，非常稀疏\n",
    "for index, row in data.iterrows(): # 获取每行的值\n",
    "    R[int(row['userId'])][int(row['movieId'])] = row['rating']\n",
    "#3.SVD分解，随机梯度下降求解P、Q，并计算测试集均方误差\n",
    "print (\"%3s%20s%20s\" % ('K','steps','RMSE'))\n",
    "for K in [30,50,80,100]:#隐因子\n",
    "    for steps in [2,100,200]:#迭代次数\n",
    "        P = np.random.rand(uNum,K)\n",
    "        Q = np.random.rand(iNum,K)\n",
    "        #SVD分解\n",
    "        nP, nQ = matrix_factorization(R, P, Q, K,steps)\n",
    "        nR = np.dot(nP, nQ.T)  \n",
    "        #RMSE评估   \n",
    "        squaredError = []\n",
    "        for index, row in test.iterrows(): # 获取每行的值\n",
    "            pRating=nR[int(row['userId'])][int(row['movieId'])] #获取预测值\n",
    "            TRating=row['rating']\n",
    "            error=TRating-pRating\n",
    "            squaredError.append(error * error)\n",
    "        RMSE =sqrt(sum(squaredError) / len(squaredError))#均方根误差RMSE \n",
    "        print (\"%3d%20d%20.6f\" % (K,steps,RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset shape is:994169 rows and 3 columns\n",
      "testrate shape is:6040 rows and 3 columns\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "#loading Movielen dataset , ml-20m\n",
    "#trainset = pd.read_csv(\"/data/fjsdata/BayesianRS/ml-20m/ratings.csv\",sep=',',low_memory=False)#, iterator =True)\n",
    "#trainset shape is:20000263 rows and 4 columns,have 138493 users\n",
    "#print ('trainset shape is:%d rows and %d columns'%(trainset.shape[0],trainset.shape[1]))\n",
    "#Loading the  MovienLen dataset, ml-1m\n",
    "def load_data_as_list(filename):\n",
    "    data_list = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            data_list.append([user, item, rating])\n",
    "            line = f.readline()\n",
    "    return data_list\n",
    "#user:1-6040, item:1-3706, rating:1-5\n",
    "trainset = load_data_as_list(\"/data/fjsdata/ctKngBase/ml/ml-1m.train.rating\")\n",
    "trainset = pd.DataFrame(trainset,columns=[\"u\",\"i\",\"r\"])\n",
    "print ('trainset shape is:%d rows and %d columns'%(trainset.shape[0],trainset.shape[1]))\n",
    "testrate = load_data_as_list(\"/data/fjsdata/ctKngBase/ml/ml-1m.test.rating\")\n",
    "testrate = pd.DataFrame(testrate,columns=[\"u\",\"i\",\"r\"])\n",
    "print ('testrate shape is:%d rows and %d columns'%(testrate.shape[0],testrate.shape[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
