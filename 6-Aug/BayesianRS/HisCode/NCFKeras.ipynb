{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  K              NegNum            HitRatio                NDCG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:98: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=55187, output_dim=2, name=\"mf_embedding_user\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:100: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=9916, output_dim=2, name=\"mf_embedding_item\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:103: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=55187, output_dim=32, name=\"mlp_embedding_user\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:105: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=9916, output_dim=32, name=\"mlp_embedding_item\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:120: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, activation=\"relu\", name=\"layer1\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:120: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"relu\", name=\"layer2\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:120: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(8, activation=\"relu\", name=\"layer3\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:130: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", name=\"prediction\", kernel_initializer=\"lecun_uniform\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:132: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"pr...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:226: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2                   4            0.737293            0.421269\n",
      "  2                  10            0.809085            0.487186\n",
      "  2                  20            0.807437            0.493963\n",
      "  2                  50            0.774295            0.470568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:98: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=55187, output_dim=10, name=\"mf_embedding_user\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:100: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=9916, output_dim=10, name=\"mf_embedding_item\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10                   4            0.783917            0.470333\n",
      " 10                  10            0.835432            0.514143\n",
      " 10                  20            0.824524            0.513770\n",
      " 10                  50            0.760560            0.453226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:98: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=55187, output_dim=50, name=\"mf_embedding_user\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:100: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=9916, output_dim=50, name=\"mf_embedding_item\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50                   4            0.816225            0.495922\n",
      " 50                  10            0.843532            0.525368\n",
      " 50                  20            0.826318            0.522346\n",
      " 50                  50            0.713375            0.419657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:98: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=55187, output_dim=100, name=\"mf_embedding_user\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:100: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=9916, output_dim=100, name=\"mf_embedding_item\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100                   4            0.825774            0.504620\n",
      "100                  10            0.840615            0.525899\n",
      "100                  20            0.826227            0.523378\n",
      "100                  50            0.707649            0.409568\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.09\n",
    "@function: Implementing NCF and Setting pinterest-20 Dataset as baseline by rmse,hitradio,ndcg\n",
    "           https://arxiv.org/pdf/1708.05031.pdf\n",
    "           https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import scipy.sparse as sp\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import keras\n",
    "from keras import backend \n",
    "from keras.regularizers import l1, l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, Reshape, Flatten, Dropout, Multiply, concatenate\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras import initializers\n",
    "import heapq # for retrieval topK\n",
    "import multiprocessing\n",
    "\n",
    "#1.Loading the  MovienLen dataset, ml-1m\n",
    "def load_rating_file_as_list(filename):\n",
    "    ratingList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item = int(arr[0]), int(arr[1])\n",
    "            ratingList.append([user, item])\n",
    "            line = f.readline()\n",
    "    return ratingList\n",
    "    \n",
    "def load_negative_file(filename):\n",
    "    negativeList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            negatives = []\n",
    "            for x in arr[1: ]:\n",
    "                negatives.append(int(x))\n",
    "            negativeList.append(negatives)\n",
    "            line = f.readline()\n",
    "    return negativeList\n",
    "    \n",
    "def load_rating_file_as_matrix(filename):\n",
    "    '''\n",
    "    Read .rating file and Return dok matrix.\n",
    "    The first line of .rating file is: num_users\\t num_items\n",
    "    '''\n",
    "    # Get number of users and items\n",
    "    num_users, num_items = 0, 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            u, i = int(arr[0]), int(arr[1])\n",
    "            num_users = max(num_users, u)\n",
    "            num_items = max(num_items, i)\n",
    "            line = f.readline()\n",
    "    # Construct matrix\n",
    "    mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            if (rating > 0):\n",
    "                mat[user, item] = 1.0\n",
    "            line = f.readline()    \n",
    "    return mat\n",
    "\n",
    "trainMatrix = load_rating_file_as_matrix(\"/data/fjsdata/ctKngBase/ml/pinterest-20.train.rating\")\n",
    "testRatings = load_rating_file_as_list(\"/data/fjsdata/ctKngBase/ml/pinterest-20.test.rating\")\n",
    "testNegatives = load_negative_file(\"/data/fjsdata/ctKngBase/ml/pinterest-20.test.negative\")\n",
    "assert len(testRatings) == len(testNegatives)\n",
    "#2.NCF Class\n",
    "class NCF():\n",
    "    \n",
    "    def __init__(self,K):\n",
    "        self.K = K#num_factors\n",
    "    \n",
    "    def get_model(self, num_users, num_items, layers=[64,32,16,8], reg_layers=[0,0,0,0], reg_mf=0):\n",
    "        mf_dim = self.K#num_factors\n",
    "        num_layer = len(layers) #Number of layers in the MLP\n",
    "        # Input variables\n",
    "        user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "        item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "        # Embedding layer\n",
    "        MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user',\n",
    "                                      embeddings_initializer='normal', W_regularizer = l2(reg_mf), input_length=1)\n",
    "        MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item',\n",
    "                                      embeddings_initializer='normal', W_regularizer = l2(reg_mf), input_length=1)   \n",
    "\n",
    "        MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = \"mlp_embedding_user\",\n",
    "                                      embeddings_initializer='normal', W_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "        MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'mlp_embedding_item',\n",
    "                                      embeddings_initializer='normal', W_regularizer = l2(reg_layers[0]), input_length=1)   \n",
    "\n",
    "        # MF part\n",
    "        mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "        mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "        #mf_vector = merge([mf_user_latent, mf_item_latent], mode = 'mul') # element-wise multiply\n",
    "        mf_vector = Multiply()([mf_user_latent, mf_item_latent])\n",
    "\n",
    "        # MLP part \n",
    "        mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "        mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "        #mlp_vector = merge([mlp_user_latent, mlp_item_latent], mode = 'concat')\n",
    "        mlp_vector = concatenate([mlp_user_latent, mlp_item_latent], axis=-1)\n",
    "        \n",
    "        for idx in range(1, num_layer):\n",
    "            layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
    "            mlp_vector = layer(mlp_vector)\n",
    "\n",
    "        # Concatenate MF and MLP parts\n",
    "        #mf_vector = Lambda(lambda x: x * alpha)(mf_vector)\n",
    "        #mlp_vector = Lambda(lambda x : x * (1-alpha))(mlp_vector)\n",
    "        #predict_vector = merge([mf_vector, mlp_vector], mode = 'concat')\n",
    "        predict_vector = concatenate([mf_vector, mlp_vector], axis=-1)\n",
    "\n",
    "        # Final prediction layer\n",
    "        prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = \"prediction\")(predict_vector)\n",
    "\n",
    "        model = Model(input=[user_input, item_input], output=prediction)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def get_train_instances(self, train, num_negatives):\n",
    "        user_input, item_input, labels = [],[],[]\n",
    "        num_users = train.shape[0]\n",
    "        for (u, i) in train.keys():\n",
    "            # positive instance\n",
    "            user_input.append(u)\n",
    "            item_input.append(i)\n",
    "            labels.append(1)\n",
    "            # negative instances\n",
    "            for t in range(num_negatives):\n",
    "                j = np.random.randint(num_items)\n",
    "                #while train.has_key((u, j)):\n",
    "                while (u,j) in train:\n",
    "                    j = np.random.randint(num_items)\n",
    "                user_input.append(u)\n",
    "                item_input.append(j)\n",
    "                labels.append(0)\n",
    "        return user_input, item_input, labels\n",
    "    \n",
    "    def evaluate_model(self, model, testRatings, testNegatives, topK, num_thread):\n",
    "        \"\"\"\n",
    "        Evaluate the performance (Hit_Ratio, NDCG) of top-K recommendation\n",
    "        Return: score of each test rating.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.testRatings = testRatings\n",
    "        self.testNegatives = testNegatives\n",
    "        self.topK = topK\n",
    "\n",
    "        hits, ndcgs = [],[]\n",
    "        if(num_thread > 1): # Multi-thread\n",
    "            pool = multiprocessing.Pool(processes=num_thread)\n",
    "            res = pool.map(self.eval_one_rating, range(len(self.testRatings)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            hits = [r[0] for r in res]\n",
    "            ndcgs = [r[1] for r in res]\n",
    "            return (hits, ndcgs)\n",
    "        # Single thread\n",
    "        for idx in range(len(self.testRatings)):\n",
    "            (hr,ndcg) = self.eval_one_rating(idx)\n",
    "            hits.append(hr)\n",
    "            ndcgs.append(ndcg)      \n",
    "        return (hits, ndcgs)\n",
    "\n",
    "    def eval_one_rating(self,idx):\n",
    "        rating = self.testRatings[idx]\n",
    "        items = self.testNegatives[idx]\n",
    "        u = rating[0]\n",
    "        gtItem = rating[1]\n",
    "        items.append(gtItem)\n",
    "        # Get prediction scores\n",
    "        map_item_score = {}\n",
    "        users = np.full(len(items), u, dtype = 'int32')\n",
    "        predictions = self.model.predict([users, np.array(items)], \n",
    "                                     batch_size=100, verbose=0)\n",
    "        for i in range(len(items)):\n",
    "            item = items[i]\n",
    "            map_item_score[item] = predictions[i]\n",
    "        items.pop()\n",
    "\n",
    "        # Evaluate top rank list\n",
    "        ranklist = heapq.nlargest(self.topK, map_item_score, key=map_item_score.get)\n",
    "        hr = self.getHitRatio(ranklist, gtItem)\n",
    "        ndcg = self.getNDCG(ranklist, gtItem)\n",
    "        return (hr, ndcg)\n",
    "\n",
    "    def getHitRatio(self,ranklist, gtItem):\n",
    "        for item in ranklist:\n",
    "            if item == gtItem:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def getNDCG(self,ranklist, gtItem):\n",
    "        for i in range(len(ranklist)):\n",
    "            item = ranklist[i]\n",
    "            if item == gtItem:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "        return 0\n",
    "#3.Training and Evaluating\n",
    "print (\"%3s%20s%20s%20s\" % ('K', 'NegNum', 'HitRatio', 'NDCG'))\n",
    "for K in [2,10,50,100]:#latent factors\n",
    "    ncf = NCF(K=K)\n",
    "    num_users, num_items = trainMatrix.shape\n",
    "    model = ncf.get_model(num_users, num_items)# Build model\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy')\n",
    "    for NegNum in [4,10,20,50]:#iterations epoches  \n",
    "        user_input, item_input, labels = ncf.get_train_instances(trainMatrix, num_negatives=NegNum)# Generate training instances\n",
    "        hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                          np.array(labels), # labels \n",
    "                          batch_size=256, nb_epoch=1, verbose=0, shuffle=True)\n",
    "        (hits, ndcgs) = ncf.evaluate_model(model=model, testRatings=testRatings, \\\n",
    "                                           testNegatives=testNegatives, topK=10, num_thread=1)\n",
    "        hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "        print (\"%3d%20d%20.6f%20.6f\" % (K, NegNum, hr, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.09\n",
    "@function: Implementing NCF and Setting Movielen Dataset(ml-1m) as baseline by rmse,hitradio,ndcg\n",
    "           https://arxiv.org/pdf/1708.05031.pdf\n",
    "           https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import scipy.sparse as sp\n",
    "\n",
    "#1.Loading the  MovienLen dataset, ml-1m\n",
    "def load_rating_file_as_list(filename):\n",
    "    ratingList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item = int(arr[0]), int(arr[1])\n",
    "            ratingList.append([user, item])\n",
    "            line = f.readline()\n",
    "    return ratingList\n",
    "    \n",
    "def load_negative_file(filename):\n",
    "    negativeList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            negatives = []\n",
    "            for x in arr[1: ]:\n",
    "                negatives.append(int(x))\n",
    "            negativeList.append(negatives)\n",
    "            line = f.readline()\n",
    "    return negativeList\n",
    "    \n",
    "def load_rating_file_as_matrix(filename):\n",
    "    '''\n",
    "    Read .rating file and Return dok matrix.\n",
    "    The first line of .rating file is: num_users\\t num_items\n",
    "    '''\n",
    "    # Get number of users and items\n",
    "    num_users, num_items = 0, 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            u, i = int(arr[0]), int(arr[1])\n",
    "            num_users = max(num_users, u)\n",
    "            num_items = max(num_items, i)\n",
    "            line = f.readline()\n",
    "    # Construct matrix\n",
    "    mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            if (rating > 0):\n",
    "                mat[user, item] = 1.0\n",
    "            line = f.readline()    \n",
    "    return mat\n",
    "\n",
    "trainMatrix = load_rating_file_as_matrix(\"/data/fjsdata/ctKngBase/ml/ml-1m.train.rating\")\n",
    "testRatings = load_rating_file_as_list(\"/data/fjsdata/ctKngBase/ml/ml-1m.test.rating\")\n",
    "testNegatives = load_negative_file(\"/data/fjsdata/ctKngBase/ml/ml-1m.test.negative\")\n",
    "assert len(testRatings) == len(testNegatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  K              NegNum            HitRatio                NDCG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=6040, output_dim=2, name=\"mf_embedding_user\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=3706, output_dim=2, name=\"mf_embedding_item\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=6040, output_dim=32, name=\"mlp_embedding_user\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=3706, output_dim=32, name=\"mlp_embedding_item\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, activation=\"relu\", name=\"layer1\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"relu\", name=\"layer2\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(8, activation=\"relu\", name=\"layer3\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", name=\"prediction\", kernel_initializer=\"lecun_uniform\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"pr...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:156: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2                   4            0.544702            0.306534\n",
      "  2                  10            0.614238            0.353439\n",
      "  2                  20            0.611093            0.354414\n",
      "  2                  50            0.582781            0.333956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=6040, output_dim=10, name=\"mf_embedding_user\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=3706, output_dim=10, name=\"mf_embedding_item\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10                   4            0.601987            0.340144\n",
      " 10                  10            0.645530            0.376645\n",
      " 10                  20            0.638742            0.374999\n",
      " 10                  50            0.561755            0.326713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=6040, output_dim=50, name=\"mf_embedding_user\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=3706, output_dim=50, name=\"mf_embedding_item\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50                   4            0.629636            0.366642\n",
      " 50                  10            0.680132            0.405432\n",
      " 50                  20            0.665397            0.400873\n",
      " 50                  50            0.576325            0.329987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=6040, output_dim=100, name=\"mf_embedding_user\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=3706, output_dim=100, name=\"mf_embedding_item\", embeddings_initializer=\"normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100                   4            0.650166            0.374791\n",
      "100                  10            0.672682            0.405173\n",
      "100                  20            0.662417            0.398848\n",
      "100                  50            0.582285            0.339298\n"
     ]
    }
   ],
   "source": [
    "#2. NCF class\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import keras\n",
    "from keras import backend \n",
    "from keras.regularizers import l1, l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, Reshape, Flatten, Dropout, Multiply, concatenate\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras import initializers\n",
    "import heapq # for retrieval topK\n",
    "import multiprocessing\n",
    "class NCF():\n",
    "    \n",
    "    def __init__(self,K):\n",
    "        self.K = K#num_factors\n",
    "    \n",
    "    def get_model(self, num_users, num_items, layers=[64,32,16,8], reg_layers=[0,0,0,0], reg_mf=0):\n",
    "        mf_dim = self.K#num_factors\n",
    "        num_layer = len(layers) #Number of layers in the MLP\n",
    "        # Input variables\n",
    "        user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "        item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "        # Embedding layer\n",
    "        MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user',\n",
    "                                      embeddings_initializer='normal', W_regularizer = l2(reg_mf), input_length=1)\n",
    "        MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item',\n",
    "                                      embeddings_initializer='normal', W_regularizer = l2(reg_mf), input_length=1)   \n",
    "\n",
    "        MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = \"mlp_embedding_user\",\n",
    "                                      embeddings_initializer='normal', W_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "        MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'mlp_embedding_item',\n",
    "                                      embeddings_initializer='normal', W_regularizer = l2(reg_layers[0]), input_length=1)   \n",
    "\n",
    "        # MF part\n",
    "        mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "        mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "        #mf_vector = merge([mf_user_latent, mf_item_latent], mode = 'mul') # element-wise multiply\n",
    "        mf_vector = Multiply()([mf_user_latent, mf_item_latent])\n",
    "\n",
    "        # MLP part \n",
    "        mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "        mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "        #mlp_vector = merge([mlp_user_latent, mlp_item_latent], mode = 'concat')\n",
    "        mlp_vector = concatenate([mlp_user_latent, mlp_item_latent], axis=-1)\n",
    "        \n",
    "        for idx in range(1, num_layer):\n",
    "            layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
    "            mlp_vector = layer(mlp_vector)\n",
    "\n",
    "        # Concatenate MF and MLP parts\n",
    "        #mf_vector = Lambda(lambda x: x * alpha)(mf_vector)\n",
    "        #mlp_vector = Lambda(lambda x : x * (1-alpha))(mlp_vector)\n",
    "        #predict_vector = merge([mf_vector, mlp_vector], mode = 'concat')\n",
    "        predict_vector = concatenate([mf_vector, mlp_vector], axis=-1)\n",
    "\n",
    "        # Final prediction layer\n",
    "        prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = \"prediction\")(predict_vector)\n",
    "\n",
    "        model = Model(input=[user_input, item_input], output=prediction)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def get_train_instances(self, train, num_negatives):\n",
    "        user_input, item_input, labels = [],[],[]\n",
    "        num_users = train.shape[0]\n",
    "        for (u, i) in train.keys():\n",
    "            # positive instance\n",
    "            user_input.append(u)\n",
    "            item_input.append(i)\n",
    "            labels.append(1)\n",
    "            # negative instances\n",
    "            for t in range(num_negatives):\n",
    "                j = np.random.randint(num_items)\n",
    "                #while train.has_key((u, j)):\n",
    "                while (u,j) in train:\n",
    "                    j = np.random.randint(num_items)\n",
    "                user_input.append(u)\n",
    "                item_input.append(j)\n",
    "                labels.append(0)\n",
    "        return user_input, item_input, labels\n",
    "    \n",
    "    def evaluate_model(self, model, testRatings, testNegatives, topK, num_thread):\n",
    "        \"\"\"\n",
    "        Evaluate the performance (Hit_Ratio, NDCG) of top-K recommendation\n",
    "        Return: score of each test rating.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.testRatings = testRatings\n",
    "        self.testNegatives = testNegatives\n",
    "        self.topK = topK\n",
    "\n",
    "        hits, ndcgs = [],[]\n",
    "        if(num_thread > 1): # Multi-thread\n",
    "            pool = multiprocessing.Pool(processes=num_thread)\n",
    "            res = pool.map(self.eval_one_rating, range(len(self.testRatings)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            hits = [r[0] for r in res]\n",
    "            ndcgs = [r[1] for r in res]\n",
    "            return (hits, ndcgs)\n",
    "        # Single thread\n",
    "        for idx in range(len(self.testRatings)):\n",
    "            (hr,ndcg) = self.eval_one_rating(idx)\n",
    "            hits.append(hr)\n",
    "            ndcgs.append(ndcg)      \n",
    "        return (hits, ndcgs)\n",
    "\n",
    "    def eval_one_rating(self,idx):\n",
    "        rating = self.testRatings[idx]\n",
    "        items = self.testNegatives[idx]\n",
    "        u = rating[0]\n",
    "        gtItem = rating[1]\n",
    "        items.append(gtItem)\n",
    "        # Get prediction scores\n",
    "        map_item_score = {}\n",
    "        users = np.full(len(items), u, dtype = 'int32')\n",
    "        predictions = self.model.predict([users, np.array(items)], \n",
    "                                     batch_size=100, verbose=0)\n",
    "        for i in range(len(items)):\n",
    "            item = items[i]\n",
    "            map_item_score[item] = predictions[i]\n",
    "        items.pop()\n",
    "\n",
    "        # Evaluate top rank list\n",
    "        ranklist = heapq.nlargest(self.topK, map_item_score, key=map_item_score.get)\n",
    "        hr = self.getHitRatio(ranklist, gtItem)\n",
    "        ndcg = self.getNDCG(ranklist, gtItem)\n",
    "        return (hr, ndcg)\n",
    "\n",
    "    def getHitRatio(self,ranklist, gtItem):\n",
    "        for item in ranklist:\n",
    "            if item == gtItem:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def getNDCG(self,ranklist, gtItem):\n",
    "        for i in range(len(ranklist)):\n",
    "            item = ranklist[i]\n",
    "            if item == gtItem:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "        return 0\n",
    "#3.Training and Evaluating\n",
    "print (\"%3s%20s%20s%20s\" % ('K', 'NegNum', 'HitRatio', 'NDCG'))\n",
    "for K in [2,10,50,100]:#latent factors\n",
    "    ncf = NCF(K=K)\n",
    "    num_users, num_items = trainMatrix.shape\n",
    "    model = ncf.get_model(num_users, num_items)# Build model\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy')\n",
    "    for NegNum in [4,10,20,50]:#iterations epoches  \n",
    "        user_input, item_input, labels = ncf.get_train_instances(trainMatrix, num_negatives=NegNum)# Generate training instances\n",
    "        hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                          np.array(labels), # labels \n",
    "                          batch_size=256, nb_epoch=1, verbose=0, shuffle=True)\n",
    "        (hits, ndcgs) = ncf.evaluate_model(model=model, testRatings=testRatings, \\\n",
    "                                           testNegatives=testNegatives, topK=10, num_thread=1)\n",
    "        hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "        print (\"%3d%20d%20.6f%20.6f\" % (K, NegNum, hr, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
