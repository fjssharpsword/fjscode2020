{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6039\n",
      "\tItem Num: 3705\n",
      "\tData Size: 994169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-23 07:26:00,509]: building the BPMF model\n",
      "[2019-07-23 07:26:21,704]: done building the BPMF model\n",
      "[2019-07-23 07:26:21,709]: finding PMF MAP using Powell optimization\n",
      "Only 100 samples in chain.\n",
      "[2019-07-23 07:26:40,130]: Only 100 samples in chain.\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "[2019-07-23 07:26:40,147]: Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [V, mu_v, corr_v, sigma_v, U, mu_u, corr_u, sigma_u]\n",
      "[2019-07-23 07:26:40,153]: NUTS: [V, mu_v, corr_v, sigma_v, U, mu_u, corr_u, sigma_u]\n",
      "Sampling 4 chains:   0%|          | 12/2400 [01:47<10:26:46, 15.75s/draws]"
     ]
    }
   ],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.22\n",
    "@function: Implementing BPMF by MCMC\n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "           https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf\n",
    "@reference: https://gist.github.com/macks22/00a17b1d374dfc267a9a\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as t\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "def getTraindata():\n",
    "    data = []\n",
    "    filePath = '/data/fjsdata/ctKngBase/ml/ml-1m.train.rating'\n",
    "    u = 0\n",
    "    i = 0\n",
    "    maxr = 0.0\n",
    "    with open(filePath, 'r') as f:\n",
    "        for line in f:\n",
    "            if line:\n",
    "                lines = line[:-1].split(\"\\t\")\n",
    "                user = int(lines[0])\n",
    "                item = int(lines[1])\n",
    "                score = float(lines[2])\n",
    "                data.append((user, item, score))\n",
    "                if user > u: u = user\n",
    "                if item > i: i = item\n",
    "                if score > maxr: maxr = score\n",
    "    print(\"Loading Success!\\n\"\n",
    "                  \"Data Info:\\n\"\n",
    "                  \"\\tUser Num: {}\\n\"\n",
    "                  \"\\tItem Num: {}\\n\"\n",
    "                  \"\\tData Size: {}\".format(u, i, len(data)))\n",
    "    R = np.zeros([u+1, i+1], dtype=np.float32)\n",
    "    for i in data:\n",
    "        user = i[0]\n",
    "        item = i[1]\n",
    "        rating = i[2]\n",
    "        R[user][item] = rating\n",
    "    return R\n",
    "def getTestdata():\n",
    "    testset = []\n",
    "    filePath = '/data/fjsdata/ctKngBase/ml/ml-1m.test.negative'\n",
    "    with open(filePath, 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            testset.append([u, eval(arr[0])[1]])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                testset.append([u, int(i)]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return testset\n",
    "\n",
    "\n",
    "def build_pmf_model(train, alpha=2, dim=8, std=0.01):\n",
    "    \"\"\"Construct the Probabilistic Matrix Factorization model using pymc3.\n",
    "    Note that the `testval` param for U and V initialize the model away from\n",
    "    0 using a small amount of Gaussian noise.\n",
    "    :param np.ndarray train: Training data (observed) to learn the model on.\n",
    "    :param int alpha: Fixed precision to use for the rating likelihood function.\n",
    "    :param int dim: Dimensionality of the model; rank of low-rank approximation.\n",
    "    :param float std: Standard deviation for Gaussian noise in model initialization.\n",
    "    \"\"\"\n",
    "    # Mean value imputation on training data.\n",
    "    train = train.copy()\n",
    "    nan_mask = np.isnan(train)\n",
    "    train[nan_mask] = train[~nan_mask].mean()\n",
    "\n",
    "    # Low precision reflects uncertainty; prevents overfitting.\n",
    "    # We use point estimates from the data to intialize.\n",
    "    # Set to mean variance across users and items.\n",
    "    alpha_u = 1 / train.var(axis=1).mean()\n",
    "    alpha_v = 1 / train.var(axis=0).mean()\n",
    "\n",
    "    logging.info('building the PMF model')\n",
    "    n, m = train.shape\n",
    "    with pm.Model() as pmf:\n",
    "        U = pm.MvNormal('U', mu=0, tau=alpha_u * np.eye(dim),shape=(n, dim), testval=np.random.randn(n, dim) * std)\n",
    "        V = pm.MvNormal('V', mu=0, tau=alpha_v * np.eye(dim),shape=(m, dim), testval=np.random.randn(m, dim) * std)\n",
    "        R = pm.Normal('R', mu=t.dot(U, V.T), tau=alpha * np.ones(train.shape),observed=train)\n",
    "    logging.info('done building PMF model')\n",
    "    return pmf\n",
    "\n",
    "\n",
    "def build_bpmf_model(train, alpha=2, dim=8, std=0.01):\n",
    "    \"\"\"Build the original BPMF model, which we cannot sample from due to\n",
    "    current limitations in pymc3's implementation of the Wishart distribution.\n",
    "    \"\"\"\n",
    "    n, m = train.shape\n",
    "    beta_0 = 1  # scaling factor for lambdas; unclear on its use\n",
    "\n",
    "    # Mean value imputation on training data.\n",
    "    train = train.copy()\n",
    "    nan_mask = np.isnan(train)\n",
    "    train[nan_mask] = train[~nan_mask].mean()\n",
    "\n",
    "    logging.info('building the BPMF model')\n",
    "    with pm.Model() as bpmf:\n",
    "        # Specify user feature matrix\n",
    "        lambda_u = pm.Wishart('lambda_u', n=dim, V=np.eye(dim), shape=(dim, dim),testval=np.random.randn(dim, dim) * std)\n",
    "        mu_u = pm.Normal('mu_u', mu=0, tau=beta_0 * lambda_u, shape=dim,testval=np.random.randn(dim) * std)\n",
    "        U = pm.MvNormal( 'U', mu=mu_u, tau=lambda_u, shape=(n, dim),testval=np.random.randn(n, dim) * std)\n",
    "\n",
    "        # Specify item feature matrix\n",
    "        lambda_v = pm.Wishart('lambda_v', n=dim, V=np.eye(dim), shape=(dim, dim),testval=np.random.randn(dim, dim) * std)\n",
    "        mu_v = pm.Normal('mu_v', mu=0, tau=beta_0 * lambda_v, shape=dim,testval=np.random.randn(dim) * std)\n",
    "        V = pm.MvNormal('V', mu=mu_v, tau=lambda_v, shape=(m, dim),testval=np.random.randn(m, dim) * std)\n",
    "\n",
    "        # Specify rating likelihood function\n",
    "        R = pm.Normal('R', mu=t.dot(U, V.T), tau=alpha * np.ones((n, m)),observed=train)\n",
    "\n",
    "    logging.info('done building the BPMF model')\n",
    "    return bpmf\n",
    "\n",
    "\n",
    "def build_mod_bpmf_model(train, alpha=2, dim=8, std=0.01):\n",
    "    \"\"\"Build the modified BPMF model using pymc3. The original model uses\n",
    "    Wishart priors on the covariance matrices. Unfortunately, the Wishart\n",
    "    distribution in pymc3 is currently not suitable for sampling. This\n",
    "    version decomposes the covariance matrix into:\n",
    "        diag(sigma) \\dot corr_matrix \\dot diag(std).\n",
    "    We use uniform priors on the standard deviations (sigma) and LKJCorr\n",
    "    priors on the correlation matrices (corr_matrix):\n",
    "        sigma ~ Uniform\n",
    "        corr_matrix ~ LKJCorr(n=1, p=dim)\n",
    "    \"\"\"\n",
    "    n, m = train.shape\n",
    "    beta_0 = 1  # scaling factor for lambdas; unclear on its use\n",
    "\n",
    "    # Mean value imputation on training data.\n",
    "    train = train.copy()\n",
    "    nan_mask = np.isnan(train)\n",
    "    train[nan_mask] = train[~nan_mask].mean()\n",
    "\n",
    "    # We will use separate priors for sigma and correlation matrix.\n",
    "    # In order to convert the upper triangular correlation values to a\n",
    "    # complete correlation matrix, we need to construct an index matrix:\n",
    "    n_elem = int(dim * (dim - 1) / 2)\n",
    "    tri_index = np.zeros([dim, dim], dtype=int)\n",
    "    tri_index[np.triu_indices(dim, k=1)] = np.arange(n_elem)\n",
    "    tri_index[np.triu_indices(dim, k=1)[::-1]] = np.arange(n_elem)\n",
    "\n",
    "    logging.info('building the BPMF model')\n",
    "    with pm.Model() as bpmf:\n",
    "        # Specify user feature matrix\n",
    "        sigma_u = pm.Uniform('sigma_u', shape=dim)\n",
    "        corr_triangle_u = pm.LKJCorr('corr_u', n=1, p=dim, testval=np.random.randn(n_elem) * std)\n",
    "\n",
    "        corr_matrix_u = corr_triangle_u[tri_index]\n",
    "        corr_matrix_u = t.fill_diagonal(corr_matrix_u, 1)\n",
    "        cov_matrix_u = t.diag(sigma_u).dot(corr_matrix_u.dot(t.diag(sigma_u)))\n",
    "        lambda_u = t.nlinalg.matrix_inverse(cov_matrix_u)\n",
    "\n",
    "        mu_u = pm.Normal('mu_u', mu=0, tau=beta_0 * t.diag(lambda_u), shape=dim,testval=np.random.randn(dim) * std)\n",
    "        U = pm.MvNormal('U', mu=mu_u, tau=lambda_u, shape=(n, dim),testval=np.random.randn(n, dim) * std)\n",
    "\n",
    "        # Specify item feature matrix\n",
    "        sigma_v = pm.Uniform('sigma_v', shape=dim)\n",
    "        corr_triangle_v = pm.LKJCorr('corr_v', n=1, p=dim,testval=np.random.randn(n_elem) * std)\n",
    "\n",
    "        corr_matrix_v = corr_triangle_v[tri_index]\n",
    "        corr_matrix_v = t.fill_diagonal(corr_matrix_v, 1)\n",
    "        cov_matrix_v = t.diag(sigma_v).dot(corr_matrix_v.dot(t.diag(sigma_v)))\n",
    "        lambda_v = t.nlinalg.matrix_inverse(cov_matrix_v)\n",
    "\n",
    "        mu_v = pm.Normal('mu_v', mu=0, tau=beta_0 * t.diag(lambda_v), shape=dim,testval=np.random.randn(dim) * std)\n",
    "        V = pm.MvNormal( 'V', mu=mu_v, tau=lambda_v, shape=(m, dim),testval=np.random.randn(m, dim) * std)\n",
    "\n",
    "        # Specify rating likelihood function\n",
    "        R = pm.Normal('R', mu=t.dot(U, V.T), tau=alpha * np.ones((n, m)),observed=train)\n",
    "\n",
    "    logging.info('done building the BPMF model')\n",
    "    return bpmf\n",
    "\n",
    "def getHitRatio(ranklist, targetItem):\n",
    "    for item in ranklist:\n",
    "        if item == targetItem:\n",
    "            return 1\n",
    "    return 0\n",
    "def getNDCG(ranklist, targetItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == targetItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO,format='[%(asctime)s]: %(message)s')\n",
    "\n",
    "    # Read data and build PMF model.\n",
    "    train = getTraindata()\n",
    "    bpmf = build_mod_bpmf_model(train, dim=8)#dim is the number of latent factors\n",
    "\n",
    "    with bpmf:# sample with BPMF\n",
    "        tstart = time.time()\n",
    "        logging.info('Starting BPMF training')\n",
    "        #start = pm.find_MAP()    \n",
    "        step = pm.NUTS()\n",
    "        #trace = pm.sample(1000, step, start=start)\n",
    "        trace = pm.sample(100, step)\n",
    "        elapsed = time.time() - tstart    \n",
    "        logging.info('Completed BPMF in %d seconds' % int(elapsed))\n",
    "        \n",
    "    with bpmf:#evaluation\n",
    "        testset = getTestdata()\n",
    "        ppc = pm.sample_posterior_predictive(trace, progressbar=True)\n",
    "        nR = np.mean(ppc['R'],0)#three dims, calcuate the mean with the first dim for posterior\n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        prev_u = testset[0][0]\n",
    "        pos_i = testset[0][1]\n",
    "        scorelist = []\n",
    "        for u, i in testset:\n",
    "            if prev_u == u:\n",
    "                scorelist.append([i,nR[u,i]])\n",
    "            else:\n",
    "                map_item_score = {}\n",
    "                for item, rate in scorelist: #turn dict\n",
    "                    map_item_score[item] = rate\n",
    "                ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "                hr = getHitRatio(ranklist, pos_i)\n",
    "                hits.append(hr)\n",
    "                ndcg = getNDCG(ranklist, pos_i)\n",
    "                ndcgs.append(ndcg)\n",
    "                #next user\n",
    "                scorelist = []\n",
    "                prev_u = u\n",
    "                pos_i = i\n",
    "                scorelist.append([i,nR[u,i]])\n",
    "        hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print(\"hr: {}, NDCG: {}, At K {}\".format(hitratio, ndcg, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-22 02:29:07,773]: reading data\n",
      "[2019-07-22 02:29:07,785]: splitting train/test sets\n",
      "[2019-07-22 02:29:07,792]: building the PMF model\n",
      "[2019-07-22 02:29:08,627]: done building PMF model\n",
      "[2019-07-22 02:29:08,628]: finding PMF MAP using Powell optimization\n",
      "logp = -15,603, ||grad|| = 0.12789: 100%|██████████| 145/145 [00:00<00:00, 982.92it/s] \n",
      "[2019-07-22 02:29:10,381]: found PMF MAP in 1 seconds\n",
      "[2019-07-22 02:29:10,384]: building the BPMF model\n",
      "[2019-07-22 02:29:49,244]: done building the BPMF model\n",
      "[2019-07-22 02:29:49,247]: drawing 100 MCMC samples using 2 jobs\n",
      "Only 100 samples in chain.\n",
      "[2019-07-22 02:34:31,858]: Only 100 samples in chain.\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "[2019-07-22 02:34:31,865]: Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [V, mu_v, corr_v, sigma_v, U, mu_u, corr_u, sigma_u]\n",
      "[2019-07-22 02:34:31,871]: NUTS: [V, mu_v, corr_v, sigma_v, U, mu_u, corr_u, sigma_u]\n",
      "Sampling 2 chains: 100%|██████████| 1200/1200 [48:18<00:00,  6.31s/draws] \n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "[2019-07-22 03:22:52,432]: The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "[2019-07-22 03:22:52,438]: The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The gelman-rubin statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\n",
      "[2019-07-22 03:22:52,443]: The gelman-rubin statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "[2019-07-22 03:22:52,448]: The number of effective samples is smaller than 10% for some parameters.\n"
     ]
    }
   ],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.22\n",
    "@function: Implementing BPMF\n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "           https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf\n",
    "@reference: https://gist.github.com/macks22/00a17b1d374dfc267a9a\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as t\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "DATA_NOT_FOUND = -1\n",
    "\n",
    "\n",
    "# data from: https://gist.github.com/macks22/b40ac9c685e920ad3ca2\n",
    "def read_jester_data(fname='/data/tmpexec/jester-dense-subset-100x20.csv'):\n",
    "    \"\"\"Read dense Jester dataset and split train/test data randomly.\n",
    "    We use a 0.9:0.1 Train:Test split.\n",
    "    \"\"\"\n",
    "    logging.info('reading data')\n",
    "    try:\n",
    "        data = pd.read_csv(fname)\n",
    "    except IOError as err:\n",
    "        print (str(err))\n",
    "        url = 'https://gist.github.com/macks22/b40ac9c685e920ad3ca2'\n",
    "        print ('download from: %s' % url)\n",
    "        sys.exit(DATA_NOT_FOUND)\n",
    "\n",
    "    # Calculate split sizes.\n",
    "    logging.info('splitting train/test sets')\n",
    "    n, m = data.shape           # # users, # jokes\n",
    "    N = n * m                   # # cells in matrix\n",
    "    test_size = int(N / 10)         # use 10% of data as test set\n",
    "    train_size = N - test_size  # and remainder for training\n",
    "\n",
    "    # Prepare train/test ndarrays.\n",
    "    train = data.copy().values\n",
    "    test = np.ones(data.shape) * np.nan\n",
    "\n",
    "    # Draw random sample of training data to use for testing.\n",
    "    tosample = np.where(~np.isnan(train))        # only sample non-missing values\n",
    "    idx_pairs = list(zip(tosample[0], tosample[1]))    # zip row/col indices\n",
    "    indices = np.arange(len(idx_pairs))      # indices of row/col index pairs\n",
    "    sample = np.random.choice(indices, replace=False, size=test_size)  # draw sample\n",
    "\n",
    "    # Transfer random sample from train set to test set.\n",
    "    for idx in sample:\n",
    "        idx_pair = idx_pairs[idx]         # retrieve sampled index pair\n",
    "        test[idx_pair] = train[idx_pair]  # transfer to test set\n",
    "        train[idx_pair] = np.nan          # remove from train set\n",
    "\n",
    "    # Verify everything worked properly\n",
    "    assert(np.isnan(train).sum() == test_size)\n",
    "    assert(np.isnan(test).sum() == train_size)\n",
    "\n",
    "    # Return the two numpy ndarrays\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def build_pmf_model(train, alpha=2, dim=10, std=0.01):\n",
    "    \"\"\"Construct the Probabilistic Matrix Factorization model using pymc3.\n",
    "    Note that the `testval` param for U and V initialize the model away from\n",
    "    0 using a small amount of Gaussian noise.\n",
    "    :param np.ndarray train: Training data (observed) to learn the model on.\n",
    "    :param int alpha: Fixed precision to use for the rating likelihood function.\n",
    "    :param int dim: Dimensionality of the model; rank of low-rank approximation.\n",
    "    :param float std: Standard deviation for Gaussian noise in model initialization.\n",
    "    \"\"\"\n",
    "    # Mean value imputation on training data.\n",
    "    train = train.copy()\n",
    "    nan_mask = np.isnan(train)\n",
    "    train[nan_mask] = train[~nan_mask].mean()\n",
    "\n",
    "    # Low precision reflects uncertainty; prevents overfitting.\n",
    "    # We use point estimates from the data to intialize.\n",
    "    # Set to mean variance across users and items.\n",
    "    alpha_u = 1 / train.var(axis=1).mean()\n",
    "    alpha_v = 1 / train.var(axis=0).mean()\n",
    "\n",
    "    logging.info('building the PMF model')\n",
    "    n, m = train.shape\n",
    "    with pm.Model() as pmf:\n",
    "        U = pm.MvNormal('U', mu=0, tau=alpha_u * np.eye(dim),shape=(n, dim), testval=np.random.randn(n, dim) * std)\n",
    "        V = pm.MvNormal('V', mu=0, tau=alpha_v * np.eye(dim),shape=(m, dim), testval=np.random.randn(m, dim) * std)\n",
    "        R = pm.Normal('R', mu=t.dot(U, V.T), tau=alpha * np.ones(train.shape),observed=train)\n",
    "    logging.info('done building PMF model')\n",
    "    return pmf\n",
    "\n",
    "\n",
    "def build_bpmf_model(train, alpha=2, dim=10, std=0.01):\n",
    "    \"\"\"Build the original BPMF model, which we cannot sample from due to\n",
    "    current limitations in pymc3's implementation of the Wishart distribution.\n",
    "    \"\"\"\n",
    "    n, m = train.shape\n",
    "    beta_0 = 1  # scaling factor for lambdas; unclear on its use\n",
    "\n",
    "    # Mean value imputation on training data.\n",
    "    train = train.copy()\n",
    "    nan_mask = np.isnan(train)\n",
    "    train[nan_mask] = train[~nan_mask].mean()\n",
    "\n",
    "    logging.info('building the BPMF model')\n",
    "    with pm.Model() as bpmf:\n",
    "        # Specify user feature matrix\n",
    "        lambda_u = pm.Wishart('lambda_u', n=dim, V=np.eye(dim), shape=(dim, dim),testval=np.random.randn(dim, dim) * std)\n",
    "        mu_u = pm.Normal('mu_u', mu=0, tau=beta_0 * lambda_u, shape=dim,testval=np.random.randn(dim) * std)\n",
    "        U = pm.MvNormal( 'U', mu=mu_u, tau=lambda_u, shape=(n, dim),testval=np.random.randn(n, dim) * std)\n",
    "\n",
    "        # Specify item feature matrix\n",
    "        lambda_v = pm.Wishart('lambda_v', n=dim, V=np.eye(dim), shape=(dim, dim),testval=np.random.randn(dim, dim) * std)\n",
    "        mu_v = pm.Normal('mu_v', mu=0, tau=beta_0 * lambda_v, shape=dim,testval=np.random.randn(dim) * std)\n",
    "        V = pm.MvNormal('V', mu=mu_v, tau=lambda_v, shape=(m, dim),testval=np.random.randn(m, dim) * std)\n",
    "\n",
    "        # Specify rating likelihood function\n",
    "        R = pm.Normal('R', mu=t.dot(U, V.T), tau=alpha * np.ones((n, m)),observed=train)\n",
    "\n",
    "    logging.info('done building the BPMF model')\n",
    "    return bpmf\n",
    "\n",
    "\n",
    "def build_mod_bpmf_model(train, alpha=2, dim=10, std=0.01):\n",
    "    \"\"\"Build the modified BPMF model using pymc3. The original model uses\n",
    "    Wishart priors on the covariance matrices. Unfortunately, the Wishart\n",
    "    distribution in pymc3 is currently not suitable for sampling. This\n",
    "    version decomposes the covariance matrix into:\n",
    "        diag(sigma) \\dot corr_matrix \\dot diag(std).\n",
    "    We use uniform priors on the standard deviations (sigma) and LKJCorr\n",
    "    priors on the correlation matrices (corr_matrix):\n",
    "        sigma ~ Uniform\n",
    "        corr_matrix ~ LKJCorr(n=1, p=dim)\n",
    "    \"\"\"\n",
    "    n, m = train.shape\n",
    "    beta_0 = 1  # scaling factor for lambdas; unclear on its use\n",
    "\n",
    "    # Mean value imputation on training data.\n",
    "    train = train.copy()\n",
    "    nan_mask = np.isnan(train)\n",
    "    train[nan_mask] = train[~nan_mask].mean()\n",
    "\n",
    "    # We will use separate priors for sigma and correlation matrix.\n",
    "    # In order to convert the upper triangular correlation values to a\n",
    "    # complete correlation matrix, we need to construct an index matrix:\n",
    "    n_elem = int(dim * (dim - 1) / 2)\n",
    "    tri_index = np.zeros([dim, dim], dtype=int)\n",
    "    tri_index[np.triu_indices(dim, k=1)] = np.arange(n_elem)\n",
    "    tri_index[np.triu_indices(dim, k=1)[::-1]] = np.arange(n_elem)\n",
    "\n",
    "    logging.info('building the BPMF model')\n",
    "    with pm.Model() as bpmf:\n",
    "        # Specify user feature matrix\n",
    "        sigma_u = pm.Uniform('sigma_u', shape=dim)\n",
    "        corr_triangle_u = pm.LKJCorr('corr_u', n=1, p=dim, testval=np.random.randn(n_elem) * std)\n",
    "\n",
    "        corr_matrix_u = corr_triangle_u[tri_index]\n",
    "        corr_matrix_u = t.fill_diagonal(corr_matrix_u, 1)\n",
    "        cov_matrix_u = t.diag(sigma_u).dot(corr_matrix_u.dot(t.diag(sigma_u)))\n",
    "        lambda_u = t.nlinalg.matrix_inverse(cov_matrix_u)\n",
    "\n",
    "        mu_u = pm.Normal('mu_u', mu=0, tau=beta_0 * t.diag(lambda_u), shape=dim,testval=np.random.randn(dim) * std)\n",
    "        U = pm.MvNormal('U', mu=mu_u, tau=lambda_u, shape=(n, dim),testval=np.random.randn(n, dim) * std)\n",
    "\n",
    "        # Specify item feature matrix\n",
    "        sigma_v = pm.Uniform('sigma_v', shape=dim)\n",
    "        corr_triangle_v = pm.LKJCorr('corr_v', n=1, p=dim,testval=np.random.randn(n_elem) * std)\n",
    "\n",
    "        corr_matrix_v = corr_triangle_v[tri_index]\n",
    "        corr_matrix_v = t.fill_diagonal(corr_matrix_v, 1)\n",
    "        cov_matrix_v = t.diag(sigma_v).dot(corr_matrix_v.dot(t.diag(sigma_v)))\n",
    "        lambda_v = t.nlinalg.matrix_inverse(cov_matrix_v)\n",
    "\n",
    "        mu_v = pm.Normal('mu_v', mu=0, tau=beta_0 * t.diag(lambda_v), shape=dim,testval=np.random.randn(dim) * std)\n",
    "        V = pm.MvNormal( 'V', mu=mu_v, tau=lambda_v, shape=(m, dim),testval=np.random.randn(m, dim) * std)\n",
    "\n",
    "        # Specify rating likelihood function\n",
    "        R = pm.Normal('R', mu=t.dot(U, V.T), tau=alpha * np.ones((n, m)),observed=train)\n",
    "\n",
    "    logging.info('done building the BPMF model')\n",
    "    return bpmf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO,format='[%(asctime)s]: %(message)s')\n",
    "\n",
    "    # Read data and build PMF model.\n",
    "    train, test = read_jester_data()\n",
    "    pmf = build_pmf_model(train)\n",
    "\n",
    "    # Find mode of posterior using optimization\n",
    "    with pmf:\n",
    "        tstart = time.time()\n",
    "        logging.info('finding PMF MAP using Powell optimization')\n",
    "        #start = pm.find_MAP(fmin=sp.optimize.fmin_powell)\n",
    "        start = pm.find_MAP()\n",
    "        elapsed = time.time() - tstart\n",
    "        logging.info('found PMF MAP in %d seconds' % int(elapsed))\n",
    "\n",
    "    # Build the modified BPMF model using same default params as PMF.\n",
    "    mod_bpmf = build_mod_bpmf_model(train)\n",
    "\n",
    "    # Use PMF MAP to initialize sampling for modified BPMF.\n",
    "    for key in mod_bpmf.test_point:\n",
    "        if key not in start:\n",
    "            start[key] = mod_bpmf.test_point[key]\n",
    "\n",
    "    # Attempt to sample with modified BPMF\n",
    "    # (this part raises PositiveDefiniteError when using the normal BPMF model).\n",
    "    with mod_bpmf:\n",
    "        nsamples = 100\n",
    "        njobs = 2\n",
    "        logging.info( 'drawing %d MCMC samples using %d jobs' % (nsamples, njobs))\n",
    "        step = pm.NUTS(scaling=start)\n",
    "        trace = pm.sample(nsamples, step, start=start, njobs=njobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 614.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.120942853091463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "with mod_bpmf:\n",
    "    ppc = pm.sample_posterior_predictive(trace, progressbar=True)\n",
    "nR = np.mean(ppc['R'],0)#three dims, calcuate the mean with the first dim \n",
    "\n",
    "def getrmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "rmses=[]\n",
    "for i in range(test.shape[0]):\n",
    "    for j in range(test.shape[1]):\n",
    "        if math.isnan(test[i][j]) == False:\n",
    "            rmse = getrmse(test[i][j],nR[i][j])\n",
    "            rmses.append(rmse)\n",
    "print (np.mean(rmses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
