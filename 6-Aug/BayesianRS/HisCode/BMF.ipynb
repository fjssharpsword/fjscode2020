{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-30 06:41:18,392]: Start BMF sampling\n",
      "Average Loss = 1.8907e+07: 100%|██████████| 1000/1000 [1:50:17<00:00,  7.50s/it] \n",
      "Finished [100%]: Average Loss = 1.8874e+07\n",
      "[2019-07-30 08:31:50,552]: Finished [100%]: Average Loss = 1.8874e+07\n",
      "[2019-07-30 10:02:11,957]: Complete BMF sampling in 12053 seconds\n",
      "100%|██████████| 500/500 [07:54<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@10: 0.09917218543046358, NDCG@10: 0.04579890812198485, At K 32 and Datasetml-1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-30 10:10:42,916]: Start BMF sampling\n",
      "Average Loss = 3.2794e+07: 100%|██████████| 1000/1000 [5:10:08<00:00, 16.38s/it] \n",
      "Finished [100%]: Average Loss = 3.2727e+07\n",
      "[2019-07-30 15:20:54,634]: Finished [100%]: Average Loss = 3.2727e+07\n",
      "[2019-07-30 18:17:50,197]: Complete BMF sampling in 29227 seconds\n",
      "100%|██████████| 500/500 [04:51<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@10: 0.10662251655629139, NDCG@10: 0.047823964501747106, At K 64 and Datasetml-1m\n",
      "Dataset Statistics: Interaction = 1445622, User = 55187, Item = 9916, Sparsity = 0.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-30 18:23:58,478]: Start BMF sampling\n",
      "Average Loss = 2.7927e+07: 100%|██████████| 1000/1000 [5:34:41<00:00,  6.73s/it] \n",
      "Finished [100%]: Average Loss = 2.788e+07\n",
      "[2019-07-30 23:58:48,472]: Finished [100%]: Average Loss = 2.788e+07\n",
      "[2019-07-31 00:59:35,659]: Complete BMF sampling in 23737 seconds\n",
      " 75%|███████▌  | 377/500 [10:31<03:23,  1.65s/it]"
     ]
    }
   ],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.30\n",
    "@function: Implementing BMF(Bayesian Matrix Factorization) By VI\n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, fileName, negNum):\n",
    "        self.negNum = negNum #negative sample ratio\n",
    "        self.trainList, self.maxu, self.maxi = self.getTrainset_as_list(fileName)\n",
    "        self.testList = self.getTestset_as_list(fileName)\n",
    "        \n",
    "    def getTrainset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".train.rating\" \n",
    "            data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "            data['rating']=data['rating'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_trainset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "    \n",
    "    def getTestset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".test.negative\" \n",
    "            dataList = []\n",
    "            with open(filePath, 'r') as fd:\n",
    "                line = fd.readline()\n",
    "                while line != None and line != '':\n",
    "                    arr = line.split('\\t')\n",
    "                    u = eval(arr[0])[0]\n",
    "                    dataList.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                    for i in arr[1:]:\n",
    "                        dataList.append([u, int(i), 0.0]) #99 negative items\n",
    "                    line = fd.readline()\n",
    "            return dataList\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_testset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList\n",
    "        \n",
    "    def list_to_matrix(self):              \n",
    "        dataMat = np.zeros([self.maxu, self.maxi], dtype=np.float32)\n",
    "        for u,i,r in self.trainList:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self):\n",
    "        dataDict = {}\n",
    "        for u,i,r in self.trainList:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def getInstances(self, isTest=False):\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        if isTest==True: #test\n",
    "            for u, i, r in self.testList:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "        else:#train\n",
    "            for u, i, r in self.trainList:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "            #negative samples\n",
    "            dataDict = self.list_to_dict()\n",
    "            for j in range(len(self.trainList)*self.negNum):\n",
    "                u = np.random.randint(self.maxu)\n",
    "                i = np.random.randint(self.maxi)\n",
    "                while (u, i) in dataDict:\n",
    "                    u = np.random.randint(self.maxu)\n",
    "                    i = np.random.randint(self.maxi)\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(0.0)) \n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "    \n",
    "def getHitRatio(ranklist, targetItem):\n",
    "    for item in ranklist:\n",
    "        if item == targetItem:\n",
    "            return 1\n",
    "    return 0\n",
    "def getNDCG(ranklist, targetItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == targetItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO,format='[%(asctime)s]: %(message)s')\n",
    "    for fileName in ['ml-1m', 'pinterest-20', 'kb-cc']:\n",
    "        dataset = DataSet(fileName=fileName, negNum=4)#loading dataset\n",
    "        #get the trainset and testset\n",
    "        train_u, train_i, train_r = dataset.getInstances(isTest=False)\n",
    "        assert(len(train_u) == len(train_i) and len(train_i) == len(train_r)) \n",
    "        shuffled_idx = np.random.permutation(np.arange(len(train_u)))\n",
    "        train_u = train_u[shuffled_idx]\n",
    "        train_i = train_i[shuffled_idx]\n",
    "        train_r = train_r[shuffled_idx]\n",
    "        test_u, test_i, test_r = dataset.getInstances(isTest=True)\n",
    "        assert(len(test_u) == len(test_i) and len(test_i) == len(test_r))\n",
    "        #R = dataset.list_to_matrix()\n",
    "        for K in [32, 64]:#[8, 16, 32, 64]\n",
    "            x_u = theano.shared(train_u)\n",
    "            x_i = theano.shared(train_i)\n",
    "            y_r = theano.shared(train_r)\n",
    "            with pm.Model() as bmf:#bulid probabilistic model\n",
    "                # Creating the model\n",
    "                P = pm.Normal('P', mu=0, sd=1, shape=(dataset.maxu,K))\n",
    "                Q = pm.Normal('Q', mu=0, sd=1, shape=(dataset.maxi,K))\n",
    "                #R = pm.Deterministic('R', tt.dot(P,Q))#pm.math.dot\n",
    "                #tY = pm.Deterministic('tY ',[R[x_u[j]][x_i[j]] for j in range(y_r.eval().shape[0])])\n",
    "                #tY =  pm.Deterministic('tY', pm.math.sum(tt.mul(P[x_u,:].T,Q[x_i,:].T), axis=1, keepdims=True))\n",
    "                tY = pm.Deterministic('tY', pm.math.sum(P[x_u,:]*Q[x_i,:], axis=1))\n",
    "                #nY = pm.Deterministic('nY', pm.math.sigmoid(tY))\n",
    "                # likelihood of observed data\n",
    "                #Y = pm.Bernoulli('Y', nY, observed=y_r)#total_size=y_r.eval().shape[0]\n",
    "                Y = pm.Normal('Y', mu=tY, sd=1, observed=y_r)\n",
    "                \n",
    "            with bmf: #train the probabilistic model by Bayesian inference\n",
    "                tstart = time.time()\n",
    "                logging.info('Start BMF sampling')\n",
    "                approx = pm.fit(n=1000, method=pm.ADVI())#obj_optimizer=pm.rmsprop(learning_rate=0.001)\n",
    "                trace = approx.sample(draws=500)\n",
    "                #start = pm.find_MAP()    \n",
    "                #step = pm.Metropolis()#NUTS()\n",
    "                #trace = pm.sample(1000, step, start=start)\n",
    "                elapsed = time.time() - tstart \n",
    "                logging.info('Complete BMF sampling in %d seconds' % int(elapsed))\n",
    "                \n",
    "            x_u.set_value(test_u)\n",
    "            x_i.set_value(test_i)\n",
    "            y_r.set_value(test_r)\n",
    "            with bmf:\n",
    "                ppc = pm.sample_posterior_predictive(trace, progressbar=True)\n",
    "                pre_r = ppc['Y'].mean(axis=0)\n",
    "            assert(pre_r.shape[0]==test_i.shape[0])\n",
    "            #every user have one positive item and 99 negative items\n",
    "            num_batches = len(test_r) // 100\n",
    "            hits = []\n",
    "            ndcgs = []\n",
    "            for i in range(num_batches):\n",
    "                test_i_batch = test_i[i*100: (i+1)*100]\n",
    "                pre_r_batch = pre_r[i*100: (i+1)*100]\n",
    "                map_item_score = {}\n",
    "                for j in range(100):\n",
    "                    map_item_score[test_i_batch[j]] = pre_r_batch[j]\n",
    "                ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "                hits.append(getHitRatio(ranklist, test_i_batch[0]))\n",
    "                ndcgs.append(getNDCG(ranklist, test_i_batch[0]))\n",
    "            hit, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "            print(\"HR@10: {}, NDCG@10: {}, At K {} and Dataset{}\".format(hit, ndcg, K, fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-30 06:20:40,334]: Start BMF sampling\n",
      "/usr/local/lib/python3.6/dist-packages/pymc3/tuning/starting.py:61: UserWarning: find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.\n",
      "  warnings.warn('find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.')\n",
      "logp = -3.7321e+06, ||grad|| = 0: 100%|██████████| 2/2 [00:10<00:00,  5.43s/it]      \n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "[2019-07-30 06:21:20,210]: Multiprocess sampling (4 chains in 4 jobs)\n",
      "CompoundStep\n",
      "[2019-07-30 06:21:20,218]: CompoundStep\n",
      ">Metropolis: [Q]\n",
      "[2019-07-30 06:21:20,224]: >Metropolis: [Q]\n",
      ">Metropolis: [P]\n",
      "[2019-07-30 06:21:20,231]: >Metropolis: [P]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-0fc33dc004bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_MAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetropolis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#NUTS()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Complete BMF sampling in %d seconds'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymc3/sampling.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(draws, step, init, n_init, start, trace, chain_idx, chains, cores, tune, progressbar, model, random_seed, discard_tuned_samples, compute_convergence_checks, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0m_print_step_hierarchy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m                 \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mp_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msample_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0m_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not pickle model, sampling singlethreaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymc3/sampling.py\u001b[0m in \u001b[0;36m_mp_sample\u001b[0;34m(draws, tune, step, chains, cores, chain, random_seed, start, progressbar, trace, model, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m     sampler = ps.ParallelSampler(\n\u001b[1;32m    964\u001b[0m         \u001b[0mdraws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m         chain, progressbar)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymc3/parallel_sampling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, draws, tune, chains, cores, seeds, start_points, step_method, start_chain_num, progressbar)\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0mdraws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstart_chain_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             )\n\u001b[0;32m--> 361\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         ]\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymc3/parallel_sampling.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0mdraws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstart_chain_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             )\n\u001b[0;32m--> 361\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         ]\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymc3/parallel_sampling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, draws, tune, step_method, chain, seed, start)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# We fork right away, so that the main process can start tqdm threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;31m# Something may have gone wrong during the fork / spawn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.28\n",
    "@function: Implementing BMF(Bayesian Matrix Factorization) By VI\n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, fileName, negNum):\n",
    "        self.negNum = negNum #negative sample ratio\n",
    "        self.trainList, self.maxu, self.maxi = self.getTrainset_as_list(fileName)\n",
    "        self.testList = self.getTestset_as_list(fileName)\n",
    "        \n",
    "    def getTrainset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".train.rating\" \n",
    "            data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "            data['rating']=data['rating'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_trainset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "    \n",
    "    def getTestset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".test.negative\" \n",
    "            dataList = []\n",
    "            with open(filePath, 'r') as fd:\n",
    "                line = fd.readline()\n",
    "                while line != None and line != '':\n",
    "                    arr = line.split('\\t')\n",
    "                    u = eval(arr[0])[0]\n",
    "                    dataList.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                    for i in arr[1:]:\n",
    "                        dataList.append([u, int(i), 0.0]) #99 negative items\n",
    "                    line = fd.readline()\n",
    "            return dataList\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_testset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList\n",
    "        \n",
    "    def list_to_matrix(self):              \n",
    "        dataMat = np.zeros([self.maxu, self.maxi], dtype=np.float32)\n",
    "        for u,i,r in self.trainList:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self):\n",
    "        dataDict = {}\n",
    "        for u,i,r in self.trainList:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def getInstances(self, isTest=False):\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        if isTest==True: #test\n",
    "            for u, i, r in self.testList:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "        else:#train\n",
    "            for u, i, r in self.trainList:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "            #negative samples\n",
    "            dataDict = self.list_to_dict()\n",
    "            for j in range(len(self.trainList)*self.negNum):\n",
    "                u = np.random.randint(self.maxu)\n",
    "                i = np.random.randint(self.maxi)\n",
    "                while (u, i) in dataDict:\n",
    "                    u = np.random.randint(self.maxu)\n",
    "                    i = np.random.randint(self.maxi)\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(0.0)) \n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "    \n",
    "def getHitRatio(ranklist, targetItem):\n",
    "    for item in ranklist:\n",
    "        if item == targetItem:\n",
    "            return 1\n",
    "    return 0\n",
    "def getNDCG(ranklist, targetItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == targetItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO,format='[%(asctime)s]: %(message)s')\n",
    "    for fileName in ['ml-1m', 'pinterest-20', 'kb-cc']:\n",
    "        dataset = DataSet(fileName=fileName, negNum=4)#loading dataset\n",
    "        #get the trainset and testset\n",
    "        train_u, train_i, train_r = dataset.getInstances(isTest=False)\n",
    "        assert(len(train_u) == len(train_i) and len(train_i) == len(train_r)) \n",
    "        shuffled_idx = np.random.permutation(np.arange(len(train_u)))\n",
    "        train_u = train_u[shuffled_idx]\n",
    "        train_i = train_i[shuffled_idx]\n",
    "        train_r = train_r[shuffled_idx]\n",
    "        test_u, test_i, test_r = dataset.getInstances(isTest=True)\n",
    "        assert(len(test_u) == len(test_i) and len(test_i) == len(test_r))\n",
    "        #R = dataset.list_to_matrix()\n",
    "        for K in [32, 64]:#[8, 16, 32, 64]\n",
    "            x_u = theano.shared(train_u)\n",
    "            x_i = theano.shared(train_i)\n",
    "            y_r = theano.shared(train_r)\n",
    "            with pm.Model() as bmf:#bulid probabilistic model\n",
    "                # Creating the model\n",
    "                P = pm.Normal('P', mu=0, sd=1, shape=(dataset.maxu,K))\n",
    "                Q = pm.Normal('Q', mu=0, sd=1, shape=(dataset.maxi,K))\n",
    "                #R = pm.Deterministic('R', tt.dot(P,Q))#pm.math.dot\n",
    "                #tY = pm.Deterministic('tY ',[R[x_u[j]][x_i[j]] for j in range(y_r.eval().shape[0])])\n",
    "                #tY =  pm.Deterministic('tY', pm.math.sum(tt.mul(P[x_u,:].T,Q[x_i,:].T), axis=1, keepdims=True))\n",
    "                tY = pm.Deterministic('tY', pm.math.sum(P[x_u,:]*Q[x_i,:], axis=1))\n",
    "                nY = pm.Deterministic('nY', pm.math.sigmoid(tY))\n",
    "                # likelihood of observed data\n",
    "                Y = pm.Bernoulli('Y', nY, observed=y_r)#total_size=y_r.eval().shape[0]\n",
    "                \n",
    "            with bmf: #train the probabilistic model by Bayesian inference\n",
    "                tstart = time.time()\n",
    "                logging.info('Start BMF sampling')\n",
    "                approx = pm.fit(n=1000, method=pm.ADVI(), obj_optimizer=pm.rmsprop(learning_rate=0.001))\n",
    "                trace = approx.sample(draws=500)\n",
    "                #start = pm.find_MAP()    \n",
    "                #step = pm.Metropolis()#NUTS()\n",
    "                #trace = pm.sample(1000, step, start=start)\n",
    "                elapsed = time.time() - tstart \n",
    "                logging.info('Complete BMF sampling in %d seconds' % int(elapsed))\n",
    "                \n",
    "            x_u.set_value(test_u)\n",
    "            x_i.set_value(test_i)\n",
    "            y_r.set_value(test_r)\n",
    "            with bmf:\n",
    "                ppc = pm.sample_posterior_predictive(trace, progressbar=True)\n",
    "                pre_r = ppc['Y'].mean(axis=0)\n",
    "            assert(pre_r.shape[0]==test_i.shape[0])\n",
    "            #every user have one positive item and 99 negative items\n",
    "            num_batches = len(test_r) // 100\n",
    "            hits = []\n",
    "            ndcgs = []\n",
    "            for i in range(num_batches):\n",
    "                test_i_batch = test_i[i*100: (i+1)*100]\n",
    "                pre_r_batch = pre_r[i*100: (i+1)*100]\n",
    "                map_item_score = {}\n",
    "                for j in range(100):\n",
    "                    map_item_score[test_i_batch[j]] = pre_r_batch[j]\n",
    "                ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "                hits.append(getHitRatio(ranklist, test_i_batch[0]))\n",
    "                ndcgs.append(getNDCG(ranklist, test_i_batch[0]))\n",
    "            hit, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "            print(\"HR@10: {}, NDCG@10: {}, At K {} and Dataset{}\".format(hit, ndcg, K, fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P -88809.37315944665\n",
      "Q -54491.28820135786\n",
      "[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "point = approx.groups[0].bij.rmap(approx.params[0].eval())\n",
    "for var in bmf.free_RVs:\n",
    "    print(var.name, var.logp(point))\n",
    "print (nY.tag.test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 55186\n",
      "\tItem Num: 9915\n",
      "\tData Size: 1445622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-24 02:10:26,486]: building the BMF model\n",
      "[2019-07-24 02:41:46,664]: done building BMF model\n",
      "[2019-07-24 02:41:46,726]: Start BMF sampling\n",
      "Average Loss = 8.0852e+09:  29%|██▊       | 286/1000 [6:50:06<14:23:39, 72.58s/it] \n",
      "Interrupted at 286 [28%]: Average Loss = 8.0742e+09\n",
      "[2019-07-24 09:42:26,364]: Interrupted at 286 [28%]: Average Loss = 8.0742e+09\n"
     ]
    }
   ],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.22\n",
    "@function: Implementing BMF(Bayesian Matrix Factorization) By VI\n",
    "           Dataset: Pinterest-20\n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as t\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "def getTraindata():\n",
    "    data = []\n",
    "    filePath = '/data/fjsdata/ctKngBase/ml/pinterest-20.train.rating'\n",
    "    u = 0\n",
    "    i = 0\n",
    "    maxr = 0.0\n",
    "    with open(filePath, 'r') as f:\n",
    "        for line in f:\n",
    "            if line:\n",
    "                lines = line[:-1].split(\"\\t\")\n",
    "                user = int(lines[0])\n",
    "                item = int(lines[1])\n",
    "                score = float(lines[2])\n",
    "                data.append((user, item, score))\n",
    "                if user > u: u = user\n",
    "                if item > i: i = item\n",
    "                if score > maxr: maxr = score\n",
    "    print(\"Loading Success!\\n\"\n",
    "                  \"Data Info:\\n\"\n",
    "                  \"\\tUser Num: {}\\n\"\n",
    "                  \"\\tItem Num: {}\\n\"\n",
    "                  \"\\tData Size: {}\".format(u, i, len(data)))\n",
    "    \n",
    "    R = np.zeros([u+1, i+1], dtype=np.float32)\n",
    "    for i in data:\n",
    "        user = i[0]\n",
    "        item = i[1]\n",
    "        rating = i[2]\n",
    "        R[user][item] = rating\n",
    "    return R\n",
    "\n",
    "def getTestdata():\n",
    "    testset = []\n",
    "    filePath = '/data/fjsdata/ctKngBase/ml/pinterest-20.test.negative'\n",
    "    with open(filePath, 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            testset.append([u, eval(arr[0])[1]])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                testset.append([u, int(i)]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return testset\n",
    "\n",
    "def getHitRatio(ranklist, targetItem):\n",
    "    for item in ranklist:\n",
    "        if item == targetItem:\n",
    "            return 1\n",
    "    return 0\n",
    "def getNDCG(ranklist, targetItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == targetItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def build_BMF(R, K, alpha=2, std=0.01):\n",
    "    \n",
    "    alpha_u = 1 / R.var(axis=1).mean()\n",
    "    alpha_v = 1 / R.var(axis=0).mean()\n",
    "\n",
    "    logging.info('building the BMF model')\n",
    "    n, m = R.shape\n",
    "    with pm.Model() as bmf:\n",
    "        U = pm.MvNormal('U', mu=0, tau=alpha_u * np.eye(K), shape=(n, K), testval=np.random.randn(n, K) * std)\n",
    "        V = pm.MvNormal('V', mu=0, tau=alpha_v * np.eye(K), shape=(m, K), testval=np.random.randn(m, K) * std)\n",
    "        nR = pm.Normal('nR', mu=t.dot(U, V.T), tau=alpha * np.ones(R.shape),observed=R)\n",
    "    logging.info('done building BMF model')\n",
    "    return bmf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO,format='[%(asctime)s]: %(message)s')\n",
    "    \n",
    "    # Read data and build BMF model.\n",
    "    R = getTraindata()\n",
    "    bmf = build_BMF(R, K=64)#dim is the number of latent factors\n",
    "\n",
    "    with bmf:# sample with BMF\n",
    "        tstart = time.time()\n",
    "        logging.info('Start BMF sampling')\n",
    "        inference = pm.ADVI()\n",
    "        approx = pm.fit(n=1000, method=inference)\n",
    "        trace = approx.sample(draws=500)\n",
    "        elapsed = time.time() - tstart    \n",
    "        logging.info('Complete BMF sampling in %d seconds' % int(elapsed))\n",
    "        \n",
    "    with bmf:#evaluation\n",
    "        testset = getTestdata()\n",
    "        ppc = pm.sample_posterior_predictive(trace, progressbar=True)\n",
    "        nR = np.mean(ppc['nR'],0)#three dims, calcuate the mean with the first dim for posterior\n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        prev_u = testset[0][0]\n",
    "        pos_i = testset[0][1]\n",
    "        scorelist = []\n",
    "        for u, i in testset:\n",
    "            if prev_u == u:\n",
    "                scorelist.append([i,nR[u,i]])\n",
    "            else:\n",
    "                map_item_score = {}\n",
    "                for item, rate in scorelist: #turn dict\n",
    "                    map_item_score[item] = rate\n",
    "                ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "                hr = getHitRatio(ranklist, pos_i)\n",
    "                hits.append(hr)\n",
    "                ndcg = getNDCG(ranklist, pos_i)\n",
    "                ndcgs.append(ndcg)\n",
    "                #next user\n",
    "                scorelist = []\n",
    "                prev_u = u\n",
    "                pos_i = i\n",
    "                scorelist.append([i,nR[u,i]])\n",
    "        hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print(\"HR@10: {}, NDCG@10: {}, At K {}\".format(hitratio, ndcg, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6039\n",
      "\tItem Num: 3705\n",
      "\tData Size: 994169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-23 23:27:45,431]: building the BMF model\n",
      "[2019-07-23 23:28:43,841]: done building BMF model\n",
      "[2019-07-23 23:28:43,846]: Start BMF sampling\n",
      "Average Loss = 2.7484e+08: 100%|██████████| 1000/1000 [53:55<00:00,  3.06s/it]\n",
      "Finished [100%]: Average Loss = 2.7423e+08\n",
      "[2019-07-24 00:23:00,032]: Finished [100%]: Average Loss = 2.7423e+08\n",
      "[2019-07-24 00:24:37,526]: Complete BMF sampling in 3353 seconds\n",
      "100%|██████████| 500/500 [1:13:32<00:00,  7.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@10: 0.10200364298724955, NDCG@10: 0.04524941138321496, At K 64\n"
     ]
    }
   ],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.22\n",
    "@function: Implementing BMF(Bayesian Matrix Factorization) By VI\n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as t\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "def getTraindata():\n",
    "    data = []\n",
    "    filePath = '/data/fjsdata/ctKngBase/ml/ml-1m.train.rating'\n",
    "    u = 0\n",
    "    i = 0\n",
    "    maxr = 0.0\n",
    "    with open(filePath, 'r') as f:\n",
    "        for line in f:\n",
    "            if line:\n",
    "                lines = line[:-1].split(\"\\t\")\n",
    "                user = int(lines[0])\n",
    "                item = int(lines[1])\n",
    "                score = float(lines[2])\n",
    "                data.append((user, item, score))\n",
    "                if user > u: u = user\n",
    "                if item > i: i = item\n",
    "                if score > maxr: maxr = score\n",
    "    print(\"Loading Success!\\n\"\n",
    "                  \"Data Info:\\n\"\n",
    "                  \"\\tUser Num: {}\\n\"\n",
    "                  \"\\tItem Num: {}\\n\"\n",
    "                  \"\\tData Size: {}\".format(u, i, len(data)))\n",
    "    \n",
    "    R = np.zeros([u+1, i+1], dtype=np.float32)\n",
    "    for i in data:\n",
    "        user = i[0]\n",
    "        item = i[1]\n",
    "        rating = i[2]\n",
    "        R[user][item] = rating\n",
    "    return R\n",
    "\n",
    "def getTestdata():\n",
    "    testset = []\n",
    "    filePath = '/data/fjsdata/ctKngBase/ml/ml-1m.test.negative'\n",
    "    with open(filePath, 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            testset.append([u, eval(arr[0])[1]])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                testset.append([u, int(i)]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return testset\n",
    "\n",
    "def getHitRatio(ranklist, targetItem):\n",
    "    for item in ranklist:\n",
    "        if item == targetItem:\n",
    "            return 1\n",
    "    return 0\n",
    "def getNDCG(ranklist, targetItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == targetItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def build_BMF(R, K, alpha=2, std=0.01):\n",
    "    \n",
    "    alpha_u = 1 / R.var(axis=1).mean()\n",
    "    alpha_v = 1 / R.var(axis=0).mean()\n",
    "\n",
    "    logging.info('building the BMF model')\n",
    "    n, m = R.shape\n",
    "    with pm.Model() as bmf:\n",
    "        U = pm.MvNormal('U', mu=0, tau=alpha_u * np.eye(K), shape=(n, K), testval=np.random.randn(n, K) * std)\n",
    "        V = pm.MvNormal('V', mu=0, tau=alpha_v * np.eye(K), shape=(m, K), testval=np.random.randn(m, K) * std)\n",
    "        nR = pm.Normal('nR', mu=t.dot(U, V.T), tau=alpha * np.ones(R.shape),observed=R)\n",
    "    logging.info('done building BMF model')\n",
    "    return bmf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO,format='[%(asctime)s]: %(message)s')\n",
    "    \n",
    "    # Read data and build BMF model.\n",
    "    R = getTraindata()\n",
    "    bmf = build_BMF(R, K=64)#dim is the number of latent factors\n",
    "\n",
    "    with bmf:# sample with BMF\n",
    "        tstart = time.time()\n",
    "        logging.info('Start BMF sampling')\n",
    "        inference = pm.ADVI()\n",
    "        approx = pm.fit(n=1000, method=inference)\n",
    "        trace = approx.sample(draws=500)\n",
    "        elapsed = time.time() - tstart    \n",
    "        logging.info('Complete BMF sampling in %d seconds' % int(elapsed))\n",
    "        \n",
    "    with bmf:#evaluation\n",
    "        testset = getTestdata()\n",
    "        ppc = pm.sample_posterior_predictive(trace, progressbar=True)\n",
    "        nR = np.mean(ppc['nR'],0)#three dims, calcuate the mean with the first dim for posterior\n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        prev_u = testset[0][0]\n",
    "        pos_i = testset[0][1]\n",
    "        scorelist = []\n",
    "        for u, i in testset:\n",
    "            if prev_u == u:\n",
    "                scorelist.append([i,nR[u,i]])\n",
    "            else:\n",
    "                map_item_score = {}\n",
    "                for item, rate in scorelist: #turn dict\n",
    "                    map_item_score[item] = rate\n",
    "                ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "                hr = getHitRatio(ranklist, pos_i)\n",
    "                hits.append(hr)\n",
    "                ndcg = getNDCG(ranklist, pos_i)\n",
    "                ndcgs.append(ndcg)\n",
    "                #next user\n",
    "                scorelist = []\n",
    "                prev_u = u\n",
    "                pos_i = i\n",
    "                scorelist.append([i,nR[u,i]])\n",
    "        hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print(\"HR@10: {}, NDCG@10: {}, At K {}\".format(hitratio, ndcg, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp = -7.7694e+08, ||grad|| = 1.0495e+07: 100%|██████████| 9/9 [00:00<00:00, 41.16it/s]   \n",
      "Only 100 samples in chain.\n",
      "Multiprocess sampling (2 chains in 8 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [Q]\n",
      ">Metropolis: [P]\n",
      "Sampling 2 chains: 100%|██████████| 1200/1200 [00:04<00:00, 299.73draws/s]\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "  6%|▌         | 11/200 [00:00<00:01, 102.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 11, 698)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 86.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 11, 698)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#矩阵分解R=PQ，推荐概率模型MCMC采样-直接矩阵采样\n",
    "import theano\n",
    "import pymc3 as pm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#1.数据集处理\n",
    "#http://files.grouplens.org/datasets/movielens/ml-20m-README.html\n",
    "#the following format of file ratings.csv: userId,movieId,rating,timestamp\n",
    "#The lines within this file are ordered first by userId, then, within user, by movieId.\n",
    "#Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "#Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "data = pd.read_csv(\"/data/fjsdata/BayesianRS/ml-20m/ratings.csv\",sep=',',low_memory=False,iterator =True)\n",
    "data = data.get_chunk(1000)\n",
    "#将userId和movieId全部标准编号\n",
    "le = LabelEncoder()\n",
    "data = data.apply(le.fit_transform)\n",
    "#2.构建U-I矩阵\n",
    "uNum = len(data['userId'].unique())#统计用户数\n",
    "iNum = len(data['movieId'].unique())#统计电影数\n",
    "UI = np.zeros((uNum, iNum))#转成R矩阵，非常稀疏\n",
    "for index, row in data.iterrows(): # 获取每行的值\n",
    "    UI[int(row['userId'])][int(row['movieId'])] = row['rating']\n",
    "#2.构建概率模型\n",
    "#概率模型参数设置\n",
    "mean= data['rating'].max()/2 #正态分布的均值和方差\n",
    "k = 100 #隐因子数\n",
    "Y_output = theano.shared(UI)#转numpy array\n",
    "with pm.Model() as BMF_model:\n",
    "    # Creating the model\n",
    "    P = pm.Normal('P', mu=mean, sd=mean, shape=(uNum,k))\n",
    "    Q = pm.Normal('Q', mu=mean, sd=mean, shape=(k,iNum))\n",
    "    R = pm.Deterministic('R', tt.dot(P,Q))\n",
    "    Y = pm.Normal('Y',mu=R, sd=mean, observed=Y_output)\n",
    "#3.后验分布计算  \n",
    "with BMF_model:        \n",
    "    start=pm.find_MAP()  # 参数初猜\n",
    "    #二值变量：指定 BinaryMetropolis  离散变量：指定 Metropolis  连续变量：指定 NUTS\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(100,start=start,step=step,chains=2,cores=8)\n",
    "\n",
    "print (trace['R'].shape) #直接用于推荐\n",
    "with BMF_model:\n",
    "    ppc = pm.sample_posterior_predictive(trace, progressbar=True)\n",
    "print (ppc['Y'].shape) #直接用于推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp = -2,280.5, ||grad|| = 0: 100%|██████████| 2/2 [00:00<00:00, 70.19it/s]\n",
      "Only 100 samples in chain.\n",
      "Multiprocess sampling (2 chains in 8 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [Q]\n",
      ">Metropolis: [P]\n",
      "Sampling 2 chains: 100%|██████████| 1200/1200 [00:03<00:00, 365.28draws/s]\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "100%|██████████| 200/200 [00:00<00:00, 2457.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n",
      "RMSE：0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#矩阵分解R=PQ，推荐概率模型MCMC采样-似然函数是Bernoulli\n",
    "import theano\n",
    "import pymc3 as pm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#1.数据集处理\n",
    "#http://files.grouplens.org/datasets/movielens/ml-20m-README.html\n",
    "#the following format of file ratings.csv: userId,movieId,rating,timestamp\n",
    "#The lines within this file are ordered first by userId, then, within user, by movieId.\n",
    "#Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "#Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "data = pd.read_csv(\"/data/fjsdata/BayesianRS/ml-20m/ratings.csv\",sep=',',low_memory=False,iterator =True)\n",
    "data = data.get_chunk(100)\n",
    "#将userId和movieId全部标准编号\n",
    "le = LabelEncoder()\n",
    "data = data.apply(le.fit_transform)\n",
    "data['rating'] = 1\n",
    "#抽样10%比例测试\n",
    "test = data.sample(frac=0.1)\n",
    "#2.构建概率模型\n",
    "uNum = len(data['userId'].unique())#统计用户数\n",
    "iNum = len(data['movieId'].unique())#统计电影数\n",
    "mean= data['rating'].max()/2 #正态分布的均值和方差\n",
    "k = 100 #隐因子数\n",
    "X_input = theano.shared(data[['userId','movieId']].values)#转numpy array\n",
    "Y_output = theano.shared(data['rating'].values)#转numpy array\n",
    "with pm.Model() as BMF_model:\n",
    "    # Creating the model\n",
    "    P = pm.Normal('P', mu=mean, sd=mean, shape=(uNum,k))\n",
    "    Q = pm.Normal('Q', mu=mean, sd=mean, shape=(k,iNum))\n",
    "    R = tt.dot(P,Q)\n",
    "    rY = []\n",
    "    for row in X_input.get_value(): # 获取每行的值\n",
    "        rr = R[int(row[0])][int(row[1])]#userId=0,movieId=1\n",
    "        rY.append(rr)\n",
    "    rY = pm.Deterministic('rY',pm.math.sigmoid(rY))\n",
    "    Y = pm.Bernoulli('Y', rY, observed=Y_output.get_value())\n",
    "#3.后验分布计算  \n",
    "with BMF_model:        \n",
    "    start=pm.find_MAP()  # 参数初猜\n",
    "    #二值变量：指定 BinaryMetropolis  离散变量：指定 Metropolis  连续变量：指定 NUTS\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(100,start=start,step=step,chains=2,cores=8)\n",
    "    \n",
    "#4.后验预测  \n",
    "#X_input.set_value(test[['userId','movieId']].values)#转numpy array\n",
    "#Y_output.set_value(test['rating'].values)\n",
    "with BMF_model:\n",
    "    ppc = pm.sample_posterior_predictive(trace)\n",
    "    pred = ppc['Y'].mean(axis=0)\n",
    "    print(pred)\n",
    "    \n",
    "print ('RMSE：%f'% mean_squared_error(Y_output.get_value(),pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
