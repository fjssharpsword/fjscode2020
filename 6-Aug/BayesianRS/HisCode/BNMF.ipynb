{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 1445622, User = 55187, Item = 9916, Sparsity = 0.0026\n",
      "start building the Multi-layer non-linear projection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 547234292 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building the the Multi-layer non-linear projection\n",
      "start training the BNCF model\n",
      "7050 / 7059 : shape = 7220224 start building the Bayesian probabilistic model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 5.3427e+06: 100%|██████████| 1000/1000 [16:50<00:00,  1.01s/it]\n",
      "Finished [100%]: Average Loss = 5.3331e+06\n",
      "I0729 12:23:47.493609 140210647013120 inference.py:248] Finished [100%]: Average Loss = 5.3331e+06\n",
      "100%|██████████| 500/500 [21:07<00:00,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building the Bayesian probabilistic model\n",
      "7050 / 7059 : loss = 0.1283096522092819228Completed training the BNCF model in 67204 seconds\n",
      "\n",
      "Mean loss in this epoch is: 5.747372627258301\n",
      "5380 / 5390 : shape = 5510144 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [13:30<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@10: 0.2364143729501513, NDCG@10: 0.12314803893484755, At K 8 and Dataset pinterest-20\n",
      "start training the BNCF model\n",
      "7050 / 7059 : shape = 7220224 start building the Bayesian probabilistic model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 5.3131e+06: 100%|██████████| 1000/1000 [20:00<00:00,  1.21s/it]\n",
      "Finished [100%]: Average Loss = 5.3073e+06\n",
      "I0730 07:34:00.587919 140210647013120 inference.py:248] Finished [100%]: Average Loss = 5.3073e+06\n",
      "100%|██████████| 500/500 [17:11<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building the Bayesian probabilistic model\n",
      "3790 / 7059 : loss = 0.13391141593456268"
     ]
    }
   ],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.29\n",
    "@function: BNMF(Bayesian Neural Matrix Factorization) \n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import heapq\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import tensorflow as tf\n",
    "import theano.tensor as tt\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, fileName, negNum):\n",
    "        self.negNum = negNum #negative sample ratio\n",
    "        self.trainList, self.maxu, self.maxi = self.getTrainset_as_list(fileName)\n",
    "        self.testList = self.getTestset_as_list(fileName)\n",
    "        \n",
    "    def getTrainset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".train.rating\" \n",
    "            data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "            data['rating']=data['rating'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_trainset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "    \n",
    "    def getTestset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".test.negative\" \n",
    "            dataList = []\n",
    "            with open(filePath, 'r') as fd:\n",
    "                line = fd.readline()\n",
    "                while line != None and line != '':\n",
    "                    arr = line.split('\\t')\n",
    "                    u = eval(arr[0])[0]\n",
    "                    dataList.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                    for i in arr[1:]:\n",
    "                        dataList.append([u, int(i), 0.0]) #99 negative items\n",
    "                    line = fd.readline()\n",
    "            return dataList\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_testset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList\n",
    "        \n",
    "    def list_to_matrix(self):              \n",
    "        dataMat = np.zeros([self.maxu, self.maxi], dtype=np.float32)\n",
    "        for u,i,r in self.trainList:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self):\n",
    "        dataDict = {}\n",
    "        for u,i,r in self.trainList:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def getInstances(self, isTest=False):\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        if isTest==True: #test\n",
    "            for u, i, r in self.testList:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "        else:#train\n",
    "            for u, i, r in self.trainList:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "            #negative samples\n",
    "            dataDict = self.list_to_dict()\n",
    "            for j in range(len(self.trainList)*self.negNum):\n",
    "                u = np.random.randint(self.maxu)\n",
    "                i = np.random.randint(self.maxi)\n",
    "                while (u, i) in dataDict:\n",
    "                    u = np.random.randint(self.maxu)\n",
    "                    i = np.random.randint(self.maxi)\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(0.0)) \n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "    \n",
    "class BNMF:\n",
    "    def __init__(self, dataset, K=8):\n",
    "        self.shape = [dataset.maxu, dataset.maxi]\n",
    "        #get the trainset and testset\n",
    "        self.train_u, self.train_i, self.train_r = dataset.getInstances(isTest=False)\n",
    "        assert(len(self.train_u) == len(self.train_i) and len(self.train_i) == len(self.train_r))\n",
    "        shuffled_idx = np.random.permutation(np.arange(len(self.train_u)))\n",
    "        self.train_u = self.train_u[shuffled_idx]\n",
    "        self.train_i = self.train_i[shuffled_idx]\n",
    "        self.train_r = self.train_r[shuffled_idx]\n",
    "        self.test_u, self.test_i, self.test_r = dataset.getInstances(isTest=True)\n",
    "        assert(len(self.test_u) == len(self.test_i) and len(self.test_i) == len(self.test_r))\n",
    "        \n",
    "        #initialize\n",
    "        #K is number of latent factors\n",
    "        self.userLayer = [512, K]\n",
    "        self.itemLayer = [512, K]\n",
    "        self.batchSize = 1024\n",
    "        self.lr = 0.001\n",
    "        tf.reset_default_graph()\n",
    "        self._build_MLP()\n",
    "        self._init_sess()\n",
    "        \n",
    "    def _init_sess(self):\n",
    "        #define seesion\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.config.allow_soft_placement = True\n",
    "        self.sess = tf.Session(config=self.config)\n",
    "        self.sess.run(tf.global_variables_initializer(), feed_dict={self.R: dataset.list_to_matrix()})\n",
    "        \n",
    "    def _build_MLP(self):\n",
    "        print('start building the Multi-layer non-linear projection')\n",
    "        # add placeholder\n",
    "        self.user = tf.placeholder(tf.int32)\n",
    "        self.item = tf.placeholder(tf.int32)\n",
    "        self.rate = tf.placeholder(tf.float32)\n",
    "        self.R = tf.placeholder(tf.float32, shape=(self.shape[0], self.shape[1]))\n",
    "        user_item_embedding = tf.convert_to_tensor(tf.Variable(self.R))\n",
    "        item_user_embedding = tf.transpose(user_item_embedding)\n",
    "        user_input = tf.nn.embedding_lookup(user_item_embedding, self.user)\n",
    "        item_input = tf.nn.embedding_lookup(item_user_embedding, self.item)\n",
    "        \n",
    "        def init_variable(shape, name):\n",
    "            return tf.Variable(tf.truncated_normal(shape=shape, dtype=tf.float32, stddev=0.01), name=name)\n",
    "\n",
    "        with tf.name_scope(\"User_Layer\"):\n",
    "            user_W1 = init_variable([self.shape[1], self.userLayer[0]], \"user_W1\")\n",
    "            user_out = tf.matmul(user_input, user_W1)\n",
    "            for i in range(0, len(self.userLayer)-1):\n",
    "                W = init_variable([self.userLayer[i], self.userLayer[i+1]], \"user_W\"+str(i+2))\n",
    "                b = init_variable([self.userLayer[i+1]], \"user_b\"+str(i+2))\n",
    "                user_out = tf.nn.relu(tf.add(tf.matmul(user_out, W), b))\n",
    "\n",
    "        with tf.name_scope(\"Item_Layer\"):\n",
    "            item_W1 = init_variable([self.shape[0], self.itemLayer[0]], \"item_W1\")\n",
    "            item_out = tf.matmul(item_input, item_W1)\n",
    "            for i in range(0, len(self.itemLayer)-1):\n",
    "                W = init_variable([self.itemLayer[i], self.itemLayer[i+1]], \"item_W\"+str(i+2))\n",
    "                b = init_variable([self.itemLayer[i+1]], \"item_b\"+str(i+2))\n",
    "                item_out = tf.nn.relu(tf.add(tf.matmul(item_out, W), b))\n",
    "           \n",
    "        self.r_ui = tf.reduce_sum(tf.multiply(user_out, item_out), axis=1, keepdims=False)\n",
    "        self.loss = tf.reduce_sum(tf.losses.mean_squared_error(labels = self.rate, predictions=self.r_ui))\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_step = optimizer.minimize(self.loss)\n",
    "        print('done building the the Multi-layer non-linear projection')\n",
    "        \n",
    "    def _build_BPF(self):\n",
    "        print('start building the Bayesian probabilistic model')  \n",
    "        self.x_u = theano.shared(self.train_u)\n",
    "        self.x_i = theano.shared(self.train_i)\n",
    "        self.y_r = theano.shared(self.train_r)\n",
    "        self.y_r_ui = theano.shared(np.array(self.nn_r_ui))\n",
    "        assert(len(self.y_r.get_value())==len(self.y_r_ui.get_value()))\n",
    "        with pm.Model() as self.bncf:#define the prior and likelihood\n",
    "            b_u = pm.Normal('b_u', 0, sd=1, shape=self.shape[0])\n",
    "            b_i = pm.Normal('b_i', 0, sd=1, shape=self.shape[1])\n",
    "            u = pm.Normal('u', 0, sd=1)\n",
    "            tY = pm.Deterministic('tY', tt.add(tt.add(tt.add(b_u[self.x_u],b_i[self.x_i]),self.y_r_ui),u))\n",
    "            #tY = pm.Deterministic('tY', ((b_u[self.x_u]+b_i[self.x_i])+self.y_r_ui)+u)#b_u+b_i+u+nn_r_ui\n",
    "            nY = pm.Deterministic('nY', pm.math.sigmoid(tY))\n",
    "            # likelihood of observed data\n",
    "            Y = pm.Bernoulli('Y', nY, observed=self.y_r)#total_size=self.y_r.get_value().shape[0]\n",
    "        with self.bncf:#inference\n",
    "            approx = pm.fit(n=1000, method=pm.ADVI())\n",
    "            self.trace = approx.sample(draws=500)\n",
    "        with self.bncf: #posterior prediction\n",
    "            ppc = pm.sample_posterior_predictive(self.trace, progressbar=True)\n",
    "            self.by_r_ui = ppc['Y'].mean(axis=0)\n",
    "        print('done building the Bayesian probabilistic model')\n",
    "        \n",
    "    def train_BNMF(self, verbose=10):       \n",
    "        print('start training the BNCF model')\n",
    "        tstart = time.time()\n",
    "        \n",
    "        num_batches = len(self.train_u) // self.batchSize + 1\n",
    "        #1.traing r_ui in neural network \n",
    "        self.nn_r_ui=[]\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([len(self.train_u), (i+1)*self.batchSize])\n",
    "            train_u_batch = self.train_u[min_idx: max_idx]\n",
    "            train_i_batch = self.train_i[min_idx: max_idx]\n",
    "            #train_r_batch = self.train_r[min_idx: max_idx]\n",
    "            pre_r_ui_batch = self.sess.run(self.r_ui, feed_dict={self.user: train_u_batch, \\\n",
    "                                                                 self.item: train_i_batch})\n",
    "            self.nn_r_ui.extend(pre_r_ui_batch)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : shape = {} '.format(i, num_batches, len(self.nn_r_ui)))\n",
    "                sys.stdout.flush()\n",
    "        #2.training bias in Bayesian inference\n",
    "        self._build_BPF()\n",
    "        #3.training self.loss in neural network\n",
    "        losses = []\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([len(self.train_u), (i+1)*self.batchSize])\n",
    "            train_u_batch = self.train_u[min_idx: max_idx]\n",
    "            train_i_batch = self.train_i[min_idx: max_idx]\n",
    "            train_r_batch = self.by_r_ui[min_idx: max_idx]\n",
    "            _, tmp_loss = self.sess.run([self.train_step, self.loss], feed_dict={self.user: train_u_batch, \\\n",
    "                                                                                 self.item: train_i_batch, \\\n",
    "                                                                                 self.rate: train_r_batch})\n",
    "            losses.append(tmp_loss)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : loss = {}'.format(i, num_batches, np.mean(losses[-verbose:])))\n",
    "                sys.stdout.flush()\n",
    "        loss = np.mean(losses)  \n",
    "        elapsed = time.time() - tstart    \n",
    "        print('Completed training the BNCF model in %d seconds' % int(elapsed))\n",
    "        return loss\n",
    "           \n",
    "    def eval_BNMF(self, verbose=10):\n",
    "        def getHitRatio(ranklist, targetItem):\n",
    "            for item in ranklist:\n",
    "                if item == targetItem:\n",
    "                    return 1\n",
    "            return 0\n",
    "    \n",
    "        def getNDCG(ranklist, targetItem):\n",
    "            for i in range(len(ranklist)):\n",
    "                item = ranklist[i]\n",
    "                if item == targetItem:\n",
    "                    return math.log(2) / math.log(i+2)\n",
    "            return 0\n",
    "    \n",
    "        #1.get r_ui in neural network\n",
    "        num_batches = len(self.test_u) // self.batchSize + 1\n",
    "        #1.traing r_ui in neural network \n",
    "        self.nn_r_ui=[]\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([len(self.test_u), (i+1)*self.batchSize])\n",
    "            test_u_batch = self.test_u[min_idx: max_idx]\n",
    "            test_i_batch = self.test_i[min_idx: max_idx]\n",
    "            pre_r_ui_batch = self.sess.run(self.r_ui, feed_dict={self.user: test_u_batch, \\\n",
    "                                                                 self.item: test_i_batch})\n",
    "            self.nn_r_ui.extend(pre_r_ui_batch)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : shape = {} '.format(i, num_batches, len(self.nn_r_ui)))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "        #2. get biais in Bayesian inference\n",
    "        self.x_u.set_value(self.test_u)\n",
    "        self.x_i.set_value(self.test_i)\n",
    "        self.y_r.set_value(self.test_r)\n",
    "        self.y_r_ui.set_value(self.nn_r_ui)\n",
    "        with self.bncf:#evaluation\n",
    "            ppc = pm.sample_posterior_predictive(self.trace, progressbar=True) \n",
    "            pre_r = ppc['Y'].mean(axis=0)\n",
    "        assert(pre_r.shape[0]==self.test_i.shape[0])\n",
    "        #every user have one positive item and 99 negative items\n",
    "        num_batches = len(self.test_r) // 100\n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        for i in range(num_batches):\n",
    "            test_i_batch = self.test_i[i*100: (i+1)*100]\n",
    "            pre_r_batch = pre_r[i*100: (i+1)*100]\n",
    "            map_item_score = {}\n",
    "            for j in range(100):\n",
    "                map_item_score[test_i_batch[j]] = pre_r_batch[j]\n",
    "            ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "            hits.append(getHitRatio(ranklist, test_i_batch[0]))\n",
    "            ndcgs.append(getNDCG(ranklist, test_i_batch[0]))\n",
    "        hit, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        return hit, ndcg\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    for fileName in ['pinterest-20', 'kb-cc']:#['ml-1m', 'pinterest-20', 'kb-cc']:\n",
    "        dataset = DataSet(fileName=fileName, negNum=4)#loading dataset\n",
    "        for K in [8, 16, 32, 64]:\n",
    "            model = BNMF(dataset, K)\n",
    "            best_hr = 0.0\n",
    "            best_ndcg = 0.0\n",
    "            for epoch in range(2):\n",
    "                loss = model.train_BNMF()\n",
    "                print(\"\\nMean loss in this epoch is: {}\".format(loss))\n",
    "                hit, ndcg = model.eval_BNMF()\n",
    "                print(\"HR@10: {}, NDCG@10: {}, At K {} and Dataset {}\".format(hit, ndcg, K, fileName ))\n",
    "                if hit>best_hr: best_hr=hit\n",
    "                if ndcg>best_ndcg: best_ndcg=ndcg\n",
    "            print(\"Best HR@10: {}, Best NDCG@10: {}, At K {} and Dataset {}\".format(best_hr, best_ndcg, K, fileName ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.29\n",
    "@function: BNMF(Bayesian Neural Matrix Factorization) \n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import heapq\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import tensorflow as tf\n",
    "import theano.tensor as tt\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, fileName, negNum):\n",
    "        self.negNum = negNum #negative sample ratio\n",
    "        self.trainList, self.maxu, self.maxi = self.getTrainset_as_list(fileName)\n",
    "        self.testList = self.getTestset_as_list(fileName)\n",
    "        \n",
    "    def getTrainset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".train.rating\" \n",
    "            data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "            data['rating']=data['rating'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_trainset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "    \n",
    "    def getTestset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".test.negative\" \n",
    "            dataList = []\n",
    "            with open(filePath, 'r') as fd:\n",
    "                line = fd.readline()\n",
    "                while line != None and line != '':\n",
    "                    arr = line.split('\\t')\n",
    "                    u = eval(arr[0])[0]\n",
    "                    dataList.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                    for i in arr[1:]:\n",
    "                        dataList.append([u, int(i), 0.0]) #99 negative items\n",
    "                    line = fd.readline()\n",
    "            return dataList\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_testset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList\n",
    "        \n",
    "    def list_to_matrix(self):              \n",
    "        dataMat = np.zeros([self.maxu, self.maxi], dtype=np.float32)\n",
    "        for u,i,r in self.trainList:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self):\n",
    "        dataDict = {}\n",
    "        for u,i,r in self.trainList:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def getInstances(self, isTest=False):\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        if isTest==True: #test\n",
    "            for u, i, r in self.testList:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "        else:#train\n",
    "            for u, i, r in self.trainList:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "            #negative samples\n",
    "            dataDict = self.list_to_dict()\n",
    "            for j in range(len(self.trainList)*self.negNum):\n",
    "                u = np.random.randint(self.maxu)\n",
    "                i = np.random.randint(self.maxi)\n",
    "                while (u, i) in dataDict:\n",
    "                    u = np.random.randint(self.maxu)\n",
    "                    i = np.random.randint(self.maxi)\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(0.0)) \n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "    \n",
    "class BNMF:\n",
    "    def __init__(self, dataset, K=8):\n",
    "        self.shape = [dataset.maxu, dataset.maxi]\n",
    "        #get the trainset and testset\n",
    "        self.train_u, self.train_i, self.train_r = dataset.getInstances(isTest=False)\n",
    "        assert(len(self.train_u) == len(self.train_i) and len(self.train_i) == len(self.train_r))\n",
    "        shuffled_idx = np.random.permutation(np.arange(len(self.train_u)))\n",
    "        self.train_u = self.train_u[shuffled_idx]\n",
    "        self.train_i = self.train_i[shuffled_idx]\n",
    "        self.train_r = self.train_r[shuffled_idx]\n",
    "        self.test_u, self.test_i, self.test_r = dataset.getInstances(isTest=True)\n",
    "        assert(len(self.test_u) == len(self.test_i) and len(self.test_i) == len(self.test_r))\n",
    "        \n",
    "        #initialize\n",
    "        #K is number of latent factors\n",
    "        self.userLayer = [512, K]\n",
    "        self.itemLayer = [512, K]\n",
    "        self.batchSize = 1024\n",
    "        self.lr = 0.001\n",
    "        tf.reset_default_graph()\n",
    "        self._build_MLP(R = dataset.list_to_matrix())\n",
    "        self._init_sess()\n",
    "        \n",
    "    def _init_sess(self):\n",
    "        #define seesion\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.config.allow_soft_placement = True\n",
    "        self.sess = tf.Session(config=self.config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build_MLP(self, R):\n",
    "        print('start building the Multi-layer non-linear projection')\n",
    "        # add placeholder\n",
    "        self.user = tf.placeholder(tf.int32)\n",
    "        self.item = tf.placeholder(tf.int32)\n",
    "        self.rate = tf.placeholder(tf.float32)\n",
    "        user_item_embedding = tf.convert_to_tensor(R)\n",
    "        item_user_embedding = tf.transpose(user_item_embedding)\n",
    "        user_input = tf.nn.embedding_lookup(user_item_embedding, self.user)\n",
    "        item_input = tf.nn.embedding_lookup(item_user_embedding, self.item)\n",
    "        \n",
    "        def init_variable(shape, name):\n",
    "            return tf.Variable(tf.truncated_normal(shape=shape, dtype=tf.float32, stddev=0.01), name=name)\n",
    "\n",
    "        with tf.name_scope(\"User_Layer\"):\n",
    "            user_W1 = init_variable([self.shape[1], self.userLayer[0]], \"user_W1\")\n",
    "            user_out = tf.matmul(user_input, user_W1)\n",
    "            for i in range(0, len(self.userLayer)-1):\n",
    "                W = init_variable([self.userLayer[i], self.userLayer[i+1]], \"user_W\"+str(i+2))\n",
    "                b = init_variable([self.userLayer[i+1]], \"user_b\"+str(i+2))\n",
    "                user_out = tf.nn.relu(tf.add(tf.matmul(user_out, W), b))\n",
    "\n",
    "        with tf.name_scope(\"Item_Layer\"):\n",
    "            item_W1 = init_variable([self.shape[0], self.itemLayer[0]], \"item_W1\")\n",
    "            item_out = tf.matmul(item_input, item_W1)\n",
    "            for i in range(0, len(self.itemLayer)-1):\n",
    "                W = init_variable([self.itemLayer[i], self.itemLayer[i+1]], \"item_W\"+str(i+2))\n",
    "                b = init_variable([self.itemLayer[i+1]], \"item_b\"+str(i+2))\n",
    "                item_out = tf.nn.relu(tf.add(tf.matmul(item_out, W), b))\n",
    "           \n",
    "        self.r_ui = tf.reduce_sum(tf.multiply(user_out, item_out), axis=1, keepdims=False)\n",
    "        self.loss = tf.reduce_sum(tf.losses.mean_squared_error(labels = self.rate, predictions=self.r_ui))\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_step = optimizer.minimize(self.loss)\n",
    "        print('done building the the Multi-layer non-linear projection')\n",
    "        \n",
    "    def _build_BPF(self):\n",
    "        print('start building the Bayesian probabilistic model')  \n",
    "        self.x_u = theano.shared(self.train_u)\n",
    "        self.x_i = theano.shared(self.train_i)\n",
    "        self.y_r = theano.shared(self.train_r)\n",
    "        self.y_r_ui = theano.shared(np.array(self.nn_r_ui))\n",
    "        assert(len(self.y_r.get_value())==len(self.y_r_ui.get_value()))\n",
    "        with pm.Model() as self.bncf:#define the prior and likelihood\n",
    "            b_u = pm.Normal('b_u', 0, sd=1, shape=self.shape[0])\n",
    "            b_i = pm.Normal('b_i', 0, sd=1, shape=self.shape[1])\n",
    "            u = pm.Normal('u', 0, sd=1)\n",
    "            tY = pm.Deterministic('tY', tt.add(tt.add(tt.add(b_u[self.x_u],b_i[self.x_i]),self.y_r_ui),u))\n",
    "            #tY = pm.Deterministic('tY', ((b_u[self.x_u]+b_i[self.x_i])+self.y_r_ui)+u)#b_u+b_i+u+nn_r_ui\n",
    "            nY = pm.Deterministic('nY', pm.math.sigmoid(tY))\n",
    "            # likelihood of observed data\n",
    "            Y = pm.Bernoulli('Y', nY, observed=self.y_r)#total_size=self.y_r.get_value().shape[0]\n",
    "        with self.bncf:#inference\n",
    "            approx = pm.fit(n=1000, method=pm.ADVI())\n",
    "            self.trace = approx.sample(draws=500)\n",
    "        with self.bncf: #posterior prediction\n",
    "            ppc = pm.sample_posterior_predictive(self.trace, progressbar=True)\n",
    "            self.by_r_ui = ppc['Y'].mean(axis=0)\n",
    "        print('done building the Bayesian probabilistic model')\n",
    "        \n",
    "    def train_BNMF(self, verbose=10):       \n",
    "        print('start training the BNCF model')\n",
    "        tstart = time.time()\n",
    "        \n",
    "        num_batches = len(self.train_u) // self.batchSize + 1\n",
    "        #1.traing r_ui in neural network \n",
    "        self.nn_r_ui=[]\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([len(self.train_u), (i+1)*self.batchSize])\n",
    "            train_u_batch = self.train_u[min_idx: max_idx]\n",
    "            train_i_batch = self.train_i[min_idx: max_idx]\n",
    "            #train_r_batch = self.train_r[min_idx: max_idx]\n",
    "            pre_r_ui_batch = self.sess.run(self.r_ui, feed_dict={self.user: train_u_batch, \\\n",
    "                                                                 self.item: train_i_batch})\n",
    "            self.nn_r_ui.extend(pre_r_ui_batch)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : shape = {} '.format(i, num_batches, len(self.nn_r_ui)))\n",
    "                sys.stdout.flush()\n",
    "        #2.training bias in Bayesian inference\n",
    "        self._build_BPF()\n",
    "        #3.training self.loss in neural network\n",
    "        losses = []\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([len(self.train_u), (i+1)*self.batchSize])\n",
    "            train_u_batch = self.train_u[min_idx: max_idx]\n",
    "            train_i_batch = self.train_i[min_idx: max_idx]\n",
    "            train_r_batch = self.by_r_ui[min_idx: max_idx]\n",
    "            _, tmp_loss = self.sess.run([self.train_step, self.loss], feed_dict={self.user: train_u_batch, \\\n",
    "                                                                                 self.item: train_i_batch, \\\n",
    "                                                                                 self.rate: train_r_batch})\n",
    "            losses.append(tmp_loss)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : loss = {}'.format(i, num_batches, np.mean(losses[-verbose:])))\n",
    "                sys.stdout.flush()\n",
    "        loss = np.mean(losses)  \n",
    "        elapsed = time.time() - tstart    \n",
    "        print('Completed training the BNCF model in %d seconds' % int(elapsed))\n",
    "        return loss\n",
    "           \n",
    "    def eval_BNMF(self, verbose=10):\n",
    "        def getHitRatio(ranklist, targetItem):\n",
    "            for item in ranklist:\n",
    "                if item == targetItem:\n",
    "                    return 1\n",
    "            return 0\n",
    "    \n",
    "        def getNDCG(ranklist, targetItem):\n",
    "            for i in range(len(ranklist)):\n",
    "                item = ranklist[i]\n",
    "                if item == targetItem:\n",
    "                    return math.log(2) / math.log(i+2)\n",
    "            return 0\n",
    "    \n",
    "        #1.get r_ui in neural network\n",
    "        num_batches = len(self.test_u) // self.batchSize + 1\n",
    "        #1.traing r_ui in neural network \n",
    "        self.nn_r_ui=[]\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([len(self.test_u), (i+1)*self.batchSize])\n",
    "            test_u_batch = self.test_u[min_idx: max_idx]\n",
    "            test_i_batch = self.test_i[min_idx: max_idx]\n",
    "            pre_r_ui_batch = self.sess.run(self.r_ui, feed_dict={self.user: test_u_batch, \\\n",
    "                                                                 self.item: test_i_batch})\n",
    "            self.nn_r_ui.extend(pre_r_ui_batch)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : shape = {} '.format(i, num_batches, len(self.nn_r_ui)))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "        #2. get biais in Bayesian inference\n",
    "        self.x_u.set_value(self.test_u)\n",
    "        self.x_i.set_value(self.test_i)\n",
    "        self.y_r.set_value(self.test_r)\n",
    "        self.y_r_ui.set_value(self.nn_r_ui)\n",
    "        with self.bncf:#evaluation\n",
    "            ppc = pm.sample_posterior_predictive(self.trace, progressbar=True) \n",
    "            pre_r = ppc['Y'].mean(axis=0)\n",
    "        assert(pre_r.shape[0]==self.test_i.shape[0])\n",
    "        #every user have one positive item and 99 negative items\n",
    "        num_batches = len(self.test_r) // 100\n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        for i in range(num_batches):\n",
    "            test_i_batch = self.test_i[i*100: (i+1)*100]\n",
    "            pre_r_batch = pre_r[i*100: (i+1)*100]\n",
    "            map_item_score = {}\n",
    "            for j in range(100):\n",
    "                map_item_score[test_i_batch[j]] = pre_r_batch[j]\n",
    "            ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "            hits.append(getHitRatio(ranklist, test_i_batch[0]))\n",
    "            ndcgs.append(getNDCG(ranklist, test_i_batch[0]))\n",
    "        hit, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        return hit, ndcg\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    for fileName in ['ml-1m', 'pinterest-20', 'kb-cc']:\n",
    "        dataset = DataSet(fileName=fileName, negNum=4)#loading dataset\n",
    "        for K in [8, 16, 32, 64]:\n",
    "            model = BNMF(dataset, K)\n",
    "            best_hr = 0.0\n",
    "            best_ndcg = 0.0\n",
    "            for epoch in range(2):\n",
    "                loss = model.train_BNMF()\n",
    "                print(\"\\nMean loss in this epoch is: {}\".format(loss))\n",
    "                hit, ndcg = model.eval_BNMF()\n",
    "                print(\"HR@10: {}, NDCG@10: {}, At K {} and Dataset {}\".format(hit, ndcg, K, fileName ))\n",
    "                if hit>best_hr: best_hr=hit\n",
    "                if ndcg>best_ndcg: best_ndcg=ndcg\n",
    "            print(\"Best HR@10: {}, Best NDCG@10: {}, At K {} and Dataset {}\".format(best_hr, best_ndcg, K, fileName ))\n",
    "            \n",
    "'''\n",
    "Best HR@10: 0.4197019867549669, Best NDCG@10: 0.22775834012977136, At K 8 and Dataset ml-1m\n",
    "Best HR@10: 0.41721854304635764, Best NDCG@10: 0.2251975547330144, At K 16 and Dataset ml-1m\n",
    "Best HR@10: 0.4197019867549669, Best NDCG@10: 0.22811742858950668, At K 32 and Dataset ml-1m\n",
    "Best HR@10: 0.42566225165562915, Best NDCG@10: 0.2340599456026715, At K 64 and Dataset ml-1m\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-25fde50d1b2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfileName\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ml-1m'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pinterest-20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kb-cc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegNum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#loading dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBNCF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_BNCF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-25fde50d1b2f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mtest_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetInstances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misTest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataMat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_u\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataMat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_u\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.25\n",
    "@function: BNMF(Bayesian Neural Matrix Factorization) \n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import heapq\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, fileName, negNum):\n",
    "        self.negNum = negNum #negative sample ratio\n",
    "        self.trainList, self.maxu, self.maxi = self.getTrainset_as_list(fileName)\n",
    "        self.testList = self.getTestset_as_list(fileName)\n",
    "        \n",
    "    def getTrainset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".train.rating\" \n",
    "            data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "            data['rating']=data['rating'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_trainset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "    \n",
    "    def getTestset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".test.negative\" \n",
    "            dataList = []\n",
    "            with open(filePath, 'r') as fd:\n",
    "                line = fd.readline()\n",
    "                while line != None and line != '':\n",
    "                    arr = line.split('\\t')\n",
    "                    u = eval(arr[0])[0]\n",
    "                    dataList.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                    for i in arr[1:]:\n",
    "                        dataList.append([u, int(i), 0.0]) #99 negative items\n",
    "                    line = fd.readline()\n",
    "            return dataList\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_testset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList\n",
    "        \n",
    "    def list_to_matrix(self):              \n",
    "        dataMat = np.zeros([self.maxu, self.maxi], dtype=np.float32)\n",
    "        for u,i,r in self.trainList:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self):\n",
    "        dataDict = {}\n",
    "        for u,i,r in self.trainList:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def getInstances(self, isTest=False):\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        if isTest==True: #test\n",
    "            for u, i, r in self.testList:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "        else:#train\n",
    "            for u, i, r in self.trainList:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "            #negative samples\n",
    "            dataDict = self.list_to_dict()\n",
    "            for j in range(len(self.trainList)*self.negNum):\n",
    "                u = np.random.randint(self.maxu)\n",
    "                i = np.random.randint(self.maxi)\n",
    "                while (u, i) in dataDict:\n",
    "                    u = np.random.randint(self.maxu)\n",
    "                    i = np.random.randint(self.maxi)\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(0.0)) \n",
    "        return user, item, rate\n",
    "    \n",
    "    def getHitRatio(self, ranklist, targetItem):\n",
    "        for item in ranklist:\n",
    "            if item == targetItem:\n",
    "                return 1\n",
    "        return 0\n",
    "    \n",
    "    def getNDCG(self, ranklist, targetItem):\n",
    "        for i in range(len(ranklist)):\n",
    "            item = ranklist[i]\n",
    "            if item == targetItem:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "        return 0\n",
    "    \n",
    "class BNCF:\n",
    "    def __init__(self, dataset):\n",
    "        self.maxu = dataset.maxu\n",
    "        self.maxi = dataset.maxi\n",
    "        self.testList = dataset.testList\n",
    "        \n",
    "        #get the trainset and testset\n",
    "        dataMat = dataset.list_to_matrix()\n",
    "        #get the test data\n",
    "        test_u, test_i, test_r = dataset.getInstances(isTest=True)\n",
    "        self.test_u = dataMat[test_u,:]\n",
    "        self.test_i = dataMat.T[test_i,:]\n",
    "        self.test_r = np.array(test_r)\n",
    "        assert(len(self.test_u) == len(self.test_i) and len(self.test_i) == len(self.test_r))\n",
    "        #get the training data and setting the input \n",
    "        train_u, train_i, train_r = dataset.getInstances(isTest=False)\n",
    "        self.train_u = dataMat[train_u,:]\n",
    "        self.train_i = dataMat.T[train_i,:]\n",
    "        self.train_r = np.array(train_r)\n",
    "        assert(len(self.train_u) == len(self.train_i) and len(self.train_i) == len(self.train_r)) \n",
    "        #release memory\n",
    "        del dataset, dataMat, train_u, train_i, train_r, test_u, test_i, test_r\n",
    "        gc.collect()\n",
    "        \n",
    "    def build_BNCF(self, K = 8):\n",
    "        layers = [1024, K] #number of latent factors\n",
    "        print('start building the BNCF model')\n",
    "        \n",
    "        self.x_u = theano.shared(self.train_u)#self.x_u = pm.Minibatch(self.train_u, batch_size=8096)\n",
    "        self.x_i = theano.shared(self.train_i)#self.x_i = pm.Minibatch(self.train_i, batch_size=8096)\n",
    "        self.y_r = theano.shared(self.train_r)#self.y_r = pm.Minibatch(self.train_r, batch_size=8096)\n",
    "        #release memory\n",
    "        del self.train_u, self.train_i, self.train_r\n",
    "        gc.collect()\n",
    "        with pm.Model() as self.bncf:\n",
    "            #user layer\n",
    "            user_W1 = pm.Normal('user_W1', 0, sd=1, shape=[self.maxi, layers[0]] )\n",
    "            user_O1 = pm.math.tanh(pm.math.dot(self.x_u, user_W1))\n",
    "            user_W2 = pm.Normal('user_W2', 0, sd=1, shape=[layers[0],layers[1]] )\n",
    "            user_O2 = pm.math.tanh(pm.math.dot(user_O1, user_W2))\n",
    "            #item layer\n",
    "            item_W1 = pm.Normal('item_W1', 0, sd=1, shape=[self.maxu, layers[0]] )\n",
    "            item_O1 = pm.math.tanh(pm.math.dot(self.x_i, item_W1))\n",
    "            item_W2 = pm.Normal('item_W2', 0, sd=1, shape=[layers[0],layers[1]] )\n",
    "            item_O2 = pm.math.tanh(pm.math.dot(item_O1, item_W2))\n",
    "            #output layer\n",
    "            act_out = pm.math.sigmoid(np.multiply(user_O2, item_O2).sum(axis=1, keepdims=True))\n",
    "            # Binary classification -> Bernoulli likelihood\n",
    "            r = pm.Bernoulli('r', act_out, observed=self.y_r)                       \n",
    "        print('done building BNCF model')\n",
    "                \n",
    "    def train_BNCF(self):\n",
    "        print('start training the BNCF model')\n",
    "        tstart = time.time()\n",
    "        with self.bncf:\n",
    "            inference = pm.ADVI()\n",
    "            approx = pm.fit(n=1000, method=inference)\n",
    "            self.trace = approx.sample(draws=500)       \n",
    "        elapsed = time.time() - tstart    \n",
    "        print('Completed training the BNCF model in %d seconds' % int(elapsed))\n",
    "           \n",
    "    def evaluate_BNCF(self):\n",
    "        self.x_u.set_value(self.test_u)\n",
    "        self.x_i.set_value(self.test_i)\n",
    "        self.y_r.set_value(self.test_r)\n",
    "        #release memory\n",
    "        del self.test_u, self.test_i, self.test_r\n",
    "        gc.collect()\n",
    "        with self.bncf:#evaluation\n",
    "            ppc = pm.sample_posterior_predictive(self.trace, progressbar=True)\n",
    "            pre_r = ppc['r'].mean(axis=0)\n",
    "\n",
    "            hits = []\n",
    "            ndcgs = []\n",
    "            prev_u = self.testList[0][0]\n",
    "            pos_i = self.testList[0][1]\n",
    "            scorelist = []\n",
    "            iLen = 0\n",
    "            for u, i in self.testList:\n",
    "                if prev_u == u:\n",
    "                    scorelist.append([i,pre_r[iLen]])\n",
    "                else:\n",
    "                    map_item_score = {}\n",
    "                    for item, rate in scorelist: #turn dict\n",
    "                        map_item_score[item] = rate\n",
    "                    ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "                    hits.append(self.dataset.getHitRatio(ranklist, pos_i))\n",
    "                    ndcgs.append(self.dataset.getNDCG(ranklist, pos_i))\n",
    "                    #next user\n",
    "                    scorelist = []\n",
    "                    prev_u = u\n",
    "                    pos_i = i\n",
    "                    scorelist.append([i,pre_r[iLen]])\n",
    "                iLen = iLen + 1\n",
    "            hit, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "            return hit, ndcg\n",
    "if __name__ == \"__main__\":\n",
    "    for fileName in ['ml-1m', 'pinterest-20', 'kb-cc']:\n",
    "        dataset = DataSet(fileName=fileName, negNum=4)#loading dataset\n",
    "        model = BNCF(dataset)\n",
    "        for K in [8, 16, 32, 64]:\n",
    "            model.build_BNCF(K)\n",
    "            model.train_BNCF()\n",
    "            hit, ndcg = model.evaluate_BNCF()\n",
    "            print(\"HR@10: {}, NDCG@10: {}, At K {} and Dataset {}\".format(hit, ndcg, K, fileName ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.23\n",
    "@function: Implementing BMF(Bayesian Neural Collaborative Filtering) which is designed by Jason.F\n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import tensorflow as tf\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, fileName, negNum):\n",
    "        self.negNum = negNum #negative sample ratio\n",
    "        self.trainList, self.maxu, self.maxi = self.getTrainset_as_list(fileName)\n",
    "        self.testList = self.getTestset_as_list(fileName)\n",
    "        \n",
    "    def getTrainset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".train.rating\" \n",
    "            data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_trainset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "    \n",
    "    def getTestset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".test.negative\" \n",
    "            dataList = []\n",
    "            with open(filePath, 'r') as fd:\n",
    "                line = fd.readline()\n",
    "                while line != None and line != '':\n",
    "                    arr = line.split('\\t')\n",
    "                    u = eval(arr[0])[0]\n",
    "                    dataList.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                    for i in arr[1:]:\n",
    "                        dataList.append([u, int(i), 0.0]) #99 negative items\n",
    "                    line = fd.readline()\n",
    "            return dataList\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_testset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList\n",
    "        \n",
    "    def list_to_matrix(self):              \n",
    "        dataMat = np.zeros([self.maxu, self.maxi], dtype=np.float32)\n",
    "        for u,i,r in self.trainList:\n",
    "            dataMat[int(u)][int(i)] = float(1.0)#float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self):\n",
    "        dataDict = {}\n",
    "        for u,i,r in self.trainList:\n",
    "            dataDict[int(u), int(i)] = float(1.0)#float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def getInstances(self, isTest=False):\n",
    "        #dataMat = self.list_to_matrix(self.trainList, self.maxu, self.maxi)\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        if isTest==True: #test\n",
    "            for u, i, r in self.testList:\n",
    "                user.append(int(u))#user.append(dataMat[int(u),:].tolist())\n",
    "                item.append(int(i))#item.append(dataMat[:,int(i)].tolist())\n",
    "                rate.append(1.0)#rate.append(float(r))\n",
    "        else:#train\n",
    "            for u, i, r in self.trainList:\n",
    "                user.append(int(u))#user.append(dataMat[int(u),:].tolist())\n",
    "                item.append(int(i))#item.append(dataMat[:,int(i)].tolist())\n",
    "                rate.append(1.0)#rate.append(float(r))\n",
    "            #negative samples\n",
    "            dataDict = self.list_to_dict()\n",
    "            for j in range(len(self.trainList)*self.negNum):\n",
    "                u = np.random.randint(self.maxu)\n",
    "                i = np.random.randint(self.maxi)\n",
    "                while (u, i) in dataDict:\n",
    "                    u = np.random.randint(self.maxu)\n",
    "                    i = np.random.randint(self.maxi)\n",
    "                user.append(int(u))#user.append(dataMat[u,:].tolist())\n",
    "                item.append(int(i))#item.append(dataMat[:,i].tolist())\n",
    "                rate.append(0.0) \n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "    \n",
    "    def getHitRatio(self, ranklist, targetItem):\n",
    "        for item in ranklist:\n",
    "            if item == targetItem:\n",
    "                return 1\n",
    "        return 0\n",
    "    \n",
    "    def getNDCG(self, ranklist, targetItem):\n",
    "        for i in range(len(ranklist)):\n",
    "            item = ranklist[i]\n",
    "            if item == targetItem:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "        return 0\n",
    "class BNCF:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        #self.trainList = self.dataset.trainList\n",
    "        #self.testList = self.dataset.testList\n",
    "        self.maxu = self.dataset.maxu\n",
    "        self.maxi = self.dataset.maxi\n",
    "        \n",
    "        #get the test data\n",
    "        self.test_u, self.test_i, self.test_r = self.dataset.getInstances(isTest=True)\n",
    "        #get the training data and setting the input \n",
    "        self.train_u, self.train_i, self.train_r = self.dataset.getInstances(isTest=False)\n",
    "        assert(self.train_u.shape == self.train_i.shape and self.train_i.shape == self.train_r.shape)\n",
    "        #initiate the seesion\n",
    "        self.init_sess()\n",
    "        #train data by mini-batch\n",
    "        self.arrUser, self.arrItem = self.batchTrainset()\n",
    "       \n",
    "    def init_sess(self):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.user = tf.placeholder(tf.int32)\n",
    "        self.item = tf.placeholder(tf.int32)\n",
    "        self.rate = tf.placeholder(tf.float32)\n",
    "        self.R = tf.placeholder(tf.float32, shape=(self.maxu, self.maxi))\n",
    "        #embedding layer\n",
    "        self.user_item_embedding = tf.convert_to_tensor(self.R)\n",
    "        self.item_user_embedding = tf.transpose(self.user_item_embedding)\n",
    "        self.user_input = tf.nn.embedding_lookup(self.user_item_embedding, self.user)\n",
    "        self.item_input = tf.nn.embedding_lookup(self.item_user_embedding, self.item)\n",
    "        #define seesion\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.config.allow_soft_placement = True\n",
    "        self.sess = tf.Session(config=self.config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def batchTrainset(self):\n",
    "        #get input in the way of mini-batch\n",
    "        batchSize=8192\n",
    "        num_batches = len(self.train_u) // batchSize + 1\n",
    "        train_u_batch = self.train_u[0: batchSize]\n",
    "        train_i_batch = self.train_i[0: batchSize]\n",
    "        arrUser, arrItem = self.sess.run([self.user_input, self.item_input], \\\n",
    "                                             feed_dict={self.user: train_u_batch, \\\n",
    "                                                        self.item: train_i_batch, \\\n",
    "                                                        self.R: self.dataset.list_to_matrix()})\n",
    "        '''\n",
    "        for i in range(1, num_batches):\n",
    "            min_idx = i * batchSize\n",
    "            max_idx = np.min([len(self.train_u), (i+1)*batchSize])\n",
    "            train_u_batch = self.train_u[min_idx: max_idx]\n",
    "            train_i_batch = self.train_i[min_idx: max_idx]\n",
    "            train_r_batch = self.train_r[min_idx: max_idx]\n",
    "            batchUser, batchItem = self.sess.run([self.user_input, self.item_input], \\\n",
    "                                             feed_dict={self.user: train_u_batch, \\\n",
    "                                                        self.item: train_i_batch, \\\n",
    "                                                        self.R: self.dataset.list_to_matrix()})\n",
    "            arrUser = np.concatenate((arrUser, batchUser),axis=0)\n",
    "            arrItem = np.concatenate((arrItem, batchItem),axis=0)\n",
    "        '''\n",
    "        return arrUser, arrItem\n",
    "    \n",
    "    \n",
    "    def build_BNCF(self, K = 8):\n",
    "        layers = [1024, K] #number of latent factors\n",
    "        logging.info('start building the BNCF model')\n",
    "        \n",
    "        self.x_u = theano.shared(self.arrUser)\n",
    "        self.x_i = theano.shared(self.arrItem)\n",
    "        self.y_r = theano.shared(self.train_r)       \n",
    "        with pm.Model() as self.bncf:\n",
    "            #user layer\n",
    "            user_W1 = pm.Normal('user_W1', 0, sd=1, shape=[self.maxi, layers[0]] )\n",
    "            user_O1 = pm.math.tanh(pm.math.dot(self.x_u, user_W1))\n",
    "            user_W2 = pm.Normal('user_W2', 0, sd=1, shape=[layers[0],layers[1]] )\n",
    "            user_O2 = pm.math.tanh(pm.math.dot(user_O1, user_W2))\n",
    "            #item layer\n",
    "            item_W1 = pm.Normal('item_W1', 0, sd=1, shape=[self.maxu, layers[0]] )\n",
    "            item_O1 = pm.math.tanh(pm.math.dot(self.x_i, item_W1))\n",
    "            item_W2 = pm.Normal('item_W2', 0, sd=1, shape=[layers[0],layers[1]] )\n",
    "            item_O2 = pm.math.tanh(pm.math.dot(item_O1, item_W2))\n",
    "            #output layer\n",
    "            #act_out = pm.math.sigmoid(pm.math.dot(user_O2, item_O2.T))\n",
    "            #act_out = pm.math.sigmoid(np.sum(np.multiply(user_O2, item_O2),axis=1, keepdims=True))\n",
    "            act_out = pm.math.sigmoid(np.multiply(user_O2, item_O2).sum(axis=1, keepdims=True))\n",
    "            #act_out = pm.math.sigmoid(tf.reduce_sum(tf.multiply(user_O2, item_O2),axis=1, keep_dims=True))\n",
    "            # Binary classification -> Bernoulli likelihood\n",
    "            r = pm.Bernoulli('r', act_out, observed=self.y_r, total_size=self.y_r.shape[0]) # IMPORTANT for minibatches                         \n",
    "        logging.info('done building BNCF model')\n",
    "                \n",
    "    def train_BNCF(self):\n",
    "        logging.info('start training the BNCF model')\n",
    "        tstart = time.time()\n",
    "        with self.bncf:\n",
    "            inference = pm.ADVI()\n",
    "            approx = pm.fit(n=1000, method=inference)\n",
    "            self.trace = approx.sample(draws=500)       \n",
    "        elapsed = time.time() - tstart    \n",
    "        logging.info('Completed training the BNCF model in %d seconds' % int(elapsed))\n",
    "           \n",
    "    def evaluate_BNCF(self):\n",
    "        arrUser, arrItem = self.sess.run([self.user_input, self.item_input], \\\n",
    "                               feed_dict={self.user: self.test_u, \\\n",
    "                                          self.item: self.test_i, \\\n",
    "                                          self.R: self.dataset.list_to_matrix()})\n",
    "        self.x_u.set_value(arrUser)\n",
    "        self.x_i.set_value(arrItem)\n",
    "        self.y_r.set_value(self.test_r)\n",
    "        with self.bncf:#evaluation\n",
    "            ppc = pm.sample_posterior_predictive(self.trace, progressbar=True)\n",
    "            pre_r = ppc['r'].mean(axis=0)\n",
    "\n",
    "            hits = []\n",
    "            ndcgs = []\n",
    "            prev_u = self.dataset.testList[0][0]\n",
    "            pos_i = self.dataset.testList[0][1]\n",
    "            scorelist = []\n",
    "            iLen = 0\n",
    "            for u, i in self.dataset.testList:\n",
    "                if prev_u == u:\n",
    "                    scorelist.append([i,pre_r[iLen]])\n",
    "                else:\n",
    "                    map_item_score = {}\n",
    "                    for item, rate in scorelist: #turn dict\n",
    "                        map_item_score[item] = rate\n",
    "                    ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "                    hits.append(self.dataset.getHitRatio(ranklist, pos_i))\n",
    "                    ndcgs.append(self.dataset.getNDCG(ranklist, pos_i))\n",
    "                    #next user\n",
    "                    scorelist = []\n",
    "                    prev_u = u\n",
    "                    pos_i = i\n",
    "                    scorelist.append([i,pre_r[iLen]])\n",
    "                iLen = iLen + 1\n",
    "            hit, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "            return hit, ndcg\n",
    "if __name__ == \"__main__\":\n",
    "    for fileName in ['ml-1m', 'pinterest-20', 'kb-cc']:\n",
    "        dataset = DataSet(fileName=fileName, negNum=4)#loading dataset\n",
    "        model = BNCF(dataset)\n",
    "        for K in [8, 16, 32, 64]:\n",
    "            model.build_BNCF(K)\n",
    "            model.train_BNCF()\n",
    "            hit, ndcg = model.evaluate_BNCF()\n",
    "            print(\"HR@10: {}, NDCG@10: {}, At K {} and Dataset {}\".format(hit, ndcg, K, fileName ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6039\n",
      "\tItem Num: 3705\n",
      "\tData Size: 994169\n"
     ]
    }
   ],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.22\n",
    "@function: Implementing BMF(Bayesian Neural Collaborative Filtering) which is designed by Jason.F\n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import tensorflow as tf\n",
    "\n",
    "def getTraindata():\n",
    "    data = pd.read_csv(\"/data/fjsdata/ctKngBase/ml/ml-1m.train.rating\", \\\n",
    "                             sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                             usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "    maxu, maxi = data['user'].max(), data['item'].max()\n",
    "    data = data.values.tolist()\n",
    "    print(\"Loading Success!\\n\"\n",
    "                  \"Data Info:\\n\"\n",
    "                  \"\\tUser Num: {}\\n\"\n",
    "                  \"\\tItem Num: {}\\n\"\n",
    "                  \"\\tData Size: {}\".format(maxu, maxi, len(data)))\n",
    "    R = np.zeros([maxu+1, maxi+1], dtype=np.float32)\n",
    "    for i in data:\n",
    "        user = int(i[0])\n",
    "        item = int(i[1])\n",
    "        rating = float(i[2])\n",
    "        R[user][item] = rating\n",
    "    return R, data, maxu, maxi\n",
    "\n",
    "def getTrainDict(data):\n",
    "    dataDict = {}\n",
    "    for i in data:\n",
    "        dataDict[(i[0], i[1])] = i[2]\n",
    "    return dataDict\n",
    "    \n",
    "def getInstances(R, data, maxu, maxi, negNum):\n",
    "    user = []\n",
    "    item = []\n",
    "    rate = []\n",
    "    '''\n",
    "    for i in data:\n",
    "        user.append(R[int(i[0]),:].tolist())\n",
    "        item.append(R[:,int(i[1])].tolist())\n",
    "        rate.append(1.0)\n",
    "        for t in range(negNum):\n",
    "            j = np.random.randint(maxi)\n",
    "            while (i[0], j) in dataDict:\n",
    "                j = np.random.randint(maxi)\n",
    "            user.append(R[int(i[0]),:].tolist())\n",
    "            item.append(R[:,j].tolist())\n",
    "            rate.append(0.0)\n",
    "    '''\n",
    "    for u, i, _ in data:\n",
    "        user.append(R[int(u),:].tolist())\n",
    "        item.append(R[:,int(i)].tolist())\n",
    "        rate.append(1.0)\n",
    "    '''\n",
    "    dataDict = getTrainDict(data)\n",
    "    for j in range(len(data)*negNum):\n",
    "        u = np.random.randint(maxu)\n",
    "        i = np.random.randint(maxi)\n",
    "        while (u, i) in dataDict:\n",
    "            u = np.random.randint(maxu)\n",
    "            i = np.random.randint(maxi)\n",
    "        user.append(R[u,:].tolist())\n",
    "        item.append(R[:,i].tolist())\n",
    "        rate.append(0.0)\n",
    "    '''\n",
    "    return np.array(user), np.array(item), np.array(rate)\n",
    "\n",
    "def getTestdata():\n",
    "    testset = []\n",
    "    filePath = '/data/fjsdata/ctKngBase/ml/ml-1m.test.negative'\n",
    "    with open(filePath, 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            testset.append([u, eval(arr[0])[1], 1.0])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                testset.append([u, int(i), 0.0]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return testset\n",
    "\n",
    "\n",
    "def getTestInstances(R, testset):\n",
    "    for i in testset:\n",
    "        user.append(R[int(i[0]),:].tolist())\n",
    "        item.append(R[:,int(i[1])].tolist())\n",
    "        rate.append(float(i[2]))\n",
    "    return np.array(user), np.array(item), np.array(rate)\n",
    "    \n",
    "def build_BNCF(x_u, x_i, y_r, maxu, maxi, K=8):\n",
    "    logging.info('building the BMF model')\n",
    "\n",
    "    Layers = [1024, K]\n",
    "    with pm.Model() as bncf:\n",
    "        #user layer\n",
    "        user_W1 = pm.Normal('user_W1', 0, sd=1, shape=[maxi+1, Layer[0]] )\n",
    "        user_O1 = pm.math.tanh(pm.math.dot(x_u, user_W1))\n",
    "        user_W2 = pm.Normal('user_W2', 0, sd=1, shape=[Layer[0],Layer[1]] )\n",
    "        user_O2 = pm.math.tanh(pm.math.dot(user_O1, user_W2))\n",
    "        #item layer\n",
    "        item_W1 = pm.Normal('item_W1', 0, sd=1, shape=[maxu+1, Layer[0]] )\n",
    "        item_O1 = pm.math.tanh(pm.math.dot(x_i, item_W1))\n",
    "        item_W2 = pm.Normal('item_W2', 0, sd=1, shape=[Layer[0],Layer[1]] )\n",
    "        item_O2 = pm.math.tanh(pm.math.dot(item_O1, item_W2))\n",
    "        #output layer\n",
    "        act_out = pm.math.sigmoid(pm.math.dot(user_O2, item_O2.T))\n",
    "        # Binary classification -> Bernoulli likelihood\n",
    "        r = pm.Bernoulli('r', act_out, observed=y_r, total_size=y_r.shape[0]) # IMPORTANT for minibatches\n",
    "                                \n",
    "    logging.info('done building BMF model')\n",
    "    \n",
    "    return bncf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO,format='[%(asctime)s]: %(message)s')\n",
    "\n",
    "    # Read data and build BMF model.\n",
    "    R, data, maxu, maxi = getTraindata()\n",
    "    train_u, train_i, train_r = getInstances(R, data, maxu, maxi, negNum=4)\n",
    "    x_u = theano.shared(train_u)\n",
    "    x_i = theano.shared(train_i)\n",
    "    y_r = theano.shared(train_r)\n",
    "    bncf = build_BNCF(x_u, x_i, y_r, maxu, maxi, K=8)#dim is the number of latent factors\n",
    "\n",
    "    with bncf:# sample with BMF\n",
    "        tstart = time.time()\n",
    "        logging.info('Start BMF sampling')\n",
    "        inference = pm.ADVI()\n",
    "        approx = pm.fit(n=1000, method=inference)\n",
    "        trace = approx.sample(draws=500)\n",
    "        elapsed = time.time() - tstart    \n",
    "        logging.info('Complete BMF sampling in %d seconds' % int(elapsed))\n",
    "   \n",
    "\n",
    "    testset = getTestdata()\n",
    "    test_u, test_i, test_r = getTestInstances(R, testset)\n",
    "    x_u.set_value(test_u)\n",
    "    x_i.set_value(test_i)\n",
    "    y_r.set_value(test_r)\n",
    "    with bncf:#evaluation\n",
    "        ppc = pm.sample_posterior_predictive(trace, progressbar=True)\n",
    "        pre_r = ppc['r'].mean(axis=0)\n",
    "        \n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        prev_u = testset[0][0]\n",
    "        pos_i = testset[0][1]\n",
    "        scorelist = []\n",
    "        iLen = 0\n",
    "        for u, i in testset:\n",
    "            if prev_u == u:\n",
    "                scorelist.append([i,pre_r[iLen]])\n",
    "            else:\n",
    "                map_item_score = {}\n",
    "                for item, rate in scorelist: #turn dict\n",
    "                    map_item_score[item] = rate\n",
    "                ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "                hr = getHitRatio(ranklist, pos_i)\n",
    "                hits.append(hr)\n",
    "                ndcg = getNDCG(ranklist, pos_i)\n",
    "                ndcgs.append(ndcg)\n",
    "                #next user\n",
    "                scorelist = []\n",
    "                prev_u = u\n",
    "                pos_i = i\n",
    "                scorelist.append([i,pre_r[iLen]])\n",
    "            iLen = iLen + 1\n",
    "        hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print(\"hr: {}, NDCG: {}, At K {}\".format(hitratio, ndcg, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '15568' (I am process '16185')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /root/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.6.6-64/lock_dir\n",
      "/usr/local/lib/python3.6/dist-packages/pymc3/tuning/starting.py:61: UserWarning: find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.\n",
      "  warnings.warn('find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.')\n",
      "logp = -17,695, ||grad|| = 0.16015: 100%|██████████| 16/16 [00:00<00:00, 219.90it/s]  \n",
      "Multiprocess sampling (2 chains in 8 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [Q]\n",
      ">Metropolis: [P]\n",
      "Sampling 2 chains: 100%|██████████| 3000/3000 [00:06<00:00, 444.96draws/s]\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "100%|██████████| 2000/2000 [00:03<00:00, 527.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE：0.627957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#矩阵分解R=PQ，推荐概率模型MCMC采样-似然函数是正态\n",
    "import theano\n",
    "import pymc3 as pm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#1.数据集处理\n",
    "#http://files.grouplens.org/datasets/movielens/ml-20m-README.html\n",
    "#the following format of file ratings.csv: userId,movieId,rating,timestamp\n",
    "#The lines within this file are ordered first by userId, then, within user, by movieId.\n",
    "#Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "#Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "data = pd.read_csv(\"/data/fjsdata/BayesianRS/ml-20m/ratings.csv\",sep=',',low_memory=False, iterator =True)\n",
    "data = data.get_chunk(100)\n",
    "#将userId和movieId全部标准编号\n",
    "data_rating = data[['rating']]\n",
    "le = LabelEncoder()\n",
    "data = data[['userId','movieId']].apply(le.fit_transform)\n",
    "data = pd.concat([data,data_rating],axis=1)\n",
    "#抽样10%比例测试\n",
    "test = data.sample(frac=0.1)\n",
    "#2.构建概率模型\n",
    "#概率模型参数设置\n",
    "uNum = len(data['userId'].unique())#统计用户数\n",
    "iNum = len(data['movieId'].unique())#统计电影数\n",
    "mean= data['rating'].max()/2 #正态分布的均值和方差\n",
    "k = 100 #隐因子数\n",
    "X_input = theano.shared(data[['userId','movieId']].values)#转numpy array\n",
    "Y_output = theano.shared(data['rating'].values)#转numpy array\n",
    "with pm.Model() as BMF_model:\n",
    "    # Creating the model\n",
    "    P = pm.Normal('P', mu=mean, sd=mean, shape=(uNum,k))\n",
    "    Q = pm.Normal('Q', mu=mean, sd=mean, shape=(k,iNum))\n",
    "    R = pm.Deterministic('R', tt.dot(P,Q))\n",
    "    rY = []\n",
    "    for row in X_input.get_value(): # 获取每行的值\n",
    "        rr = R[int(row[0])][int(row[1])]#userId是0列,movieId是1列\n",
    "        rY.append(rr)\n",
    "    Y = pm.Normal('Y',mu=rY, sd=mean, observed=Y_output.get_value())\n",
    "#3.后验分布计算  \n",
    "with BMF_model:        \n",
    "    start=pm.find_MAP()  # 参数初猜\n",
    "    #二值变量：指定 BinaryMetropolis  离散变量：指定 Metropolis  连续变量：指定 NUTS\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(1000,start=start,step=step,chains=2,cores=8)\n",
    "\n",
    "#后验分布采样观察\n",
    "#pm.traceplot(trace, varnames=['P'])\n",
    "#pm.summary(trace, varnames=['P'])\n",
    "#print (trace['P'].shape)\n",
    "#print (trace['Q'].shape)\n",
    "#4.后验预测  \n",
    "#X_input.set_value(test[['userId','movieId']].values)#转numpy array\n",
    "#Y_output.set_value(test['rating'].values)\n",
    "with BMF_model:\n",
    "    ppc = pm.sample_posterior_predictive(trace)#vars=BMF_model.observed_RVs\n",
    "    pred = ppc['Y'].mean(axis=0)\n",
    "    \n",
    "print ('RMSE：%f'% mean_squared_error(Y_output.get_value(),pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 5,929.2: 100%|██████████| 10000/10000 [00:39<00:00, 254.78it/s]  \n",
      "Finished [100%]: Average Loss = 5,921.3\n",
      "100%|██████████| 5000/5000 [00:10<00:00, 481.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE：0.150872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#矩阵分解R=PQ，推荐概率模型ADVI变分推断-似然函数是正态\n",
    "import theano\n",
    "import pymc3 as pm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#1.数据集处理\n",
    "#http://files.grouplens.org/datasets/movielens/ml-20m-README.html\n",
    "#the following format of file ratings.csv: userId,movieId,rating,timestamp\n",
    "#The lines within this file are ordered first by userId, then, within user, by movieId.\n",
    "#Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "#Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "data = pd.read_csv(\"/data/fjsdata/BayesianRS/ml-20m/ratings.csv\",sep=',',low_memory=False, iterator =True)\n",
    "data = data.get_chunk(100)\n",
    "#将userId和movieId全部标准编号\n",
    "data_rating = data[['rating']]\n",
    "le = LabelEncoder()\n",
    "data = data[['userId','movieId']].apply(le.fit_transform)\n",
    "data = pd.concat([data,data_rating],axis=1)\n",
    "#抽样10%比例测试\n",
    "test = data.sample(frac=0.1)\n",
    "#2.构建概率模型\n",
    "#概率模型参数设置\n",
    "uNum = len(data['userId'].unique())#统计用户数\n",
    "iNum = len(data['movieId'].unique())#统计电影数\n",
    "mean= data['rating'].max()/2 #正态分布的均值和方差\n",
    "k = 100 #隐因子数\n",
    "X_input = theano.shared(data[['userId','movieId']].values)#转numpy array\n",
    "Y_output = theano.shared(data['rating'].values)#转numpy array\n",
    "with pm.Model() as BMF_model:\n",
    "    # Creating the model\n",
    "    P = pm.Normal('P', mu=mean, sd=mean, shape=(uNum,k))\n",
    "    Q = pm.Normal('Q', mu=mean, sd=mean, shape=(k,iNum))\n",
    "    R = tt.dot(P,Q)\n",
    "    rY = []\n",
    "    for row in X_input.get_value(): # 获取每行的值\n",
    "        rr = R[int(row[0])][int(row[1])]#userId=0,movieId=1\n",
    "        rY.append(rr)\n",
    "    Y = pm.Normal('Y',mu=rY, sd=mean, observed=Y_output.get_value())\n",
    "#3.后验分布计算  \n",
    "with BMF_model:        \n",
    "    inference = pm.ADVI()\n",
    "    approx = pm.fit(n=10000, method=inference)\n",
    "    trace = approx.sample(draws=5000)\n",
    "    \n",
    "#4.后验预测  \n",
    "#X_input.set_value(test[['userId','movieId']].values)#转numpy array\n",
    "#Y_output.set_value(test['rating'].values)\n",
    "with BMF_model:\n",
    "    ppc = pm.sample_posterior_predictive(trace)\n",
    "    pred = ppc['Y'].mean(axis=0)\n",
    "    \n",
    "print ('RMSE：%f'% mean_squared_error(Y_output.get_value(),pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
