{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.11\n",
    "@function: Implementing NCF with Torch  \n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "           https://arxiv.org/pdf/1708.05031.pdf\n",
    "           https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "\n",
    "class NCFData(torch.utils.data.Dataset):#define the dataset\n",
    "    def __init__(self, features, num_item, train_mat=None, num_ng=0, is_training=None):\n",
    "        super(NCFData, self).__init__()\n",
    "        # Note that the labels are only useful when training, we thus add them in the ng_sample() function.\n",
    "        self.features_ps = features\n",
    "        self.num_item = num_item\n",
    "        self.train_mat = train_mat\n",
    "        self.num_ng = num_ng\n",
    "        self.is_training = is_training\n",
    "        self.labels = [0 for _ in range(len(features))]\n",
    "\n",
    "    def ng_sample(self):\n",
    "        assert self.is_training, 'no need to sampling when testing'\n",
    "        self.features_ng = []\n",
    "        for x in self.features_ps:\n",
    "            u = x[0]\n",
    "            for t in range(self.num_ng):\n",
    "                j = np.random.randint(self.num_item)\n",
    "                while (u, j) in self.train_mat:\n",
    "                    j = np.random.randint(self.num_item)\n",
    "                self.features_ng.append([u, j])\n",
    "        \n",
    "        labels_ps = [1 for _ in range(len(self.features_ps))]\n",
    "        labels_ng = [0 for _ in range(len(self.features_ng))]\n",
    "        \n",
    "        self.features_fill = self.features_ps + self.features_ng\n",
    "        self.labels_fill = labels_ps + labels_ng\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.num_ng + 1) * len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        if self.is_training:\n",
    "            self.ng_sample()\n",
    "            features = self.features_fill\n",
    "            labels = self.labels_fill\n",
    "        else:\n",
    "            features = self.features_ps\n",
    "            labels = self.labels\n",
    "        '''\n",
    "        features = self.features_fill if self.is_training else self.features_ps\n",
    "        labels = self.labels_fill if self.is_training else self.labels\n",
    "        \n",
    "        user = features[idx][0]\n",
    "        item = features[idx][1]\n",
    "        label = labels[idx]\n",
    "        return user, item ,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset function\n",
    "def load_dataset(test_num=100):\n",
    "    train_data = pd.read_csv(\"/data/fjsdata/ctKngBase/ml/ml-1m.train.rating\", \\\n",
    "                             sep='\\t', header=None, names=['user', 'item'], \\\n",
    "                             usecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n",
    "\n",
    "    user_num = train_data['user'].max() + 1\n",
    "    item_num = train_data['item'].max() + 1\n",
    "    \n",
    "    train_data = train_data.values.tolist()\n",
    "    \n",
    "    # load ratings as a dok matrix\n",
    "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "    for x in train_data:\n",
    "        train_mat[x[0], x[1]] = 1.0\n",
    "\n",
    "    test_data = []\n",
    "    with open(\"/data/fjsdata/ctKngBase/ml/ml-1m.test.negative\", 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            test_data.append([u, eval(arr[0])[1]])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                test_data.append([u, int(i)]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return train_data, test_data, user_num, item_num, train_mat\n",
    "\n",
    "#evaluate function\n",
    "def hit(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def ndcg(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        index = pred_items.index(gt_item)\n",
    "        return np.reciprocal(np.log2(index+2))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def metrics(model, test_loader, top_k):\n",
    "    HR, NDCG = [], []\n",
    "    \n",
    "    for user, item, label in test_loader:\n",
    "        user = user.cuda()\n",
    "        item = item.cuda()\n",
    "        \n",
    "        predictions = model(user, item)\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "        recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
    "        \n",
    "        gt_item = item[0].item()\n",
    "        HR.append(hit(gt_item, recommends))\n",
    "        NDCG.append(ndcg(gt_item, recommends))\n",
    "    return np.mean(HR), np.mean(NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the GMF model\n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, user_num, item_num, factor_num):\n",
    "        super(GMF, self).__init__()\n",
    "        \n",
    "        self.embed_user_GMF = nn.Embedding(user_num, factor_num)\n",
    "        self.embed_item_GMF = nn.Embedding(item_num, factor_num)\n",
    "        self.predict_layer = nn.Linear(factor_num, 1)\n",
    "        \n",
    "        self._init_weight_()\n",
    "        \n",
    "    def _init_weight_(self):\n",
    "        nn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        embed_user_GMF = self.embed_user_GMF(user)\n",
    "        embed_item_GMF = self.embed_item_GMF(item)\n",
    "        output_GMF = embed_user_GMF * embed_item_GMF\n",
    "        prediction = self.predict_layer(output_GMF)\n",
    "        return prediction.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, user_num, item_num, factor_num, num_layers, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.embed_user_MLP = nn.Embedding(user_num, factor_num * (2 ** (num_layers - 1)))\n",
    "        self.embed_item_MLP = nn.Embedding(item_num, factor_num * (2 ** (num_layers - 1)))\n",
    "        \n",
    "        MLP_modules = []\n",
    "        for i in range(num_layers):\n",
    "            input_size = factor_num * (2 ** (num_layers - i))\n",
    "            MLP_modules.append(nn.Dropout(p=dropout))\n",
    "            MLP_modules.append(nn.Linear(input_size, input_size//2))\n",
    "            MLP_modules.append(nn.ReLU())\n",
    "        self.MLP_layers = nn.Sequential(*MLP_modules)\n",
    "        \n",
    "        self.predict_layer = nn.Linear(factor_num, 1)\n",
    "        \n",
    "        self._init_weight_()\n",
    "        \n",
    "    def _init_weight_(self):\n",
    "        nn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n",
    "        \n",
    "        for m in self.MLP_layers:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.kaiming_uniform_(self.predict_layer.weight,a=1, nonlinearity='sigmoid')\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        embed_user_MLP = self.embed_user_MLP(user)\n",
    "        embed_item_MLP = self.embed_item_MLP(item)\n",
    "        interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n",
    "        output_MLP = self.MLP_layers(interaction)\n",
    "        prediction = self.predict_layer(output_MLP)\n",
    "        return prediction.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the NCF model, integrating the GMF and MLP\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, user_num, item_num, factor_num, num_layers, dropout):\n",
    "        super(NCF, self).__init__()\n",
    "        \"\"\"\n",
    "        user_num: number of users;\n",
    "        item_num: number of items;\n",
    "        factor_num: number of predictive factors;\n",
    "        num_layers: the number of layers in MLP model;\n",
    "        dropout: dropout rate between fully connected layers;\n",
    "        \"\"\"\n",
    "        self.embed_user_GMF = nn.Embedding(user_num, factor_num)\n",
    "        self.embed_item_GMF = nn.Embedding(item_num, factor_num)\n",
    "        self.embed_user_MLP = nn.Embedding(user_num, factor_num * (2 ** (num_layers - 1)))\n",
    "        self.embed_item_MLP = nn.Embedding(item_num, factor_num * (2 ** (num_layers - 1)))\n",
    "\n",
    "        MLP_modules = []\n",
    "        for i in range(num_layers):\n",
    "            input_size = factor_num * (2 ** (num_layers - i))\n",
    "            MLP_modules.append(nn.Dropout(p=dropout))\n",
    "            MLP_modules.append(nn.Linear(input_size, input_size//2))\n",
    "            MLP_modules.append(nn.ReLU())\n",
    "        self.MLP_layers = nn.Sequential(*MLP_modules)\n",
    "        \n",
    "        self.predict_layer = nn.Linear(factor_num * 2, 1)\n",
    "        \n",
    "        self._init_weight_()\n",
    "        \n",
    "    def _init_weight_(self):\n",
    "        \"\"\" We leave the weights initialization here. \"\"\"\n",
    "        nn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n",
    "        \n",
    "        for m in self.MLP_layers:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.kaiming_uniform_(self.predict_layer.weight, a=1, nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        \n",
    "        embed_user_GMF = self.embed_user_GMF(user)\n",
    "        embed_item_GMF = self.embed_item_GMF(item)\n",
    "        output_GMF = embed_user_GMF * embed_item_GMF\n",
    "        \n",
    "        embed_user_MLP = self.embed_user_MLP(user)\n",
    "        embed_item_MLP = self.embed_item_MLP(item)\n",
    "        interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n",
    "        output_MLP = self.MLP_layers(interaction)\n",
    "\n",
    "        concat = torch.cat((output_GMF, output_MLP), -1)\n",
    "\n",
    "        prediction = self.predict_layer(concat)\n",
    "        return prediction.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the train and test datasets\n",
    "train_data, test_data, user_num ,item_num, train_mat = load_dataset()\n",
    "train_dataset = NCFData(train_data, item_num, train_mat, num_ng=4, is_training=True)#neg_items=4,default\n",
    "test_dataset = NCFData(test_data, item_num, train_mat, num_ng=0, is_training=False)#100\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=256, shuffle=True, num_workers=4)\n",
    "#every user have 99 negative items and one positive items，so batch_size=100\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=99+1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting GPU Enviroment\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" #using gpu \n",
    "cudnn.benchmark = True\n",
    "#training and evaluationg\n",
    "print (\"%3s%20s%20s%20s\" % ('K','Iterations', 'HitRatio', 'NDCG'))\n",
    "for K in [8,16,32,64]:#latent factors\n",
    "    #model = GMF(int(user_num), int(item_num), factor_num=16)\n",
    "    #model = MLP(int(user_num), int(item_num), factor_num=16, num_layers=3, dropout=0.0)\n",
    "    model = NCF(int(user_num), int(item_num), factor_num=16, num_layers=3, dropout=0.0)\n",
    "    model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    best_hr,best_ndcg = 0.0, 0.0\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        train_loader.dataset.ng_sample()\n",
    "        for user, item, label in train_loader:\n",
    "            user = user.cuda()\n",
    "            item = item.cuda()\n",
    "            label = label.float().cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            prediction = model(user, item)\n",
    "            loss = loss_function(prediction, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        HR, NDCG = metrics(model, test_loader, top_k=10)\n",
    "        #print(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(HR, NDCG))\n",
    "        if HR > best_hr: best_hr=HR\n",
    "        if NDCG > best_ndcg: best_ndcg=NDCG\n",
    "    print (\"%3d%20d%20.6f%20.6f\" % (K, 20, best_hr, best_ndcg))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
