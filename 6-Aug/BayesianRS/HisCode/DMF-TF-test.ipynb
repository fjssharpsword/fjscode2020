{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ml-1m data set...\n",
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6039\n",
      "\tItem Num: 3705\n",
      "\tData Size: 994169\n",
      "Start Training!\n",
      "====================Epoch  0 ====================\n",
      "30870 / 30880 : loss = 346.49325561523445\n",
      "Mean loss in this epoch is: 312.9078369140625\n",
      "==================================================\n",
      "Start Evaluation!\n",
      "Epoch  0 HR: 1.0, NDCG: 1.0\n",
      "====================Epoch  0 End====================\n",
      "====================Epoch  1 ====================\n",
      "80 / 30880 : loss = 305.59933471679695"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-80e6780b3c88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-80e6780b3c88>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-80e6780b3c88>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"Epoch \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start Evaluation!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-80e6780b3c88>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, sess, verbose)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_u_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_i_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_r_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.17\n",
    "@function: Implementing DMF with Torch  \n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "           https://www.ijcai.org/proceedings/2017/0447.pdf\n",
    "           https://github.com/RuidongZ/Deep_Matrix_Factorization_Models\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import heapq\n",
    "import math\n",
    "import sys\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, args):\n",
    "        self.dataName = args.dataName\n",
    "        self.dataSet = DataSet(self.dataName)\n",
    "        self.shape = self.dataSet.shape\n",
    "        self.maxRate = self.dataSet.maxRate\n",
    "\n",
    "        self.train = self.dataSet.train\n",
    "        self.test = self.dataSet.test\n",
    "\n",
    "        self.negNum = args.negNum\n",
    "        self.testNeg = self.dataSet.getTestNeg(self.test, 99)\n",
    "        self.add_embedding_matrix()\n",
    "\n",
    "        self.add_placeholders()\n",
    "\n",
    "        self.userLayer = args.userLayer\n",
    "        self.itemLayer = args.itemLayer\n",
    "        self.add_model()\n",
    "\n",
    "        self.add_loss()\n",
    "\n",
    "        self.lr = args.lr\n",
    "        self.add_train_step()\n",
    "\n",
    "        self.init_sess()\n",
    "\n",
    "        self.maxEpochs = args.maxEpochs\n",
    "        self.batchSize = args.batchSize\n",
    "\n",
    "        self.topK = args.topK\n",
    "        self.earlyStop = args.earlyStop\n",
    "\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        self.user = tf.placeholder(tf.int32)\n",
    "        self.item = tf.placeholder(tf.int32)\n",
    "        self.rate = tf.placeholder(tf.float32)\n",
    "        self.drop = tf.placeholder(tf.float32)\n",
    "\n",
    "    def add_embedding_matrix(self):\n",
    "        self.user_item_embedding = tf.convert_to_tensor(self.dataSet.getEmbedding())\n",
    "        self.item_user_embedding = tf.transpose(self.user_item_embedding)\n",
    "\n",
    "    def add_model(self):\n",
    "        user_input = tf.nn.embedding_lookup(self.user_item_embedding, self.user)\n",
    "        item_input = tf.nn.embedding_lookup(self.item_user_embedding, self.item)\n",
    "\n",
    "        def init_variable(shape, name):\n",
    "            return tf.Variable(tf.truncated_normal(shape=shape, dtype=tf.float32, stddev=0.01), name=name)\n",
    "\n",
    "        with tf.name_scope(\"User_Layer\"):\n",
    "            user_W1 = init_variable([self.shape[1], self.userLayer[0]], \"user_W1\")\n",
    "            user_out = tf.matmul(user_input, user_W1)\n",
    "            for i in range(0, len(self.userLayer)-1):\n",
    "                W = init_variable([self.userLayer[i], self.userLayer[i+1]], \"user_W\"+str(i+2))\n",
    "                b = init_variable([self.userLayer[i+1]], \"user_b\"+str(i+2))\n",
    "                user_out = tf.nn.relu(tf.add(tf.matmul(user_out, W), b))\n",
    "\n",
    "        with tf.name_scope(\"Item_Layer\"):\n",
    "            item_W1 = init_variable([self.shape[0], self.itemLayer[0]], \"item_W1\")\n",
    "            item_out = tf.matmul(item_input, item_W1)\n",
    "            for i in range(0, len(self.itemLayer)-1):\n",
    "                W = init_variable([self.itemLayer[i], self.itemLayer[i+1]], \"item_W\"+str(i+2))\n",
    "                b = init_variable([self.itemLayer[i+1]], \"item_b\"+str(i+2))\n",
    "                item_out = tf.nn.relu(tf.add(tf.matmul(item_out, W), b))\n",
    "\n",
    "        norm_user_output = tf.sqrt(tf.reduce_sum(tf.square(user_out), axis=1))\n",
    "        norm_item_output = tf.sqrt(tf.reduce_sum(tf.square(item_out), axis=1))\n",
    "        self.y_ = tf.reduce_sum(tf.multiply(user_out, item_out), axis=1, keepdims=False) / (norm_item_output* norm_user_output)\n",
    "        self.y_ = tf.maximum(1e-6, self.y_)\n",
    "\n",
    "    def add_loss(self):\n",
    "        regRate = self.rate / self.maxRate\n",
    "        losses = regRate * tf.log(self.y_) + (1 - regRate) * tf.log(1 - self.y_)\n",
    "        loss = -tf.reduce_sum(losses)\n",
    "        # regLoss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "        # self.loss = loss + self.reg * regLoss\n",
    "        self.loss = loss\n",
    "\n",
    "    def add_train_step(self):\n",
    "        '''\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(self.lr, global_step,\n",
    "                                             self.decay_steps, self.decay_rate, staircase=True)\n",
    "        '''\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_step = optimizer.minimize(self.loss)\n",
    "\n",
    "    def init_sess(self):\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.config.allow_soft_placement = True\n",
    "        self.sess = tf.Session(config=self.config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def run(self):\n",
    "        best_hr = -1\n",
    "        best_NDCG = -1\n",
    "        best_epoch = -1\n",
    "        print(\"Start Training!\")\n",
    "        for epoch in range(self.maxEpochs):\n",
    "            print(\"=\"*20+\"Epoch \", epoch, \"=\"*20)\n",
    "            self.run_epoch(self.sess)\n",
    "            print('='*50)\n",
    "            print(\"Start Evaluation!\")\n",
    "            hr, NDCG = self.evaluate(self.sess, self.topK)\n",
    "            print(\"Epoch \", epoch, \"HR: {}, NDCG: {}\".format(hr, NDCG))\n",
    "            if hr > best_hr or NDCG > best_NDCG:\n",
    "                best_hr = hr\n",
    "                best_NDCG = NDCG\n",
    "                best_epoch = epoch\n",
    "            if epoch - best_epoch > self.earlyStop:\n",
    "                print(\"Normal Early stop!\")\n",
    "                break\n",
    "            print(\"=\"*20+\"Epoch \", epoch, \"End\"+\"=\"*20)\n",
    "        print(\"Best hr: {}, NDCG: {}, At Epoch {}\".format(best_hr, best_NDCG, best_epoch))\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    def run_epoch(self, sess, verbose=10):\n",
    "        train_u, train_i, train_r = self.dataSet.getInstances(self.train, self.negNum)\n",
    "        train_len = len(train_u)\n",
    "        shuffled_idx = np.random.permutation(np.arange(train_len))\n",
    "        train_u = train_u[shuffled_idx]\n",
    "        train_i = train_i[shuffled_idx]\n",
    "        train_r = train_r[shuffled_idx]\n",
    "\n",
    "        num_batches = len(train_u) // self.batchSize + 1\n",
    "\n",
    "        losses = []\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([train_len, (i+1)*self.batchSize])\n",
    "            train_u_batch = train_u[min_idx: max_idx]\n",
    "            train_i_batch = train_i[min_idx: max_idx]\n",
    "            train_r_batch = train_r[min_idx: max_idx]\n",
    "            \n",
    "            feed_dict = self.create_feed_dict(train_u_batch, train_i_batch, train_r_batch)\n",
    "            _, tmp_loss = sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "            losses.append(tmp_loss)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : loss = {}'.format(\n",
    "                    i, num_batches, np.mean(losses[-verbose:])\n",
    "                ))\n",
    "                sys.stdout.flush()\n",
    "        loss = np.mean(losses)\n",
    "        print(\"\\nMean loss in this epoch is: {}\".format(loss))\n",
    "        return loss\n",
    "\n",
    "    def create_feed_dict(self, u, i, r=None, drop=None):\n",
    "        return {self.user: u,\n",
    "                self.item: i,\n",
    "                self.rate: r,\n",
    "                self.drop: drop}\n",
    "\n",
    "    def evaluate(self, sess, topK):\n",
    "        def getHitRatio(ranklist, targetItem):\n",
    "            for item in ranklist:\n",
    "                if item == targetItem:\n",
    "                    return 1\n",
    "            return 0\n",
    "        def getNDCG(ranklist, targetItem):\n",
    "            for i in range(len(ranklist)):\n",
    "                item = ranklist[i]\n",
    "                if item == targetItem:\n",
    "                    return math.log(2) / math.log(i+2)\n",
    "            return 0\n",
    "\n",
    "\n",
    "        hr =[]\n",
    "        NDCG = []\n",
    "        testUser = self.testNeg[0]\n",
    "        testItem = self.testNeg[1]\n",
    "        for i in range(len(testUser)):\n",
    "            target = testItem[i][0]\n",
    "            feed_dict = self.create_feed_dict(testUser[i], testItem[i])\n",
    "            predict = sess.run(self.y_, feed_dict=feed_dict)\n",
    "\n",
    "            item_score_dict = {}\n",
    "\n",
    "            for j in range(len(testItem[i])):\n",
    "                item = testItem[i][j]\n",
    "                item_score_dict[item] = predict[j]\n",
    "\n",
    "            ranklist = heapq.nlargest(topK, item_score_dict, key=item_score_dict.get)\n",
    "\n",
    "            tmp_hr = getHitRatio(ranklist, target)\n",
    "            tmp_NDCG = getNDCG(ranklist, target)\n",
    "            hr.append(tmp_hr)\n",
    "            NDCG.append(tmp_NDCG)\n",
    "        return np.mean(hr), np.mean(NDCG)\n",
    "    \n",
    "class DataSet(object):\n",
    "    \n",
    "    def __init__(self, fileName):\n",
    "        self.data, self.shape = self.getData(fileName)\n",
    "        self.train, self.test = self.getTrainTest()\n",
    "        self.trainDict = self.getTrainDict()\n",
    "\n",
    "    def getData(self, fileName):\n",
    "        if fileName == 'ml-1m':\n",
    "            print(\"Loading ml-1m data set...\")\n",
    "            data = []\n",
    "            filePath = '/data/comcode/Deep_Matrix_Factorization_Models/Data/ml-1m/ratings.dat'\n",
    "            u = 0\n",
    "            i = 0\n",
    "            maxr = 0.0\n",
    "            with open(filePath, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line:\n",
    "                        lines = line[:-1].split(\"\\t\")\n",
    "                        user = int(lines[0])\n",
    "                        movie = int(lines[1])\n",
    "                        score = float(lines[2])\n",
    "                        data.append((user, movie, score))\n",
    "                        if user > u:\n",
    "                            u = user\n",
    "                        if movie > i:\n",
    "                            i = movie\n",
    "                        if score > maxr:\n",
    "                            maxr = score\n",
    "            self.maxRate = maxr\n",
    "            print(\"Loading Success!\\n\"\n",
    "                  \"Data Info:\\n\"\n",
    "                  \"\\tUser Num: {}\\n\"\n",
    "                  \"\\tItem Num: {}\\n\"\n",
    "                  \"\\tData Size: {}\".format(u, i, len(data)))\n",
    "            return data, [u+1, i+1]\n",
    "        else:\n",
    "            print(\"Current data set is not support!\")\n",
    "            sys.exit()\n",
    "\n",
    "    def getTrainTest(self):\n",
    "        data = self.data\n",
    "        data = sorted(data, key=lambda x: (x[0]))\n",
    "        train = []\n",
    "        test = []\n",
    "        for i in range(len(data)-1):\n",
    "            user = data[i][0]\n",
    "            item = data[i][1]\n",
    "            rate = data[i][2]\n",
    "            if data[i][0] != data[i+1][0]:\n",
    "                test.append((user, item, rate))\n",
    "            else:\n",
    "                train.append((user, item, rate))\n",
    "\n",
    "        test.append((data[-1][0], data[-1][1], data[-1][2]))\n",
    "        return train, test\n",
    "\n",
    "    def getTrainDict(self):\n",
    "        dataDict = {}\n",
    "        for i in self.train:\n",
    "            dataDict[(i[0], i[1])] = i[2]\n",
    "        return dataDict\n",
    "\n",
    "    def getEmbedding(self):\n",
    "        train_matrix = np.zeros([self.shape[0], self.shape[1]], dtype=np.float32)\n",
    "        for i in self.train:\n",
    "            user = i[0]\n",
    "            movie = i[1]\n",
    "            rating = i[2]\n",
    "            train_matrix[user][movie] = rating\n",
    "        return np.array(train_matrix)\n",
    "\n",
    "    def getInstances(self, data, negNum):\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        for i in data:\n",
    "            user.append(i[0])\n",
    "            item.append(i[1])\n",
    "            rate.append(i[2])\n",
    "            for t in range(negNum):\n",
    "                j = np.random.randint(self.shape[1])\n",
    "                while (i[0], j) in self.trainDict:\n",
    "                    j = np.random.randint(self.shape[1])\n",
    "                user.append(i[0])\n",
    "                item.append(j)\n",
    "                rate.append(0.0)\n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "\n",
    "    def getTestNeg(self, testData, negNum):\n",
    "        user = []\n",
    "        item = []\n",
    "        for s in testData:\n",
    "            tmp_user = []\n",
    "            tmp_item = []\n",
    "            u = s[0]\n",
    "            i = s[1]\n",
    "            tmp_user.append(u)\n",
    "            tmp_item.append(i)\n",
    "            neglist = set()\n",
    "            neglist.add(i)\n",
    "            for t in range(negNum):\n",
    "                j = np.random.randint(self.shape[1])\n",
    "                while (u, j) in self.trainDict or j in neglist:\n",
    "                    j = np.random.randint(self.shape[1])\n",
    "                neglist.add(j)\n",
    "                tmp_user.append(u)\n",
    "                tmp_item.append(j)\n",
    "            user.append(tmp_user)\n",
    "            item.append(tmp_item)\n",
    "        return [np.array(user), np.array(item)]\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Options\")\n",
    "\n",
    "    parser.add_argument('-dataName', action='store', dest='dataName', default='ml-1m')\n",
    "    parser.add_argument('-negNum', action='store', dest='negNum', default=7, type=int)\n",
    "    parser.add_argument('-userLayer', action='store', dest='userLayer', default=[512, 8])\n",
    "    parser.add_argument('-itemLayer', action='store', dest='itemLayer', default=[1024, 8])\n",
    "    parser.add_argument('-lr', action='store', dest='lr', default=0.001)\n",
    "    parser.add_argument('-maxEpochs', action='store', dest='maxEpochs', default=20, type=int)\n",
    "    parser.add_argument('-batchSize', action='store', dest='batchSize', default=256, type=int)\n",
    "    parser.add_argument('-earlyStop', action='store', dest='earlyStop', default=5)\n",
    "    parser.add_argument('-topK', action='store', dest='topK', default=10)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    classifier = Model(args)\n",
    "\n",
    "    classifier.run()\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.16\n",
    "@function: Implementing DMF with Torch  \n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "           https://www.ijcai.org/proceedings/2017/0447.pdf\n",
    "           https://github.com/RuidongZ/Deep_Matrix_Factorization_Models\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import heapq\n",
    "import math\n",
    "import sys\n",
    "\n",
    "#define class DMF\n",
    "class DMF():\n",
    "    \n",
    "    def __init__(self, trainset, trainmatrix, traindict, testset, maxr, maxu, maxi, K=64, neg_num=4, lr=0.001):\n",
    "        #getting trainset and testset.\n",
    "        self.trainset = trainset\n",
    "        self.trainmatrix = trainmatrix\n",
    "        self.traindict = traindict\n",
    "        self.testset = testset\n",
    "        \n",
    "        #setting parameters\n",
    "        self.maxr = maxr\n",
    "        self.maxu = maxu+1\n",
    "        self.maxi = maxi+1\n",
    "        self.neg_num = neg_num\n",
    "        self.lr = lr\n",
    "        self.batchSize = 256\n",
    "        self.userLayer = [512, K]#user hidden layers\n",
    "        self.itemLayer = [1024, K]#item hidden layers\n",
    "        \n",
    "        #init model\n",
    "        self.add_embedding_matrix()\n",
    "        self.add_placeholders()\n",
    "        self.add_model()\n",
    "        self.add_loss()\n",
    "        self.add_train_step()\n",
    "        self.init_sess()\n",
    "        \n",
    "    def add_embedding_matrix(self):\n",
    "        self.user_item_embedding = tf.convert_to_tensor(self.trainmatrix)\n",
    "        self.item_user_embedding = tf.transpose(self.user_item_embedding)\n",
    "        \n",
    "    def add_placeholders(self):\n",
    "        self.user = tf.placeholder(tf.int32)\n",
    "        self.item = tf.placeholder(tf.int32)\n",
    "        self.rate = tf.placeholder(tf.float32)\n",
    "        self.drop = tf.placeholder(tf.float32)\n",
    "        \n",
    "    def add_model(self):#network structure\n",
    "        user_input = tf.nn.embedding_lookup(self.user_item_embedding, self.user)\n",
    "        item_input = tf.nn.embedding_lookup(self.item_user_embedding, self.item)\n",
    "\n",
    "        def init_variable(shape, name):\n",
    "            return tf.Variable(tf.truncated_normal(shape=shape, dtype=tf.float32, stddev=0.01), name=name)\n",
    "\n",
    "        with tf.name_scope(\"User_Layer\"):\n",
    "            user_W1 = init_variable([self.maxi, self.userLayer[0]], \"user_W1\")\n",
    "            user_out = tf.matmul(user_input, user_W1)\n",
    "            for i in range(0, len(self.userLayer)-1):\n",
    "                W = init_variable([self.userLayer[i], self.userLayer[i+1]], \"user_W\"+str(i+2))\n",
    "                b = init_variable([self.userLayer[i+1]], \"user_b\"+str(i+2))\n",
    "                user_out = tf.nn.relu(tf.add(tf.matmul(user_out, W), b))\n",
    "\n",
    "        with tf.name_scope(\"Item_Layer\"):\n",
    "            item_W1 = init_variable([self.maxu, self.itemLayer[0]], \"item_W1\")\n",
    "            item_out = tf.matmul(item_input, item_W1)\n",
    "            for i in range(0, len(self.itemLayer)-1):\n",
    "                W = init_variable([self.itemLayer[i], self.itemLayer[i+1]], \"item_W\"+str(i+2))\n",
    "                b = init_variable([self.itemLayer[i+1]], \"item_b\"+str(i+2))\n",
    "                item_out = tf.nn.relu(tf.add(tf.matmul(item_out, W), b))\n",
    "\n",
    "        norm_user_output = tf.sqrt(tf.reduce_sum(tf.square(user_out), axis=1))\n",
    "        norm_item_output = tf.sqrt(tf.reduce_sum(tf.square(item_out), axis=1))\n",
    "        self.y_ = tf.reduce_sum(tf.multiply(user_out, item_out), axis=1, keep_dims=False) / (norm_item_output* norm_user_output)\n",
    "        self.y_ = tf.maximum(1e-6, self.y_)\n",
    "        \n",
    "        \n",
    "    def add_loss(self):#normalized cross-entropy loss\n",
    "        regRate = self.rate / self.maxr\n",
    "        losses = regRate * tf.log(self.y_) + (1 - regRate) * tf.log(1 - self.y_)\n",
    "        loss = -tf.reduce_sum(losses)\n",
    "        # regLoss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "        # self.loss = loss + self.reg * regLoss\n",
    "        self.loss = loss\n",
    "    \n",
    "    def add_train_step(self):\n",
    "        '''\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(self.lr, global_step,\n",
    "                                             self.decay_steps, self.decay_rate, staircase=True)\n",
    "        '''\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_step = optimizer.minimize(self.loss)\n",
    "        \n",
    "    def init_sess(self):\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.config.allow_soft_placement = True\n",
    "        self.sess = tf.Session(config=self.config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        '''\n",
    "        self.saver = tf.train.Saver()\n",
    "        if os.path.exists(self.checkPoint):\n",
    "            [os.remove(f) for f in os.listdir(self.checkPoint)]\n",
    "        else:\n",
    "            os.mkdir(self.checkPoint)\n",
    "        '''\n",
    "        \n",
    "    def run(self):\n",
    "        best_hr, best_ndcg= 0.0, 0.0\n",
    "        for epoch in range(1):#default 20\n",
    "            loss = self.run_epoch(self.sess)\n",
    "            print(\"\\nMean loss in this epoch is: {}\".format(loss))\n",
    "            hr, ndcg = self.evaluate(self.sess, topK=10)#default recommend top 10 items\n",
    "            print(\"Epoch \", epoch, \"HR: {}, NDCG: {}\".format(hr, ndcg))\n",
    "            #self.saver.save(self.sess, self.checkPoint)\n",
    "            if hr>best_hr: best_hr=hr\n",
    "            if ndcg>best_ndcg: best_ndcg=ndcg\n",
    "        return best_hr, best_ndcg\n",
    "    \n",
    "    def run_epoch(self, sess, verbose=10):\n",
    "        train_u, train_i, train_r = self.getInstances()\n",
    "        train_len = len(train_u)\n",
    "        shuffled_idx = np.random.permutation(np.arange(train_len))\n",
    "        train_u = train_u[shuffled_idx]\n",
    "        train_i = train_i[shuffled_idx]\n",
    "        train_r = train_r[shuffled_idx]\n",
    "\n",
    "        num_batches = len(train_u) // self.batchSize + 1\n",
    "\n",
    "        losses = []\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([train_len, (i+1)*self.batchSize])\n",
    "            train_u_batch = train_u[min_idx: max_idx]\n",
    "            train_i_batch = train_i[min_idx: max_idx]\n",
    "            train_r_batch = train_r[min_idx: max_idx]\n",
    "\n",
    "            feed_dict = self.create_feed_dict(train_u_batch, train_i_batch, train_r_batch)\n",
    "            _, tmp_loss = sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "            losses.append(tmp_loss)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : loss = {}'.format(i, num_batches, np.mean(losses[-verbose:])))\n",
    "                sys.stdout.flush()\n",
    "        loss = np.mean(losses)\n",
    "        print(\"\\nMean loss in this epoch is: {}\".format(loss))\n",
    "        return loss\n",
    "       \n",
    "    def getInstances(self):\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        for i in self.trainset:\n",
    "            user.append(i[0])\n",
    "            item.append(i[1])\n",
    "            rate.append(i[2])\n",
    "            for t in range(self.neg_num):\n",
    "                j = np.random.randint(self.maxi)\n",
    "                while (i[0], j) in self.traindict:\n",
    "                    j = np.random.randint(self.maxi)\n",
    "                user.append(i[0])\n",
    "                item.append(j)\n",
    "                rate.append(0.0)\n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "    \n",
    "    def create_feed_dict(self, u, i, r=None, drop=None):\n",
    "        return {self.user: u,\n",
    "                self.item: i,\n",
    "                self.rate: r,\n",
    "                self.drop: drop}\n",
    "    \n",
    "    def evaluate(self, sess, topK):\n",
    "        def getHitRatio(ranklist, targetItem):\n",
    "            for item in ranklist:\n",
    "                if item == targetItem:\n",
    "                    return 1\n",
    "            return 0\n",
    "        def getNDCG(ranklist, targetItem):\n",
    "            for i in range(len(ranklist)):\n",
    "                item = ranklist[i]\n",
    "                if item == targetItem:\n",
    "                    return math.log(2) / math.log(i+2)\n",
    "            return 0\n",
    "        \n",
    "        hr =[]\n",
    "        NDCG = []\n",
    "        testUser = self.testset[0]\n",
    "        testItem = self.testset[1]\n",
    "        for i in range(len(testUser)):\n",
    "            target = testItem[i][0]\n",
    "            feed_dict = self.create_feed_dict(testUser[i], testItem[i])\n",
    "            predict = sess.run(self.y_, feed_dict=feed_dict)\n",
    "\n",
    "            item_score_dict = {}\n",
    "\n",
    "            for j in range(len(testItem[i])):\n",
    "                item = testItem[i][j]\n",
    "                item_score_dict[item] = predict[j]\n",
    "\n",
    "            ranklist = heapq.nlargest(topK, item_score_dict, key=item_score_dict.get)\n",
    "\n",
    "            tmp_hr = getHitRatio(ranklist, target)\n",
    "            tmp_NDCG = getNDCG(ranklist, target)\n",
    "            hr.append(tmp_hr)\n",
    "            NDCG.append(tmp_NDCG)\n",
    "        return np.mean(hr), np.mean(NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 32.9250\n",
      "  K               HR@10             NDCG@10\n"
     ]
    }
   ],
   "source": [
    "#1.loading dataset\n",
    "def getTrainset(filePath):\n",
    "    trainset = []\n",
    "    maxu = 0 \n",
    "    maxi = 0 \n",
    "    maxr = 0.0\n",
    "    with open(filePath, 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            u, i, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            trainset.append([int(arr[0]), int(arr[1]), float(arr[2])])\n",
    "            if rating > maxr: maxr = rating\n",
    "            if u > maxu: maxu = u\n",
    "            if i > maxi: maxi = i\n",
    "            line = fd.readline()\n",
    "        return trainset, maxr, maxu, maxi\n",
    "\n",
    "def getTestset(filePath):\n",
    "    testset = []\n",
    "    with open(filePath, 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            testset.append([u, eval(arr[0])[1]])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                testset.append([u, int(i)]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return testset     \n",
    "\n",
    "def getTestPosNeg(testset):\n",
    "    user = []\n",
    "    item = []\n",
    "    u_prev = testset[0][0]\n",
    "    tmp_user = []\n",
    "    tmp_item = []\n",
    "    for u, i in testset:\n",
    "        if u_prev ==u:\n",
    "            tmp_user.append(u)\n",
    "            tmp_item.append(i)\n",
    "        else:\n",
    "            user.append(tmp_user)\n",
    "            item.append(tmp_item)\n",
    "            tmp_user = []\n",
    "            tmp_item = []\n",
    "            tmp_user.append(u)\n",
    "            tmp_item.append(i)\n",
    "        u_prev = u\n",
    "    return [np.array(user), np.array(item)]\n",
    "\n",
    "def getTrainMatrix(trainset, maxu, maxi):\n",
    "    train_matrix = np.zeros([maxu+1, maxi+1], dtype=np.float32)\n",
    "    for i in trainset:\n",
    "        user = i[0]\n",
    "        movie = i[1]\n",
    "        rating = i[2]\n",
    "        train_matrix[user][movie] = rating\n",
    "    return np.array(train_matrix)\n",
    "\n",
    "def getTrainDict(trainset):\n",
    "    dataDict = {}\n",
    "    for i in trainset:\n",
    "        dataDict[(i[0], i[1])] = i[2]\n",
    "    return dataDict\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    trainset, maxr, maxu, maxi = getTrainset(\"/data/fjsdata/ctKngBase/ml/ml-1m.train.rating\")\n",
    "    testset = getTestset(\"/data/fjsdata/ctKngBase/ml/ml-1m.test.negative\")\n",
    "    trainmatrix = getTrainMatrix(trainset, maxu, maxi)\n",
    "    traindict = getTrainDict(trainset)\n",
    "    testset = getTestPosNeg(testset)\n",
    "    print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % (len(trainset), maxu+1, maxi+1, len(trainset)/(maxu*maxr)))\n",
    "    print (\"%3s%20s%20s\" % ('K','HR@10', 'NDCG@10'))\n",
    "    for K in [8,16,32,64]:\n",
    "        mdl = DMF(trainset, trainmatrix, traindict, testset, maxr, maxu, maxi, K=K, neg_num = 4, lr = 0.001) #the ratio of samples is pos 1: neg 4\n",
    "        best_hr, best_ndcg = mdl.run()\n",
    "        print (\"%3d%20.6f%20.6f\" % (K, best_hr, best_ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6265938069216758\n",
      "0.365656729630621\n"
     ]
    }
   ],
   "source": [
    "def evaluate(topK):\n",
    "        def getHitRatio(ranklist, targetItem):\n",
    "            for item in ranklist:\n",
    "                if item == targetItem:\n",
    "                    return 1\n",
    "            return 0\n",
    "        def getNDCG(ranklist, targetItem):\n",
    "            for i in range(len(ranklist)):\n",
    "                item = ranklist[i]\n",
    "                if item == targetItem:\n",
    "                    return math.log(2) / math.log(i+2)\n",
    "            return 0\n",
    "        \n",
    "        hr =[]\n",
    "        NDCG = []\n",
    "        testUser = mdl.testPosNeg[0]\n",
    "        testItem = mdl.testPosNeg[1]\n",
    "        for i in range(len(testUser)):\n",
    "            target = testItem[i][0]\n",
    "            feed_dict = mdl.create_feed_dict(testUser[i], testItem[i])\n",
    "            predict = mdl.sess.run(mdl.y_, feed_dict=feed_dict)\n",
    "            item_score_dict = {}\n",
    "\n",
    "            for j in range(len(testItem[i])):\n",
    "                item = testItem[i][j]\n",
    "                item_score_dict[item] = predict[j]\n",
    "\n",
    "            ranklist = heapq.nlargest(topK, item_score_dict, key=item_score_dict.get)\n",
    "            \n",
    "            tmp_hr = getHitRatio(ranklit, target)\n",
    "            tmp_NDCG = getNDCG(ranklist, target)\n",
    "            hr.append(tmp_hr)\n",
    "            NDCG.append(tmp_NDCG)\n",
    "        return np.mean(hr), np.mean(NDCG)\n",
    "hr,ndcg = evaluate(10)\n",
    "print (hr)\n",
    "print (ndcg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
