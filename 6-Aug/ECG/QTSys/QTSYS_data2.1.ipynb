{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import baostock as bs#pip install baostock\n",
    "import mplfinance as mpf #pip install mplfinance\n",
    "from matplotlib.pylab import date2num\n",
    "import datetime\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.header import Header\n",
    "torch.cuda.set_device(2)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Training model and output database(faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407 / 407 "
     ]
    }
   ],
   "source": [
    "#Generate Dataset\n",
    "root_dir = '/data/fjsdata/qtsys/img/' #the path of images\n",
    "data = pd.read_csv('/data/fjsdata/qtsys/label.csv') \n",
    "data = data.sample(frac=1).reset_index(drop=True) #shuffle\n",
    "#Dataset\n",
    "trN,trI, trY =[], [],[]\n",
    "for _, row in data.iterrows():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, row['name'])\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(1600,800,3)->(256,256,3)\n",
    "        trN.append(row['name'])\n",
    "        trI.append(img)\n",
    "        if row['label']=='B':\n",
    "            trY.append(0) #buy\n",
    "        else:# row['label']=='S':\n",
    "            trY.append(1) #sell\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(trY),data.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs():\n",
    "    if (len(trY) % 2) == 0: spls = len(trY)\n",
    "    else:  spls = len(trY)-1\n",
    "    idx_sf = random.sample(range(0, spls),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "\n",
    "#define model: ASH\n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ASHNet(nn.Module):\n",
    "    def __init__(self, code_size: int):\n",
    "        super().__init__()\n",
    "        #Resnet\n",
    "        self.net = nn.Sequential(\n",
    "            ResBlock(in_channels=3, out_channels=16),\n",
    "            ResBlock(in_channels=16, out_channels=16),\n",
    "            ResBlock(in_channels=16, out_channels=16, stride=2),\n",
    "        ) \n",
    "        #Attention \n",
    "        self.sa = SpatialAttention() \n",
    "        #fully connected\n",
    "        self.linear = nn.Sequential(\n",
    "            #nn.Linear(16*128*128, 4096),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.Linear(16*128*128, code_size),\n",
    "            #nn.ReLU(inplace=True) #nn.Tanh()#[-1,1]\n",
    "        )\n",
    "        \n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.sa(x)*x\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "#dfine loss function:pairwise loss            \n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.l1_loss = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    def forward(self,h1,h2,y):    \n",
    "        margin_val = self.margin * h1.shape[1]\n",
    "        squared_loss = torch.mean(self.mse_loss(h1, h2), dim=1)\n",
    "        # T1: 0.5 * (1 - y) * dist(x1, x2)\n",
    "        positive_pair_loss = (0.5 * (1 - y) * squared_loss)\n",
    "        mean_positive_pair_loss = torch.mean(positive_pair_loss)\n",
    "        # T2: 0.5 * y * max(margin - dist(x1, x2), 0)\n",
    "        zeros = torch.zeros_like(squared_loss)\n",
    "        marginMat = margin_val * torch.ones_like(squared_loss)\n",
    "        negative_pair_loss = 0.5 * y * torch.max(zeros, marginMat - squared_loss)\n",
    "        mean_negative_pair_loss = torch.mean(negative_pair_loss)\n",
    "\n",
    "        # T3: alpha(dst_l1(abs(x1), 1)) + dist_l1(abs(x2), 1)))\n",
    "        mean_value_regularization = self.alpha * (\n",
    "                self.l1_loss(torch.abs(h1), torch.ones_like(h1)) +\n",
    "                self.l1_loss(torch.abs(h2), torch.ones_like(h2)))\n",
    "\n",
    "        loss = mean_positive_pair_loss + mean_negative_pair_loss + mean_value_regularization\n",
    "        return loss\n",
    "\n",
    "\n",
    "#train model\n",
    "hash_size=12\n",
    "model = ASHNet(code_size=hash_size).cuda()\n",
    "criterion  = HashLossFunc(margin=0.5).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(50):#iteration\n",
    "    trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs()\n",
    "    losses = []\n",
    "    num_batches = len(trY_sf) // batchSize +1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        #sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        #sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    #print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "loss=loss.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#output the feature with best model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    X_batch = torch.tanh(X_batch) #[-1,1]\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# buliding index for retrieval\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(hash_size) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate the K line and retrieve from database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login success!\n",
      "sh.600637-B-sh.600028-1.png\n",
      "sz.000766-B-sh.600038-6.png\n",
      "sz.000997-B-sh.600196-3.png\n",
      "sh.603888-B-sh.600025-2.png\n",
      "sz.002368-B-sz.002373-0319.png\n",
      "sh.603377-B-sz.002739-1.png\n",
      "sz.000425-B-sh.603986-3.png\n",
      "sh.600703-B-sz.002736-0319.png\n",
      "sz.300033-B-sh.601998-4.png\n",
      "logout success!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<baostock.data.resultset.ResultData at 0x7f659079e290>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate MACD\n",
    "def cal_macd_system(data,short_,long_,m):\n",
    "    '''\n",
    "    data=['Open','High','Low','Close','Volume']\n",
    "    parameter: short_,long_,m\n",
    "    return:data=['Open','High','Low','Close','Volume','diff','dea','macd']\n",
    "    '''\n",
    "    data['diff']=data['Close'].ewm(adjust=False,alpha=2/(short_+1),ignore_na=True).mean()-\\\n",
    "                data['Close'].ewm(adjust=False,alpha=2/(long_+1),ignore_na=True).mean()\n",
    "    data['dea']=data['diff'].ewm(adjust=False,alpha=2/(m+1),ignore_na=True).mean()\n",
    "    data['macd']=2*(data['diff']-data['dea'])\n",
    "    return data\n",
    "def macd_zero(macd):\n",
    "    pos_signal, neg_signal = [],[]\n",
    "    for idx,value in macd.iteritems():\n",
    "        if value > 0:\n",
    "            pos_signal.append(value)\n",
    "            neg_signal.append(np.nan)\n",
    "        else:\n",
    "            neg_signal.append(value)\n",
    "            pos_signal.append(np.nan)\n",
    "    return pos_signal,neg_signal\n",
    "\n",
    "#http://baostock.com/baostock/index.php/Python_API\n",
    "#generate market chart\n",
    "lg = bs.login() #login\n",
    "#read stocks information\n",
    "rs_hs300 = bs.query_hs300_stocks() \n",
    "rs_zh500 = bs.query_zz500_stocks()\n",
    "rs_sh50 = bs.query_sz50_stocks()\n",
    "hs_stocks = []\n",
    "while (rs_hs300.error_code == '0') & rs_hs300.next():\n",
    "    hs_stocks.append(rs_hs300.get_row_data()[1])\n",
    "while (rs_zh500.error_code == '0') & rs_zh500.next():\n",
    "    hs_stocks.append(rs_zh500.get_row_data()[1])\n",
    "while (rs_sh50.error_code == '0') & rs_sh50.next():\n",
    "    hs_stocks.append(rs_sh50.get_row_data()[1])\n",
    "hs_stocks = list(set(hs_stocks)) #get rid of repeated stock\n",
    "#read k data\n",
    "fields= \"Date,Code,Open,High,Low,Close,Volume\"\n",
    "for code in hs_stocks:\n",
    "    #read transaction data\n",
    "    rs = bs.query_history_k_data(code=code, fields=fields, \\\n",
    "                                 start_date='2020-02-23', end_date='2020-03-21', \\\n",
    "                                 frequency=\"30\",adjustflag=\"3\") #20days，one k line per 30 minutes\n",
    "    data_list = []\n",
    "    while (rs.error_code == '0') & rs.next():\n",
    "        data_list.append(rs.get_row_data())\n",
    "    result = pd.DataFrame(data_list, columns=rs.fields)\n",
    "    result=result.apply(pd.to_numeric, errors='ignore')\n",
    "    if result.shape[0] ==160:\n",
    "        #plot K line \n",
    "        result = result[['Open','High','Low','Close','Volume']]\n",
    "        result.index=pd.to_datetime(result.index)#turn index to datatime\n",
    "        result = cal_macd_system(result,12,26,9)\n",
    "        pos_macd, neg_macd  = macd_zero(result['macd']) \n",
    "        apds = [ mpf.make_addplot(result['diff'],panel='lower',color='b'),\n",
    "                     mpf.make_addplot(result['dea'],panel='lower',color='y'),\n",
    "                     mpf.make_addplot(pos_macd,panel='lower',color='r',scatter=True),\n",
    "                     mpf.make_addplot(neg_macd,panel='lower',color='g',scatter=True)\n",
    "                   ]\n",
    "        kwargs = dict(type='candle',figratio =(16,8),volume=False,figscale=1)#line，mav=(5,10)\n",
    "        Kline_path ='/data/fjsdata/qtsys/real0320/'+code+'.png'\n",
    "        save = dict(fname=Kline_path,dpi=100, pad_inches=0.2)\n",
    "        mpf.plot(result,**kwargs,addplot=apds,style='sas',savefig=save)#charles\n",
    "        plt.close()\n",
    "        Kline_img = cv2.resize(cv2.imread(Kline_path).astype(np.float32), (256, 256)) #read image \n",
    "        teI = []\n",
    "        teI.append(Kline_img)\n",
    "        #output feature with model\n",
    "        teI = torch.from_numpy(np.array(teI)).type(torch.FloatTensor).cuda()\n",
    "        teI = best_net(teI.permute(0, 3, 1, 2))#forword\n",
    "        teI = torch.tanh(teI) #[-1,1]\n",
    "        teI = teI.cpu().data.numpy().tolist()\n",
    "        #retrieve from DB\n",
    "        #np.linalg.norm(vec1 - vec2) #consine l2-norm \n",
    "        scores, neighbors = gpu_index.search(np.ascontiguousarray(teI, dtype=np.float32), k=1) #return top1\n",
    "        if scores.flatten()[0]< 0.002: #similarity for sell\n",
    "            label = trY[neighbors.flatten()[0]] \n",
    "            name = trN[neighbors.flatten()[0]]\n",
    "            if label == 1:\n",
    "                print('%s-S-%s'%(code,name))\n",
    "            elif (label == 0 and scores.flatten()[0]< 0.001): #similarity for buy\n",
    "                print('%s-B-%s'%(code,name))\n",
    "            else:os.remove(Kline_path) #remove the image file if no handle\n",
    "        else: os.remove(Kline_path) #remove the image file if no handle \n",
    "    #else: print('Error in collecting data:%s' % (code))\n",
    "bs.logout()#logout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   name    open pre_close   price    high     low     bid     ask    volume  \\\n",
      "0  招商银行  29.800    29.310  30.280  30.890  29.800  30.270  30.280  81172164   \n",
      "\n",
      "           amount  ...    a2_p a3_v    a3_p a4_v    a4_p a5_v    a5_p  \\\n",
      "0  2469136983.000  ...  30.290  850  30.300   42  30.310   40  30.320   \n",
      "\n",
      "         date      time    code  \n",
      "0  2020-03-20  11:20:09  600036  \n",
      "\n",
      "[1 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import tushare as ts\n",
    "\n",
    "#ts.get_today_all()\n",
    "df = ts.get_realtime_quotes('600036')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
