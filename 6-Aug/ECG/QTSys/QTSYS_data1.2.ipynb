{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import baostock as bs#pip install baostock\n",
    "import mplfinance as mpf #pip install mplfinance\n",
    "from matplotlib.pylab import date2num\n",
    "import datetime\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.header import Header\n",
    "torch.cuda.set_device(0)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371, 2)\n",
      "371 / 371 The length of train set is 333\n",
      "The length of test set is 38\n",
      " 17 / 17 : loss = 0.682281Eopch:     1 mean_loss = 1.112316\n",
      " 17 / 17 : loss = 1.082869Eopch:     2 mean_loss = 1.490716\n",
      " 17 / 17 : loss = 0.807407Eopch:     3 mean_loss = 1.073985\n",
      " 17 / 17 : loss = 0.882276Eopch:     4 mean_loss = 0.562639\n",
      " 17 / 17 : loss = 0.841838Eopch:     5 mean_loss = 0.499972\n",
      " 17 / 17 : loss = 0.072625Eopch:     6 mean_loss = 0.270806\n",
      " 17 / 17 : loss = 0.153274Eopch:     7 mean_loss = 0.268146\n",
      " 17 / 17 : loss = 0.101438Eopch:     8 mean_loss = 0.148876\n",
      " 17 / 17 : loss = 0.169425Eopch:     9 mean_loss = 0.154675\n",
      " 17 / 17 : loss = 0.074092Eopch:    10 mean_loss = 0.092092\n",
      " 17 / 17 : loss = 0.038789Eopch:    11 mean_loss = 0.073155\n",
      " 17 / 17 : loss = 0.031585Eopch:    12 mean_loss = 0.051518\n",
      " 17 / 17 : loss = 0.050454Eopch:    13 mean_loss = 0.143127\n",
      " 17 / 17 : loss = 0.322315Eopch:    14 mean_loss = 0.094282\n",
      " 17 / 17 : loss = 0.027282Eopch:    15 mean_loss = 0.088943\n",
      " 17 / 17 : loss = 0.050266Eopch:    16 mean_loss = 0.060307\n",
      " 17 / 17 : loss = 0.053411Eopch:    17 mean_loss = 0.085426\n",
      " 17 / 17 : loss = 0.014575Eopch:    18 mean_loss = 0.054928\n",
      " 17 / 17 : loss = 0.058729Eopch:    19 mean_loss = 0.061722\n",
      " 17 / 17 : loss = 0.051538Eopch:    20 mean_loss = 0.050535\n",
      " 17 / 17 : loss = 0.032456Eopch:    21 mean_loss = 0.043755\n",
      " 17 / 17 : loss = 0.013379Eopch:    22 mean_loss = 0.033391\n",
      " 17 / 17 : loss = 0.012478Eopch:    23 mean_loss = 0.024047\n",
      " 17 / 17 : loss = 0.141494Eopch:    24 mean_loss = 0.030595\n",
      " 17 / 17 : loss = 0.022945Eopch:    25 mean_loss = 0.020141\n",
      " 17 / 17 : loss = 0.018693Eopch:    26 mean_loss = 0.035751\n",
      " 17 / 17 : loss = 0.030284Eopch:    27 mean_loss = 0.020925\n",
      " 17 / 17 : loss = 0.027284Eopch:    28 mean_loss = 0.020069\n",
      " 17 / 17 : loss = 0.011986Eopch:    29 mean_loss = 0.019758\n",
      " 17 / 17 : loss = 0.011019Eopch:    30 mean_loss = 0.012563\n",
      " 17 / 17 : loss = 0.022324Eopch:    31 mean_loss = 0.021555\n",
      " 17 / 17 : loss = 0.016587Eopch:    32 mean_loss = 0.019893\n",
      " 17 / 17 : loss = 0.013635Eopch:    33 mean_loss = 0.016857\n",
      " 17 / 17 : loss = 0.014886Eopch:    34 mean_loss = 0.014301\n",
      " 17 / 17 : loss = 0.016251Eopch:    35 mean_loss = 0.019567\n",
      " 17 / 17 : loss = 0.015206Eopch:    36 mean_loss = 0.015345\n",
      " 17 / 17 : loss = 0.010193Eopch:    37 mean_loss = 0.015937\n",
      " 17 / 17 : loss = 0.019252Eopch:    38 mean_loss = 0.017590\n",
      " 17 / 17 : loss = 0.014264Eopch:    39 mean_loss = 0.019516\n",
      " 17 / 17 : loss = 0.017596Eopch:    40 mean_loss = 0.019122\n",
      " 17 / 17 : loss = 0.010401Eopch:    41 mean_loss = 0.016914\n",
      " 17 / 17 : loss = 0.017296Eopch:    42 mean_loss = 0.014883\n",
      " 17 / 17 : loss = 0.013651Eopch:    43 mean_loss = 0.016796\n",
      " 17 / 17 : loss = 0.011452Eopch:    44 mean_loss = 0.014482\n",
      " 17 / 17 : loss = 0.017969Eopch:    45 mean_loss = 0.018425\n",
      " 17 / 17 : loss = 0.014877Eopch:    46 mean_loss = 0.024198\n",
      " 17 / 17 : loss = 0.044199Eopch:    47 mean_loss = 0.019602\n",
      " 17 / 17 : loss = 0.112761Eopch:    48 mean_loss = 0.025745\n",
      " 17 / 17 : loss = 0.011258Eopch:    49 mean_loss = 0.017969\n",
      " 17 / 17 : loss = 0.006862Eopch:    50 mean_loss = 0.017457\n",
      "best_loss = 0.012563\n",
      " 3 / 4 4 Completed buliding index in 1 seconds\n",
      "Accuracy: 0.973684\n",
      "[[11  0]\n",
      " [ 1 26]]\n",
      "Sensitivity of B: 1.000000\n",
      "Sensitivity of S: 0.962963\n"
     ]
    }
   ],
   "source": [
    "#Generate Dataset\n",
    "root_dir = '/data/fjsdata/qtsys/img/' #the path of images\n",
    "data = pd.read_csv('/data/fjsdata/qtsys/label.csv') \n",
    "data['label'] = data['label'].fillna('H')\n",
    "data=data[data['label']!='H']\n",
    "print(data.shape)\n",
    "data = data.sample(frac=1).reset_index(drop=True) #shuffle\n",
    "#Dataset\n",
    "X, Y = [],[]\n",
    "for _, row in data.iterrows():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, row['name'])\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(1600,800,3)->(256,256,3)\n",
    "        X.append(img)\n",
    "        if row['label']=='B':\n",
    "            Y.append(0) #buy\n",
    "        else:# row['label']=='S':\n",
    "            Y.append(1) #sell\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(Y),data.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "#split trainset and testset \n",
    "trI, teI, trY, teY = train_test_split(X, Y, test_size=0.1, random_state=42) #list after return\n",
    "print('The length of train set is %d'%len(trI))\n",
    "print('The length of test set is %d'%len(teI))\n",
    "\n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ASHNet(nn.Module):\n",
    "    def __init__(self, code_size: int):\n",
    "        super().__init__()\n",
    "        #Resnet\n",
    "        self.net = nn.Sequential(\n",
    "            ResBlock(in_channels=3, out_channels=16),\n",
    "            ResBlock(in_channels=16, out_channels=16),\n",
    "            ResBlock(in_channels=16, out_channels=16, stride=2),\n",
    "        ) \n",
    "        #Attention \n",
    "        self.sa = SpatialAttention() \n",
    "        #fully connected\n",
    "        self.linear = nn.Sequential(\n",
    "            #nn.Linear(16*128*128, 4096),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.Linear(16*128*128, code_size),\n",
    "            #nn.ReLU(inplace=True) #nn.Tanh()#[-1,1]\n",
    "        )\n",
    "        \n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.sa(x)*x\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "            \n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.l1_loss = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    def forward(self,h1,h2,y):    \n",
    "        margin_val = self.margin * h1.shape[1]\n",
    "        squared_loss = torch.mean(self.mse_loss(h1, h2), dim=1)\n",
    "        # T1: 0.5 * (1 - y) * dist(x1, x2)\n",
    "        positive_pair_loss = (0.5 * (1 - y) * squared_loss)\n",
    "        mean_positive_pair_loss = torch.mean(positive_pair_loss)\n",
    "        # T2: 0.5 * y * max(margin - dist(x1, x2), 0)\n",
    "        zeros = torch.zeros_like(squared_loss)\n",
    "        marginMat = margin_val * torch.ones_like(squared_loss)\n",
    "        negative_pair_loss = 0.5 * y * torch.max(zeros, marginMat - squared_loss)\n",
    "        mean_negative_pair_loss = torch.mean(negative_pair_loss)\n",
    "\n",
    "        # T3: alpha(dst_l1(abs(x1), 1)) + dist_l1(abs(x2), 1)))\n",
    "        mean_value_regularization = self.alpha * (\n",
    "                self.l1_loss(torch.abs(h1), torch.ones_like(h1)) +\n",
    "                self.l1_loss(torch.abs(h2), torch.ones_like(h2)))\n",
    "\n",
    "        loss = mean_positive_pair_loss + mean_negative_pair_loss + mean_value_regularization\n",
    "        return loss\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs():\n",
    "    spls = len(trY)-1\n",
    "    idx_sf = random.sample(range(0, spls),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "\n",
    "\n",
    "#define model\n",
    "hash_size=12\n",
    "model = ASHNet(code_size=hash_size).cuda()\n",
    "criterion  = HashLossFunc(margin=0.5).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(50):#iteration\n",
    "    trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs()\n",
    "    losses = []\n",
    "    num_batches = len(trY_sf) // batchSize +1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "loss=loss.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    #X_batch = torch.sign(torch.tanh(X_batch))\n",
    "    X_batch = torch.tanh(X_batch)\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize + 1\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    #X_batch = torch.sign(torch.tanh(X_batch))\n",
    "    X_batch = torch.tanh(X_batch)\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(hash_size) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "#confusion matrix\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels ) #labels=[B,S]\n",
    "print (cm)\n",
    "print ('Sensitivity of B: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of S: %.6f'%float(cm[1][1]/np.sum(cm[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371, 2)\n",
      "371 / 371 The length of train set is 333\n",
      "The length of test set is 38\n",
      "Completed buliding index in 1 seconds\n",
      "Accuracy: 0.868421\n",
      "[[ 7  4]\n",
      " [ 1 26]]\n",
      "Sensitivity of B: 0.636364\n",
      "Sensitivity of S: 0.962963\n"
     ]
    }
   ],
   "source": [
    "#PHA\n",
    "def avhash(im): #Perceptual hash algorithm\n",
    "    if not isinstance(im, Image.Image):\n",
    "        im = Image.open(im)\n",
    "    im = im.resize((16, 16), Image.ANTIALIAS).convert('L')\n",
    "    avg = reduce(lambda x, y: x + y, im.getdata()) / 64.\n",
    "    return reduce(lambda x, yz: x | (yz[1] << yz[0]),\n",
    "                  enumerate(map(lambda i: 0 if i < avg else 1, im.getdata())),\n",
    "                  0)\n",
    "\n",
    "root_dir = '/data/fjsdata/qtsys/img/' #the path of images\n",
    "data = pd.read_csv('/data/fjsdata/qtsys/label.csv') \n",
    "data['label'] = data['label'].fillna('H')\n",
    "data=data[data['label']!='H']\n",
    "print(data.shape)\n",
    "data = data.sample(frac=1).reset_index(drop=True) #shuffle\n",
    "#Dataset\n",
    "X, Y = [],[]\n",
    "for _, row in data.iterrows():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, row['name'])\n",
    "        img = cv2.resize(cv2.imread(image_path,cv2.IMREAD_GRAYSCALE).astype(np.float32), (128, 128))\n",
    "        X.append(img.flatten())\n",
    "        if row['label']=='B':\n",
    "            Y.append(0) #buy\n",
    "        else: #row['label']=='S':\n",
    "            Y.append(1) #sell\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(Y),data.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "#split trainset and testset \n",
    "trI, teI, trY, teY = train_test_split(X, Y, test_size=0.1, random_state=42) #list after return\n",
    "print('The length of train set is %d'%len(trI))\n",
    "print('The length of test set is %d'%len(teI))\n",
    "\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(128*128) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trI, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teI, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "#confusion matrix\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels ) \n",
    "print (cm)\n",
    "print ('Sensitivity of B: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of S: %.6f'%float(cm[1][1]/np.sum(cm[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "#send email: https://zhuanlan.zhihu.com/p/24180606\n",
    "mail_host = 'smtp.163.com'  \n",
    "mail_user = 'sharpsword'\n",
    "mail_pass = 'fjs231104' \n",
    "sender = 'sharpsword@163.com' \n",
    "receivers = ['36370408@qq.com']\n",
    "message = MIMEText('content','plain','utf-8')\n",
    "message['Subject'] = 'Test2' \n",
    "message['From'] = sender \n",
    "message['To'] = receivers[0]  \n",
    "\n",
    "try:\n",
    "    smtpObj = smtplib.SMTP_SSL(mail_host)\n",
    "    #smtpObj = smtplib.SMTP()\n",
    "    #smtpObj.connect(mail_host,25)\n",
    "    smtpObj.login(mail_user,mail_pass) \n",
    "    smtpObj.sendmail(sender,receivers,message.as_string()) \n",
    "    smtpObj.quit() \n",
    "    print('success')\n",
    "except smtplib.SMTPException as e:\n",
    "    print('error',e) #打印错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
