{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import shutil\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import baostock as bs#pip install baostock\n",
    "import mplfinance as mpf #pip install mplfinance\n",
    "from matplotlib.pylab import date2num\n",
    "import tushare as ts # pip install tushare\n",
    "tstoken='2621bdfffbde695d0d256a69a71d9344c94c1d8a58f389cd391ceeeb' #youer token\n",
    "ts.set_token(tstoken)\n",
    "pro = ts.pro_api()\n",
    "torch.cuda.set_device(2)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Training model and output database(faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(850, 3)\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/data/fjsdata/qtsys/img/' #the path of images\n",
    "data = pd.read_csv('/data/fjsdata/qtsys/label.csv') \n",
    "print(data.shape)\n",
    "name = []\n",
    "for idx, row in data.iterrows():\n",
    "    if '20200327' in row['name']:\n",
    "        if os.path.exists(root_dir+row['name']):\n",
    "            shutil.copyfile( root_dir+row['name'], '/data/tmpexec/qtsysimg0327/'+row['name'])  \n",
    "            name.append(row['name'])\n",
    "print(len(name))\n",
    "\n",
    "#data['flag'] = data['flag'].fillna('Y')\n",
    "#data_n = data[data['flag']=='N']\n",
    "#data_y = data[data['flag']=='Y']\n",
    "#print(data_n.shape)\n",
    "#print(data_y.shape)\n",
    "#for idx, row in data_n.iterrows():\n",
    "#    os.remove(root_dir+row['name']) #remove file\n",
    "#data_y.to_csv('/data/fjsdata/qtsys/label.csv',index=False)\n",
    "\n",
    "#for idx, row in data.iterrows():\n",
    "#    if '20200323' in row['name']:\n",
    "#        if os.path.exists(root_dir+row['name']):\n",
    "#            os.remove(root_dir+row['name']) #remove file\n",
    "#        data = data.drop(idx)\n",
    "#        print(row['name'])\n",
    "#print(data.shape)\n",
    "#data_y.to_csv('/data/fjsdata/qtsys/label.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806 / 806 best_loss = 0.015099\n",
      " 80 / 81 Completed buliding index in 21 seconds\n"
     ]
    }
   ],
   "source": [
    "#Generate Dataset\n",
    "root_dir = '/data/fjsdata/qtsys/img/' #the path of images\n",
    "data = pd.read_csv('/data/fjsdata/qtsys/label.csv') \n",
    "data = data.drop_duplicates()\n",
    "data = data.sample(frac=1).reset_index(drop=True) #shuffle\n",
    "#Dataset\n",
    "trN,trI, trY =[], [],[]\n",
    "for _, row in data.iterrows():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, row['name'])\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(1600,800,3)->(256,256,3)\n",
    "        trN.append(row['name'])\n",
    "        trI.append(img)\n",
    "        if row['label']=='B':\n",
    "            trY.append(0) #buy\n",
    "        else:# row['label']=='S':\n",
    "            trY.append(1) #sell\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(trY),data.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs():\n",
    "    if (len(trY) % 2) == 0: spls = len(trY)\n",
    "    else:  spls = len(trY)-1\n",
    "    idx_sf = random.sample(range(0, spls),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "\n",
    "#define model: ASH\n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ASHNet(nn.Module):\n",
    "    def __init__(self, code_size: int):\n",
    "        super().__init__()\n",
    "        #Resnet\n",
    "        self.net = nn.Sequential(\n",
    "            ResBlock(in_channels=3, out_channels=16),\n",
    "            ResBlock(in_channels=16, out_channels=16),\n",
    "            ResBlock(in_channels=16, out_channels=16, stride=2),\n",
    "        ) \n",
    "        #Attention \n",
    "        self.sa = SpatialAttention() \n",
    "        #fully connected\n",
    "        self.linear = nn.Sequential(\n",
    "            #nn.Linear(16*128*128, 4096),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.Linear(16*128*128, code_size),\n",
    "            #nn.ReLU(inplace=True) #nn.Tanh()#[-1,1]\n",
    "        )\n",
    "        \n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.sa(x)*x\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "#define loss function:pairwise loss            \n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.l1_loss = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    def forward(self,h1,h2,y):    \n",
    "        margin_val = self.margin * h1.shape[1]\n",
    "        squared_loss = torch.mean(self.mse_loss(h1, h2), dim=1)\n",
    "        # T1: 0.5 * (1 - y) * dist(x1, x2)\n",
    "        positive_pair_loss = (0.5 * (1 - y) * squared_loss)\n",
    "        mean_positive_pair_loss = torch.mean(positive_pair_loss)\n",
    "        # T2: 0.5 * y * max(margin - dist(x1, x2), 0)\n",
    "        zeros = torch.zeros_like(squared_loss)\n",
    "        marginMat = margin_val * torch.ones_like(squared_loss)\n",
    "        negative_pair_loss = 0.5 * y * torch.max(zeros, marginMat - squared_loss)\n",
    "        mean_negative_pair_loss = torch.mean(negative_pair_loss)\n",
    "\n",
    "        # T3: alpha(dst_l1(abs(x1), 1)) + dist_l1(abs(x2), 1)))\n",
    "        mean_value_regularization = self.alpha * (\n",
    "                self.l1_loss(torch.abs(h1), torch.ones_like(h1)) +\n",
    "                self.l1_loss(torch.abs(h2), torch.ones_like(h2)))\n",
    "\n",
    "        loss = mean_positive_pair_loss + mean_negative_pair_loss + mean_value_regularization\n",
    "        return loss\n",
    "\n",
    "\n",
    "#train model\n",
    "hash_size=12\n",
    "model = ASHNet(code_size=hash_size).cuda()\n",
    "criterion  = HashLossFunc(margin=0.5).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(50):#iteration\n",
    "    trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs()\n",
    "    losses = []\n",
    "    num_batches = len(trY_sf) // batchSize +1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        #sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        #sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    #print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "loss=loss.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#output the feature with best model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    X_batch = torch.tanh(X_batch) #[-1,1]\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# buliding index for retrieval\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(hash_size) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate the K line and retrieve from database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login success!\n",
      "Error in collecting data:sz.000029\n",
      "Error in collecting data:sz.000502\n",
      "Error in collecting data:sz.000505\n",
      "sz.000586-20200327.png-S<-->sz.000423-3.png\n",
      "Error in collecting data:sz.000670\n",
      "sz.000791-20200327.png-S<-->sz.000661-6.png\n",
      "sz.000876-20200327.png-S<-->sz.002714-0.png\n",
      "Error in collecting data:sz.000901\n",
      "Error in collecting data:sz.000977\n",
      "sz.002184-20200327.png-S<-->sz.002673-5.png\n",
      "sz.002213-20200327.png-S<-->sh.600188-5.png\n",
      "Error in collecting data:sz.002308\n",
      "sz.002332-20200327.png-S<-->sz.000629-5.png\n",
      "sz.002338-20200327.png-S<-->sz.002271-2.png\n",
      "sz.002339-20200327.png-S<-->sz.002945-0319.png\n",
      "sz.002365-20200327.png-S<-->sz.300260-20200326.png\n",
      "Error in collecting data:sz.002450\n",
      "sz.002471-20200327.png-S<-->sh.600027-1.png\n",
      "sz.002539-20200327.png-S<-->sh.600340-5.png\n",
      "sz.002629-20200327.png-S<-->sh.600399-20200324.png\n",
      "sz.002702-20200327.png-S<-->sz.002938-6.png\n",
      "Error in collecting data:sz.002724\n",
      "sz.002816-20200327.png-S<-->sz.300003-5.png\n",
      "Error in collecting data:sz.002884\n",
      "sz.002899-20200327.png-S<-->sz.002415-2.png\n",
      "sz.002955-20200327.png-S<-->sz.002475-6.png\n",
      "Error in collecting data:sz.002976\n",
      "Error in collecting data:sz.002977\n",
      "sz.300023-20200327.png-S<-->sh.600100-2.png\n",
      "Error in collecting data:sz.300070\n",
      "sz.300215-20200327.png-S<-->sh.601808-7.png\n",
      "Error in collecting data:sz.300256\n",
      "Error in collecting data:sz.300295\n",
      "sz.300312-20200327.png-S<-->sh.600027-1.png\n",
      "sz.300440-20200327.png-S<-->sh.601186-5.png\n",
      "sz.300485-20200327.png-S<-->sh.600872-0319.png\n",
      "sz.300489-20200327.png-S<-->sh.600383-7.png\n",
      "sz.300561-20200327.png-S<-->sz.002714-7.png\n",
      "Error in collecting data:sz.300592\n",
      "sz.300634-20200327.png-S<-->sh.603833-5.png\n",
      "sz.300636-20200327.png-S<-->sz.000656-7.png\n",
      "sz.300702-20200327.png-S<-->sz.002714-7.png\n",
      "sz.300713-20200327.png-S<-->sh.601288-3.png\n",
      "sz.300721-20200327.png-S<-->sz.002938-6.png\n",
      "sz.300737-20200327.png-S<-->sz.300408-5.png\n",
      "sz.300815-20200327.png-S<-->sz.002075-0319.png\n",
      "Error in collecting data:sz.300819\n",
      "Error in collecting data:sz.300821\n",
      "Error in collecting data:sz.300822\n",
      "Error in collecting data:sz.300823\n",
      "Error in collecting data:sz.300825\n",
      "sh.600051-20200327.png-S<-->sz.000703-3.png\n",
      "Error in collecting data:sh.600079\n",
      "Error in collecting data:sh.600145\n",
      "Error in collecting data:sh.600158\n",
      "Error in collecting data:sh.600167\n",
      "Error in collecting data:sh.600228\n",
      "Error in collecting data:sh.600278\n",
      "Error in collecting data:sh.600310\n",
      "Error in collecting data:sh.600354\n",
      "sh.600475-20200327.png-S<-->sz.002050-3.png\n",
      "sh.600522-20200327.png-S<-->sz.002945-2.png\n",
      "Error in collecting data:sh.600732\n",
      "Error in collecting data:sh.600745\n",
      "Error in collecting data:sh.600766\n",
      "Error in collecting data:sh.600816\n",
      "sh.600836-20200327.png-S<-->sh.600515-0318.png\n",
      "Error in collecting data:sh.600890\n",
      "Error in collecting data:sh.600891\n",
      "Error in collecting data:sh.601127\n",
      "Error in collecting data:sh.601162\n",
      "Error in collecting data:sh.601231\n",
      "sh.601366-20200327.png-S<-->sh.600383-7.png\n",
      "Error in collecting data:sh.601555\n",
      "Error in collecting data:sh.603002\n",
      "sh.603050-20200327.png-S<-->sz.300498-7.png\n",
      "Error in collecting data:sh.603066\n",
      "sh.603088-20200327.png-S<-->sz.000629-5.png\n",
      "Error in collecting data:sh.603221\n",
      "sh.603380-20200327.png-S<-->sh.600383-7.png\n",
      "Error in collecting data:sh.603399\n",
      "sh.603600-20200327.png-S<-->sh.603259-7.png\n",
      "sh.603668-20200327.png-S<-->sz.002001-1.png\n",
      "sh.603696-20200327.png-S<-->sz.002466-4.png\n",
      "sh.603709-20200327.png-S<-->sh.601878-5.png\n",
      "sh.603823-20200327.png-S<-->sz.002925-7.png\n",
      "sh.603829-20200327.png-S<-->sz.300070-2.png\n",
      "Error in collecting data:sh.603948\n",
      "Error in collecting data:sh.603949\n",
      "Error in collecting data:sh.603960\n",
      "Error in collecting data:sh.688051\n",
      "Error in collecting data:sh.688189\n",
      "Error in collecting data:sh.688228\n",
      "logout success!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<baostock.data.resultset.ResultData at 0x7f95140af590>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate MACD\n",
    "def cal_macd_system(data,short_,long_,m):\n",
    "    '''\n",
    "    data=['Open','High','Low','Close','Volume']\n",
    "    parameter: short_,long_,m\n",
    "    return:data=['Open','High','Low','Close','Volume','diff','dea','macd']\n",
    "    '''\n",
    "    data['diff']=data['Close'].ewm(adjust=False,alpha=2/(short_+1),ignore_na=True).mean()-\\\n",
    "                data['Close'].ewm(adjust=False,alpha=2/(long_+1),ignore_na=True).mean()\n",
    "    data['dea']=data['diff'].ewm(adjust=False,alpha=2/(m+1),ignore_na=True).mean()\n",
    "    data['macd']=2*(data['diff']-data['dea'])\n",
    "    return data\n",
    "def macd_zero(macd):\n",
    "    pos_signal, neg_signal = [],[]\n",
    "    for idx,value in macd.iteritems():\n",
    "        if value > 0:\n",
    "            pos_signal.append(value)\n",
    "            neg_signal.append(np.nan)\n",
    "        else:\n",
    "            neg_signal.append(value)\n",
    "            pos_signal.append(np.nan)\n",
    "    return pos_signal,neg_signal\n",
    "\n",
    "#http://baostock.com/baostock/index.php/Python_API\n",
    "#generate market chart\n",
    "lg = bs.login() #login\n",
    "#read stocks information\n",
    "df_stocks = pro.stock_basic(exchange='', list_status='L', fields='symbol,name')\n",
    "#read k data\n",
    "fields= \"Date,Code,Open,High,Low,Close,Volume\"\n",
    "today  = (datetime.datetime.now()+datetime.timedelta(hours=8)).strftime('%Y%m%d')#UTC->CTS +8hours\n",
    "#today =datetime.datetime.now().strftime('%Y%m%d') \n",
    "df_cal = pro.trade_cal(exchange='', start_date='20200101', end_date=today)\n",
    "df_cal =df_cal[df_cal['is_open']==1].reset_index(drop=True)\n",
    "edate = df_cal[-21:][-1:]['cal_date'].tolist()[0] #last-1\n",
    "sdate = df_cal[-21:].head(1)['cal_date'].tolist()[0] #first\n",
    "edate = datetime.datetime.strptime(edate, '%Y%m%d').strftime('%Y-%m-%d') #turn to datetime\n",
    "sdate = datetime.datetime.strptime(sdate, '%Y%m%d').strftime('%Y-%m-%d')  #turn to datetime\n",
    "for code in df_stocks['symbol'].tolist():\n",
    "    if code[0]=='6': code = 'sh.'+ code\n",
    "    elif code[0]=='0' or code[0]=='3': code = 'sz.'+ code\n",
    "    else: continue\n",
    "    #read transaction data\n",
    "    rs = bs.query_history_k_data(code=code, fields=fields, \\\n",
    "                                 start_date=sdate, end_date=edate, \\\n",
    "                                 frequency=\"30\",adjustflag=\"3\") #40days，one k line per 60 minutes\n",
    "    data_list = []\n",
    "    while (rs.error_code == '0') & rs.next():\n",
    "        data_list.append(rs.get_row_data())\n",
    "    result = pd.DataFrame(data_list, columns=rs.fields)\n",
    "    result=result.apply(pd.to_numeric, errors='ignore')\n",
    "    if result.shape[0] ==160:\n",
    "        #plot K line \n",
    "        result = result[['Open','High','Low','Close','Volume']]\n",
    "        result.index=pd.to_datetime(result.index)#turn index to datatime\n",
    "        result = cal_macd_system(result,12,26,9)\n",
    "        pos_macd, neg_macd  = macd_zero(result['macd']) \n",
    "        apds = [ mpf.make_addplot(result['diff'],panel='lower',color='b'),\n",
    "                     mpf.make_addplot(result['dea'],panel='lower',color='y'),\n",
    "                     mpf.make_addplot(pos_macd,panel='lower',color='r',scatter=True),\n",
    "                     mpf.make_addplot(neg_macd,panel='lower',color='g',scatter=True)\n",
    "                   ]\n",
    "        kwargs = dict(type='candle',figratio =(16,8),volume=False,figscale=1)#line，mav=(5,10)\n",
    "        Kline_path ='/data/fjsdata/qtsys/img/'+code+'-'+today+'.png'\n",
    "        save = dict(fname=Kline_path,dpi=100, pad_inches=0.2)\n",
    "        mpf.plot(result,**kwargs,addplot=apds,style='sas',savefig=save)#charles\n",
    "        plt.close()\n",
    "        Kline_img = cv2.resize(cv2.imread(Kline_path).astype(np.float32), (256, 256)) #read image \n",
    "        teI = []\n",
    "        teI.append(Kline_img)\n",
    "        #output feature with model\n",
    "        teI = torch.from_numpy(np.array(teI)).type(torch.FloatTensor).cuda()\n",
    "        teI = best_net(teI.permute(0, 3, 1, 2))#forword\n",
    "        teI = torch.tanh(teI) #[-1,1]\n",
    "        teI = teI.cpu().data.numpy().tolist()\n",
    "        #retrieve from DB\n",
    "        #np.linalg.norm(vec1 - vec2) #consine l2-norm \n",
    "        scores, neighbors = gpu_index.search(np.ascontiguousarray(teI, dtype=np.float32), k=1) #return top1\n",
    "        if scores.flatten()[0]< 0.001: #similarity for sell\n",
    "            label = trY[neighbors.flatten()[0]] \n",
    "            name = trN[neighbors.flatten()[0]]\n",
    "            with open('/data/fjsdata/qtsys/label.csv','a+') as f:\n",
    "                csv_write = csv.writer(f)\n",
    "                if label == 1:\n",
    "                    print('%s-S<-->%s'%(code+'-'+today+'.png',name))\n",
    "                    csv_write.writerow([code+'-'+today+'.png','S'])\n",
    "                elif (label == 0 and scores.flatten()[0]< 0.0005): #similarity for buy\n",
    "                    print('%s-B<-->%s'%(code+'-'+today+'.png',name))\n",
    "                    csv_write.writerow([code+'-'+today+'.png','B'])          \n",
    "                else:os.remove(Kline_path) #remove the image file if no handle\n",
    "        else: os.remove(Kline_path) #remove the image file if no handle \n",
    "    else: print('Error in collecting data:%s' % (code))\n",
    "bs.logout()#logout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26\n",
      "2020-02-27\n",
      "20200326\n"
     ]
    }
   ],
   "source": [
    "print(edate)\n",
    "print(sdate)\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
