{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import itertools  \n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "from scipy.spatial.distance import pdist\n",
    "import cv2\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc,silhouette_score \n",
    "from sklearn.cluster import KMeans,DBSCAN\n",
    "from scipy.spatial import distance\n",
    "from functools import reduce\n",
    "import wfdb#https://github.com/MIT-LCP/wfdb-python\n",
    "from wfdb import processing\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.set_device(2)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of MIT-BIH for PVC trainset is: (6902,361)\n",
      "The shape of MIT-BIH for PVC testset is: (1996,362)\n"
     ]
    }
   ],
   "source": [
    "rootdir = '/data/fjsdata/physionet/MIT-BIH/mitdb/'\n",
    "rootdir = '/data/fjsdata/physionet/MIT-BIH/mitdb/'\n",
    "right_len = 180 #right sample length around of peak value of QRS\n",
    "left_len = 180 #left sample length around of peak value of QRS\n",
    "#train data\n",
    "trPVC = [] #[Subject,sig_name,QRS]\n",
    "for bt in [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230,\\\n",
    "            100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]:\n",
    "    #44 records for train\n",
    "    file = os.path.join(rootdir,str(bt))\n",
    "    try:\n",
    "        annotation = wfdb.rdann(file, 'atr') \n",
    "        qrs_spl = annotation.sample #numpy.ndarray\n",
    "        qrs_sym = annotation.symbol #list\n",
    "        record = wfdb.rdrecord(file)\n",
    "        signal = record.p_signal #numpy.ndarray\n",
    "        max_len = record.sig_len #length of samples\n",
    "        lead_name =  record.sig_name #names of lead channels,list\n",
    "        for i in range(annotation.ann_len):\n",
    "            if qrs_sym[i] in ['V']:#PVC samples\n",
    "                pos = qrs_spl[i] #corresponding position of peak value of QRS\n",
    "                if pos+right_len<=max_len and pos-left_len>=0:\n",
    "                    max_idx = pos+right_len#np.min([max_len, pos+trunc_len])\n",
    "                    min_idx = pos-left_len#np.max([0, pos-trunc_len])\n",
    "                    for j, val in enumerate(lead_name):\n",
    "                        if val == 'MLII':\n",
    "                            QRS_value = signal[:,j][min_idx:max_idx]\n",
    "                            trPVC.append([bt,QRS_value])#[Subject,sig_name,QRS]\n",
    "    except: pass\n",
    "trPVC = pd.DataFrame(np.array(trPVC),columns=['sub','qrs'])\n",
    "trPVC_QRS = pd.DataFrame(trPVC['qrs'].values.tolist()) #QRS extrend\n",
    "trPVC = trPVC.drop(['qrs'],axis=1) #drop column 2\n",
    "trPVC = pd.concat([trPVC, trPVC_QRS], axis=1)\n",
    "print('The shape of MIT-BIH for PVC trainset is: (%d,%d)'%(trPVC.shape[0],trPVC.shape[1]))\n",
    "\n",
    "#test data\n",
    "txt_dir = '/data/fjsdata/ECG/PVC/MIT-BIH-label/' #the path of images\n",
    "sgl_dir = '/data/fjsdata/physionet/MIT-BIH/mitdb/'\n",
    "tePVC = []#[Subject,QRS,PVC label]\n",
    "for iname in os.listdir(txt_dir):\n",
    "    #read label \n",
    "    txt_path = os.path.join(txt_dir, iname) \n",
    "    label_list = []\n",
    "    with open(txt_path,'r') as f:\n",
    "        for line in f:\n",
    "            label_list.append(line.strip('\\n'))\n",
    "    bt = os.path.splitext(iname)[0]\n",
    "    sgl_path = os.path.join(sgl_dir,str(bt))\n",
    "    #read positon of PVC\n",
    "    annotation = wfdb.rdann(sgl_path, 'atr') \n",
    "    qrs_spl = annotation.sample #numpy.ndarray\n",
    "    qrs_sym = annotation.symbol #list\n",
    "    pos_list = []\n",
    "    for i in range(annotation.ann_len):\n",
    "        if qrs_sym[i] in ['V']:#PVC samples\n",
    "            pos = qrs_spl[i] #corresponding position of peak value of QRS\n",
    "            pos_list.append(qrs_spl[i])\n",
    "    #read signal        \n",
    "    record = wfdb.rdrecord(sgl_path)\n",
    "    signal = record.p_signal #numpy.ndarray\n",
    "    max_len = record.sig_len #length of samples\n",
    "    lead_name =  record.sig_name #names of lead channels,list\n",
    "    for i in range(len(label_list)):\n",
    "        pos = pos_list[i]\n",
    "        if pos+right_len<=max_len and pos-left_len>=0:\n",
    "            max_idx = pos+right_len#np.min([max_len, pos+trunc_len])\n",
    "            min_idx = pos-left_len#np.max([0, pos-trunc_len])\n",
    "            for j, val in enumerate(lead_name):\n",
    "                if val == 'MLII':\n",
    "                    QRS_value = signal[:,j][min_idx:max_idx]\n",
    "                    tePVC.append([bt,label_list[i],QRS_value])\n",
    "tePVC = pd.DataFrame(np.array(tePVC),columns=['sub','label','qrs'])\n",
    "tePVC_QRS = pd.DataFrame(tePVC['qrs'].values.tolist()) #QRS extrend\n",
    "tePVC = tePVC.drop(['qrs'],axis=1) #drop column 2\n",
    "tePVC = pd.concat([tePVC, tePVC_QRS], axis=1)\n",
    "print('The shape of MIT-BIH for PVC testset is: (%d,%d)'%(tePVC.shape[0],tePVC.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.857143\n",
      "[[ 5  2]\n",
      " [ 2 19]]\n",
      "Sensitivity of unifocal pvc: 0.714286\n",
      "Sensitivity of multifocal pvc: 0.904762\n"
     ]
    }
   ],
   "source": [
    "#Hanmming Distance\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "#Testing hamming distance with true label\n",
    "sub_list = list(set(np.array(tePVC['sub']).tolist()))\n",
    "y_pred, y_true = [], []\n",
    "for bt in sub_list:\n",
    "    sub_df =  tePVC[tePVC['sub']==bt]\n",
    "    if (sub_df.shape[0]>1): \n",
    "        y = list(set(np.array(sub_df['label']).tolist()))\n",
    "        if len(y)==1: y_true.append(0) #unifocal\n",
    "        else:  y_true.append(1) #multifocal\n",
    "        #calculate hanmming distance\n",
    "        X = sub_df.drop(['sub','label'],axis=1).reset_index(drop=True) \n",
    "        X = np.sign(tanh(np.array(X))) \n",
    "        k = 1\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(i+1,X.shape[0]):\n",
    "                dist = pdist(np.vstack([X[i],X[j]]),'hamming')\n",
    "                if dist>0.3: k = k+1\n",
    "        if k==1: y_pred.append(0) #unifocal\n",
    "        else:  y_pred.append(1) #multifocal\n",
    "            \n",
    "print ( 'Accuracy: %.6f'%accuracy_score(y_true, y_pred))\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1] ) \n",
    "print (cm) \n",
    "print ('Sensitivity of unifocal pvc: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of multifocal pvc: %.6f'%float(cm[1][1]/np.sum(cm[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_loss = 0.109405\n",
      "Accuracy: 0.892857\n",
      "[[ 5  2]\n",
      " [ 1 20]]\n",
      "Sensitivity of unifocal pvc: 0.714286\n",
      "Sensitivity of multifocal pvc: 0.952381\n"
     ]
    }
   ],
   "source": [
    "#Unsupervised Hashing Methods: autoencoder\n",
    "#https://github.com/L1aoXingyu/pytorch-beginner/blob/master/08-AutoEncoder/simple_autoencoder.py\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(360, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True), nn.Linear(64, 12), nn.ReLU(True), nn.Linear(12, 3))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), nn.Linear(128, 360), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 100\n",
    "num_epochs = 200\n",
    "X = tePVC.drop(['sub','label'],axis=1).reset_index(drop=True) \n",
    "X = np.array(X)\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    num_batches = X.shape[0] // batchSize + 1\n",
    "    for i in range(num_batches):\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([X.shape[0], (i+1)*batchSize])\n",
    "        inputs = torch.from_numpy(X[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        # ===================forward=====================\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "        losses.append(loss.item())\n",
    "    #print('epoch [{}/{}], loss:{:.6f}'.format(epoch + 1, num_epochs, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))    \n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "torch.cuda.empty_cache()    \n",
    "\n",
    "#Testing hamming distance with true label\n",
    "sub_list = list(set(np.array(tePVC['sub']).tolist()))\n",
    "y_pred, y_true = [], []\n",
    "#pdist = nn.PairwiseDistance(2)\n",
    "for bt in sub_list:\n",
    "    sub_df =  tePVC[tePVC['sub']==bt]\n",
    "    if (sub_df.shape[0]>1): \n",
    "        y = list(set(np.array(sub_df['label']).tolist()))\n",
    "        if len(y)==1: y_true.append(0) #unifocal\n",
    "        else:  y_true.append(1) #multifocal\n",
    "        #predict\n",
    "        X = sub_df.drop(['sub','label'],axis=1).reset_index(drop=True) \n",
    "        X = np.array(X)\n",
    "        k = 1\n",
    "        for i in range(X.shape[0]-1):\n",
    "            Q_batch = torch.from_numpy(X[i:i+1]).type(torch.FloatTensor).cuda()\n",
    "            N_batch = torch.from_numpy(X[i+1:i+2]).type(torch.FloatTensor).cuda()\n",
    "            Q_hash = torch.sign(best_net(Q_batch)).cpu().detach().numpy()\n",
    "            N_hash = torch.sign(best_net(N_batch)).cpu().detach().numpy()\n",
    "            #loss_neg = torch.mean(pdist(Q_hash, N_hash)).item()\n",
    "            dist = pdist(np.vstack([Q_hash,N_hash]),'hamming')\n",
    "            if dist>0.2: \n",
    "                k = k+1\n",
    "                break\n",
    "        if k==1: y_pred.append(0) #unifocal\n",
    "        else:  y_pred.append(1) #multifocal\n",
    "            \n",
    "print ( 'Accuracy: %.6f'%accuracy_score(y_true, y_pred))\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1] ) \n",
    "print (cm) \n",
    "print ('Sensitivity of unifocal pvc: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of multifocal pvc: %.6f'%float(cm[1][1]/np.sum(cm[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16 / 16 : loss = 17.079418Eopch:     1 mean_loss = 17.925476\n",
      " 16 / 16 : loss = 17.553492Eopch:     2 mean_loss = 17.909584\n",
      " 16 / 16 : loss = 17.915691Eopch:     3 mean_loss = 17.906256\n",
      " 16 / 16 : loss = 17.647793Eopch:     4 mean_loss = 17.887211\n",
      " 16 / 16 : loss = 17.057114Eopch:     5 mean_loss = 17.857051\n",
      " 16 / 16 : loss = 17.597784Eopch:     6 mean_loss = 17.856803\n",
      " 16 / 16 : loss = 18.378519Eopch:     7 mean_loss = 17.865483\n",
      " 16 / 16 : loss = 17.742271Eopch:     8 mean_loss = 17.835155\n",
      " 16 / 16 : loss = 17.884775Eopch:     9 mean_loss = 17.829639\n",
      " 16 / 16 : loss = 17.814701Eopch:    10 mean_loss = 17.814885\n",
      "best_loss = 17.814885\n"
     ]
    }
   ],
   "source": [
    "#Supervised Hashing Methods: Hashnet is pretrained with ground-truth samples.\n",
    "#https://github.com/luyajie/triplet-deep-hash-pytorch/blob/master/hashNet.py\n",
    "class HashTripletNet(nn.Module):\n",
    "    def __init__(self, features=360, hashLength=36):\n",
    "        super(HashTripletNet, self).__init__()\n",
    "        self.fc = nn.Linear(features, hashLength)\n",
    "        self.sm = nn.Sigmoid() ##nn.ReLU(inplace=True)\n",
    "        self.initLinear()\n",
    "\n",
    "    def forward(self, q, p, n):\n",
    "        q = self.sm(self.fc(q))\n",
    "        p = self.sm(self.fc(p))\n",
    "        n = self.sm(self.fc(n))\n",
    "        return q, p, n\n",
    "\n",
    "    def initLinear(self):\n",
    "        self.fc.weight.data.normal_(1.0, 0.3)\n",
    "        self.fc.bias.data.fill_(0.1)\n",
    "        \n",
    "#generate triplet label\n",
    "sub_list = list(set(np.array(tePVC['sub']).tolist()))\n",
    "trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "for bt in sub_list:\n",
    "    sub_df =  tePVC[tePVC['sub']==bt]\n",
    "    y = np.array(sub_df['label']).tolist()\n",
    "    if (sub_df.shape[0]>3 and len(list(set(y)))>1): #query:positive:negative\n",
    "        X = sub_df.drop(['sub','label'],axis=1).reset_index(drop=True)\n",
    "        X = np.array(X)\n",
    "        for i in range(X.shape[0]):#query\n",
    "            for j in range(X.shape[0]):#positive\n",
    "                if (i!=j) and (y[i]==y[j]):\n",
    "                    for k in range(X.shape[0]):#negative\n",
    "                        if(y[i]!=y[k]):\n",
    "                            trQ_sf.append(X[i])\n",
    "                            trP_sf.append(X[j])\n",
    "                            trN_sf.append(X[k])\n",
    "                            break\n",
    "                    break\n",
    "assert (len(trQ_sf)==len(trP_sf))\n",
    "assert (len(trQ_sf)==len(trN_sf))\n",
    "trQ_sf = np.array(trQ_sf)\n",
    "trP_sf = np.array(trP_sf)\n",
    "trN_sf = np.array(trN_sf)\n",
    "#training triplet model\n",
    "#define model\n",
    "model = HashTripletNet().cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "pdist = nn.PairwiseDistance(2)\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 100\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize + 1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash,P_hash,N_hash = model(Q_batch,P_batch,N_batch)#permute the dims of matrix\n",
    "        # loss\n",
    "        loss_pos = pdist(Q_hash, P_hash)\n",
    "        loss_neg = pdist(Q_hash, N_hash)\n",
    "        l = 18 - loss_neg + loss_pos\n",
    "        loss = torch.mean(F.relu(l))#\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.642857\n",
      "[[ 4  3]\n",
      " [ 7 14]]\n",
      "Sensitivity of unifocal pvc: 0.571429\n",
      "Sensitivity of multifocal pvc: 0.666667\n"
     ]
    }
   ],
   "source": [
    "#Testing hamming distance with true label\n",
    "sub_list = list(set(np.array(tePVC['sub']).tolist()))\n",
    "y_pred, y_true = [], []\n",
    "pdist = nn.PairwiseDistance(2)\n",
    "for bt in sub_list:\n",
    "    sub_df =  tePVC[tePVC['sub']==bt]\n",
    "    if (sub_df.shape[0]>1): \n",
    "        y = list(set(np.array(sub_df['label']).tolist()))\n",
    "        if len(y)==1: y_true.append(0) #unifocal\n",
    "        else:  y_true.append(1) #multifocal\n",
    "        #predict\n",
    "        X = sub_df.drop(['sub','label'],axis=1).reset_index(drop=True) \n",
    "        X = np.array(X)\n",
    "        k = 1\n",
    "        for i in range(X.shape[0]-1):\n",
    "            Q_batch = torch.from_numpy(X[i:i+1]).type(torch.FloatTensor).cuda()\n",
    "            N_batch = torch.from_numpy(X[i+1:i+2]).type(torch.FloatTensor).cuda()\n",
    "            Q_hash,_,N_hash = best_net(Q_batch,Q_batch,N_batch)\n",
    "            #loss_neg = torch.mean(F.relu(pdist(Q_hash, N_hash))).item()\n",
    "            loss_neg = torch.mean(pdist(Q_hash, N_hash)).item()\n",
    "            if loss_neg>0.2: \n",
    "                k = k+1\n",
    "                break\n",
    "        if k==1: y_pred.append(0) #unifocal\n",
    "        else:  y_pred.append(1) #multifocal\n",
    "            \n",
    "print ( 'Accuracy: %.6f'%accuracy_score(y_true, y_pred))\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1] ) \n",
    "print (cm) \n",
    "print ('Sensitivity of unifocal pvc: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of multifocal pvc: %.6f'%float(cm[1][1]/np.sum(cm[1])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.750000\n",
      "[[ 0  7]\n",
      " [ 0 21]]\n",
      "Sensitivity of unifocal pvc: 0.000000\n",
      "Sensitivity of multifocal pvc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "#Testing Inverted index with true label\n",
    "def genInvertedIndex(X, bin_len=0.01):\n",
    "    # parameter: X ,numpy array (n*m)\n",
    "    # bin_len, float, discretize the continuous value with bins\n",
    "    # output: X_i, numpy array (w*n), w is the length of bins\n",
    "    con_values = sorted(list(set(X.flatten().tolist())))\n",
    "    bins = np.arange(con_values[0],con_values[-1],bin_len)\n",
    "    X_i = np.zeros((len(bins)+1,X.shape[0]+1))\n",
    "    for i in range(X.shape[0]):\n",
    "        for val in X[i].tolist():\n",
    "            X_i[np.digitize(val,bins),i]=1\n",
    "    return X_i\n",
    "    \n",
    "sub_list = list(set(np.array(tePVC['sub']).tolist()))\n",
    "y_pred, y_true = [], []\n",
    "for bt in sub_list:\n",
    "    sub_df =  tePVC[tePVC['sub']==bt]\n",
    "    if (sub_df.shape[0]>1): \n",
    "        y = list(set(np.array(sub_df['label']).tolist()))\n",
    "        if len(y)==1: y_true.append(0) #unifocal\n",
    "        else:  y_true.append(1) #multifocal\n",
    "        #calculate hanmming distance\n",
    "        X = sub_df.drop(['sub','label'],axis=1).reset_index(drop=True) \n",
    "        X = np.array(X)\n",
    "        X_i = genInvertedIndex(X)\n",
    "        k = 1\n",
    "        for i in range(X_i.shape[1]):\n",
    "            for j in range(i+1,X_i.shape[1]):\n",
    "                dist = pdist(np.vstack([X_i[:,i],X_i[:,j]]),'hamming')\n",
    "                if dist>0.3: k = k+1\n",
    "        if k==1: y_pred.append(0) #unifocal\n",
    "        else:  y_pred.append(1) #multifocal\n",
    "            \n",
    "print ( 'Accuracy: %.6f'%accuracy_score(y_true, y_pred))\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1] ) \n",
    "print (cm) \n",
    "print ('Sensitivity of unifocal pvc: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of multifocal pvc: %.6f'%float(cm[1][1]/np.sum(cm[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
