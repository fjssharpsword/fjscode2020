{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import itertools  \n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import distance\n",
    "from functools import reduce\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.set_device(2)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hanmming Distance\n",
    "#Input: X, numpy array, m*n, m is samples of PVC (m subjects), n is the dimensions of PVC\n",
    "#       diff, float, [0,1], the difference between PVCs\n",
    "#Output: y, 0 is unifocal, 1 is multifocal\n",
    "\n",
    "def Func_HammingDist(X, diff=0.3):\n",
    "   \n",
    "    def tanh(x): #tangent\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "    \n",
    "    if X.shape[0]>1: \n",
    "        X = np.sign(tanh(X)) \n",
    "        k = 1\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(i+1,X.shape[0]):\n",
    "                dist = pdist(np.vstack([X[i],X[j]]),'hamming')\n",
    "                if dist>0.3: \n",
    "                    k = k+1\n",
    "                    break\n",
    "        if k==1: return 0 #unifocal\n",
    "        else:  return 1 #multifocal\n",
    "    else: return 0 #unifocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoder + Hanmming Distance\n",
    "#Input: X, numpy array, m*n, m is samples of PVC (m subjects), n is the dimensions of PVC\n",
    "#       diff, float, [0,1], the difference between PVCs\n",
    "#Output: y, 0 is unifocal, 1 is multifocal\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self, X_n):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(X_n, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True), nn.Linear(64, 12), nn.ReLU(True), nn.Linear(12, 3))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), nn.Linear(128, X_n), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "def Func_AutoencoderHammingDist(X, diff=0.2):\n",
    "    if X.shape[0]<2: return 0 #unifocal\n",
    "    #model training \n",
    "    model = autoencoder(X_n=X.shape[1]).cuda()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    best_net, best_loss = None, float('inf')\n",
    "    batchSize = 100\n",
    "    num_epochs = 200\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        num_batches = X.shape[0] // batchSize + 1\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * batchSize\n",
    "            max_idx = np.min([X.shape[0], (i+1)*batchSize])\n",
    "            inputs = torch.from_numpy(X[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "            # ===================forward=====================\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            # ===================backward====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "           # ===================log========================\n",
    "            losses.append(loss.item())\n",
    "        #print('epoch [{}/{}], loss:{:.6f}'.format(epoch + 1, num_epochs, np.mean(losses)))\n",
    "        if np.mean(losses) < best_loss:\n",
    "            best_loss = np.mean(losses)\n",
    "            best_net = copy.deepcopy(model)\n",
    "    print(\"best_loss = %.6f\" % (best_loss))    \n",
    "    #release gpu memory\n",
    "    model = model.cpu()\n",
    "    torch.cuda.empty_cache()    \n",
    "    #predict PVC \n",
    "    k = 1\n",
    "    for i in range(X.shape[0]-1):\n",
    "        Q_batch = torch.from_numpy(X[i:i+1]).type(torch.FloatTensor).cuda()\n",
    "        N_batch = torch.from_numpy(X[i+1:i+2]).type(torch.FloatTensor).cuda()\n",
    "        Q_hash = torch.sign(best_net(Q_batch)).cpu().detach().numpy()\n",
    "        N_hash = torch.sign(best_net(N_batch)).cpu().detach().numpy()\n",
    "        #loss_neg = torch.mean(pdist(Q_hash, N_hash)).item()\n",
    "        dist = pdist(np.vstack([Q_hash,N_hash]),'hamming')\n",
    "        if dist>0.2: \n",
    "            k = k+1\n",
    "            break\n",
    "    if k==1: return 0 #unifocal\n",
    "    else:  return 1 #multifocal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
