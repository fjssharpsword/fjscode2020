{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "from scipy.spatial.distance import pdist\n",
    "import cv2\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc \n",
    "from functools import reduce\n",
    "import wfdb#https://github.com/MIT-LCP/wfdb-python\n",
    "from wfdb import processing\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.set_device(0)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of trainset is: (50995,360)\n",
      "0    45841\n",
      "2     3788\n",
      "1      944\n",
      "3      414\n",
      "4        8\n",
      "Name: 1, dtype: int64\n",
      "The shape of testset is: (49687,360)\n",
      "0    44235\n",
      "2     3220\n",
      "1     1837\n",
      "3      388\n",
      "4        7\n",
      "Name: 1, dtype: int64\n",
      "Completed buliding index in 1 seconds\n",
      "Accuracy: 0.885423\n",
      "[[41528  1207   666   809    25]\n",
      " [ 1728    88    11     6     4]\n",
      " [  649    44  2373   154     0]\n",
      " [  347     3    33     5     0]\n",
      " [    5     0     2     0     0]]\n",
      "Specificity: 0.938804\n",
      "Sensitivity of S: 0.047904\n",
      "Sensitivity of V: 0.736957\n",
      "Sensitivity of F: 0.012887\n",
      "Sensitivity of Q: 0.000000\n"
     ]
    }
   ],
   "source": [
    "#Beats generation,\n",
    "#we defined a single ECG beat image by centering the Q-wave peak signal while\n",
    "#excluding the first and the last 20 ECG signals from the previous and afterward Q-wave peak signal\n",
    "#https://github.com/MIT-LCP/wfdb-python/blob/master/demo.ipynb\n",
    "#https://archive.physionet.org/physiobank/database/html/mitdbdir/mitdbdir.htm\n",
    "#http://www.tara.tcd.ie/bitstream/handle/2262/17623/automatic.pdf?sequence=1\n",
    "def labeltotext(val):\n",
    "    if val in ['N','L','R','e','j'] :\n",
    "        return 0 #N\n",
    "    elif val in ['A','a','J','S']:\n",
    "        return 1 #S\n",
    "    elif val in ['V','E']:\n",
    "        return 2 #V\n",
    "    elif val == 'F':\n",
    "        return 3 #F\n",
    "    elif val in ['/','f','Q']:\n",
    "        return 4 #Q\n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "rootdir = '/data/fjsdata/physionet/MIT-BIH/mitdb/'\n",
    "right_len = 180 #right sample length around of peak value of QRS\n",
    "left_len = 180 #left sample length around of peak value of QRS\n",
    "#get trainset\n",
    "trData = [] #[QRS value, label]\n",
    "for bt in [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]:#22 records for train\n",
    "    file = os.path.join(rootdir,str(bt))\n",
    "    try:\n",
    "        annotation = wfdb.rdann(file, 'atr') \n",
    "        qrs_spl = annotation.sample #numpy.ndarray\n",
    "        qrs_sym = annotation.symbol #list\n",
    "        record = wfdb.rdrecord(file)\n",
    "        signal = record.p_signal #numpy.ndarray\n",
    "        max_len = record.sig_len #length of samples\n",
    "        lead_name =  record.sig_name #names of lead channels,list\n",
    "        for i in range(annotation.ann_len):\n",
    "            if qrs_sym[i] in ['N','L','R','e','j','A','a','J','S','V','E','F','/','f','Q']:#seven diseases samples\n",
    "                pos = qrs_spl[i] #corresponding position of peak value of QRS\n",
    "                if pos+right_len<=max_len and pos-left_len>=0:\n",
    "                    max_idx = pos+right_len#np.min([max_len, pos+trunc_len])\n",
    "                    min_idx = pos-left_len#np.max([0, pos-trunc_len])\n",
    "                    #for j, val in enumerate(lead_name):\n",
    "                        #QRS_value = signal[:,j][min_idx:max_idx]\n",
    "                        #data.append([QRS_value,labeltotext(qrs_sym[i]),val])#[QRS value, label, lead name]\n",
    "                    QRS_value = signal[:,0][min_idx:max_idx] #only one lead\n",
    "                    trData.append([QRS_value,labeltotext(qrs_sym[i])])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "trData = pd.DataFrame(np.array(trData))\n",
    "X_train = pd.DataFrame(trData[0].values.tolist())\n",
    "y_train = trData[1]\n",
    "print('The shape of trainset is: (%d,%d)'%(X_train.shape[0],X_train.shape[1]))\n",
    "print(trData[1].value_counts())\n",
    "#get testset\n",
    "teData = [] #[QRS value, label]\n",
    "for bt in [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]:#22 records for test\n",
    "    file = os.path.join(rootdir,str(bt))\n",
    "    try:\n",
    "        annotation = wfdb.rdann(file, 'atr') \n",
    "        qrs_spl = annotation.sample #numpy.ndarray\n",
    "        qrs_sym = annotation.symbol #list\n",
    "        record = wfdb.rdrecord(file)\n",
    "        signal = record.p_signal #numpy.ndarray\n",
    "        max_len = record.sig_len #length of samples\n",
    "        lead_name =  record.sig_name #names of lead channels,list\n",
    "        for i in range(annotation.ann_len):\n",
    "            if qrs_sym[i] in ['N','L','R','e','j','A','a','J','S','V','E','F','/','f','Q']:#seven diseases samples\n",
    "                pos = qrs_spl[i] #corresponding position of peak value of QRS\n",
    "                if pos+right_len<=max_len and pos-left_len>=0:\n",
    "                    max_idx = pos+right_len#np.min([max_len, pos+trunc_len])\n",
    "                    min_idx = pos-left_len#np.max([0, pos-trunc_len])\n",
    "                    #for j, val in enumerate(lead_name):\n",
    "                        #QRS_value = signal[:,j][min_idx:max_idx]\n",
    "                        #data.append([QRS_value,labeltotext(qrs_sym[i]),val])#[QRS value, label, lead name]\n",
    "                    QRS_value = signal[:,0][min_idx:max_idx] #only one lead\n",
    "                    teData.append([QRS_value,labeltotext(qrs_sym[i])])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "teData = pd.DataFrame(np.array(teData))\n",
    "X_test = pd.DataFrame(teData[0].values.tolist())\n",
    "y_test = teData[1]\n",
    "print('The shape of testset is: (%d,%d)'%(X_test.shape[0],X_test.shape[1]))\n",
    "print(teData[1].value_counts())\n",
    "\n",
    "#model: faiss+index\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(right_len+left_len) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(X_train, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "#performance\n",
    "X_test = np.ascontiguousarray(X_test, dtype=np.float32)\n",
    "scores, neighbors = gpu_index.search(X_test, k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(y_train)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(y_test.tolist(), y_pred))\n",
    "#confusion matrix\n",
    "labels = list(set(y_pred))\n",
    "cm = confusion_matrix(y_test.tolist(), y_pred, labels=labels ) #labels=['N','S','V','F','Q']\n",
    "print (cm)\n",
    "print ('Specificity: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of S: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of V: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of F: %.6f'%float(cm[3][3]/np.sum(cm[3])))\n",
    "print ('Sensitivity of Q: %.6f'%float(cm[4][4]/np.sum(cm[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed buliding index in 1 seconds\n",
      "Accuracy: 0.739895\n",
      "[[244570  37574  25312   8600    436]\n",
      " [     0      0      0      0      0]\n",
      " [ 11079   3269  16322    498     16]\n",
      " [   607    334    273     50      2]\n",
      " [  2036    372   1314     13      8]]\n",
      "Specificity: 0.772753\n",
      "Sensitivity of S: nan\n",
      "Sensitivity of V: 0.523409\n",
      "Sensitivity of F: 0.039494\n",
      "Sensitivity of Q: 0.002137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:67: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "#AHA2MIT\n",
    "#https://archive.physionet.org/physiobank/annotations.shtml\n",
    "\n",
    "def labeltotext(val):\n",
    "    if val in ['N','L','R','e','j'] :\n",
    "        return 0 #N\n",
    "    elif val in ['A','a','J','S']:\n",
    "        return 1 #S\n",
    "    elif val in ['V','E']:\n",
    "        return 2 #V\n",
    "    elif val == 'F':\n",
    "        return 3 #F\n",
    "    elif val in ['/','f','Q']:\n",
    "        return 4 #Q\n",
    "    else: \n",
    "        pass\n",
    "rootdir = '/data/fjsdata/ECG/AHA2MIT'\n",
    "filename = list(set( [os.path.splitext(base)[0] for base in os.listdir(rootdir)]) )\n",
    "teData = [] #[QRS value, label]\n",
    "for bt in filename:#22 records for test\n",
    "    file = os.path.join(rootdir,str(bt))\n",
    "    try:\n",
    "        annotation = wfdb.rdann(file, 'atr') \n",
    "        qrs_spl = annotation.sample #numpy.ndarray\n",
    "        qrs_sym = annotation.symbol #list\n",
    "        #print (list(set(qrs_sym)))\n",
    "        record = wfdb.rdrecord(file)\n",
    "        signal = record.p_signal #numpy.ndarray\n",
    "        max_len = record.sig_len #length of samples\n",
    "        lead_name =  record.sig_name #names of lead channels,list\n",
    "        for i in range(annotation.ann_len):\n",
    "            if qrs_sym[i] in ['N','L','R','e','j','A','a','J','S','V','E','F','/','f','Q']:#five diseases samples\n",
    "                pos = qrs_spl[i] #corresponding position of peak value of QRS\n",
    "                if pos+right_len<=max_len and pos-left_len>=0:\n",
    "                    max_idx = pos+right_len#np.min([max_len, pos+trunc_len])\n",
    "                    min_idx = pos-left_len#np.max([0, pos-trunc_len])\n",
    "                    QRS_value = signal[:,0][min_idx:max_idx] #only one lead\n",
    "                    teData.append([QRS_value, labeltotext(qrs_sym[i])])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "teData = pd.DataFrame(np.array(teData))\n",
    "X_test = pd.DataFrame(teData[0].values.tolist()).fillna(0)\n",
    "y_test = teData[1]\n",
    "print('The shape of testset is: (%d,%d)'%(X_test.shape[0],X_test.shape[1]))\n",
    "print(teData[1].value_counts())\n",
    "\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(right_len+left_len) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(X_train, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "#performance\n",
    "X_test = np.ascontiguousarray(X_test, dtype=np.float32)\n",
    "scores, neighbors = gpu_index.search(X_test, k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(y_train)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(y_test.tolist(), y_pred))\n",
    "#confusion matrix\n",
    "labels = list(set(y_pred))\n",
    "cm = confusion_matrix(y_test.tolist(), y_pred, labels=labels ) #labels=['N','S','V','F','Q']\n",
    "print (cm)\n",
    "print ('Specificity: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of S: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of V: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of F: %.6f'%float(cm[3][3]/np.sum(cm[3])))\n",
    "print ('Sensitivity of Q: %.6f'%float(cm[4][4]/np.sum(cm[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
