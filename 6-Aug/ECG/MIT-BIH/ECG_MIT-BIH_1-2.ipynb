{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "from scipy.spatial.distance import pdist\n",
    "import cv2\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc \n",
    "from functools import reduce\n",
    "import wfdb#https://github.com/MIT-LCP/wfdb-python\n",
    "from wfdb import processing\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.set_device(0)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of trainset is: (51009,200)\n",
      "0    45854\n",
      "2     3788\n",
      "1      944\n",
      "3      415\n",
      "4        8\n",
      "Name: 1, dtype: int64\n",
      "The shape of testset is: (49698,200)\n",
      "0    44245\n",
      "2     3221\n",
      "1     1837\n",
      "3      388\n",
      "4        7\n",
      "Name: 1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Beats generation,\n",
    "#we defined a single ECG beat image by centering the Q-wave peak signal while\n",
    "#excluding the first and the last 20 ECG signals from the previous and afterward Q-wave peak signal\n",
    "#https://github.com/MIT-LCP/wfdb-python/blob/master/demo.ipynb\n",
    "#https://archive.physionet.org/physiobank/database/html/mitdbdir/mitdbdir.htm\n",
    "#http://www.tara.tcd.ie/bitstream/handle/2262/17623/automatic.pdf?sequence=1\n",
    "def labeltotext(val):\n",
    "    if val in ['N','L','R','e','j'] :\n",
    "        return 0 #N\n",
    "    elif val in ['A','a','J','S']:\n",
    "        return 1 #S\n",
    "    elif val in ['V','E']:\n",
    "        return 2 #V\n",
    "    elif val == 'F':\n",
    "        return 3 #F\n",
    "    elif val in ['/','f','Q']:\n",
    "        return 4 #Q\n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "rootdir = '/data/fjsdata/physionet/MIT-BIH/mitdb/'\n",
    "right_len = 180 #right sample length around of peak value of QRS\n",
    "left_len = 20 #left sample length around of peak value of QRS\n",
    "#get trainset\n",
    "trData = [] #[QRS value, label]\n",
    "for bt in [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]:#22 records for train\n",
    "    file = os.path.join(rootdir,str(bt))\n",
    "    try:\n",
    "        annotation = wfdb.rdann(file, 'atr') \n",
    "        qrs_spl = annotation.sample #numpy.ndarray\n",
    "        qrs_sym = annotation.symbol #list\n",
    "        record = wfdb.rdrecord(file)\n",
    "        signal = record.p_signal #numpy.ndarray\n",
    "        max_len = record.sig_len #length of samples\n",
    "        lead_name =  record.sig_name #names of lead channels,list\n",
    "        for i in range(annotation.ann_len):\n",
    "            if qrs_sym[i] in ['N','L','R','e','j','A','a','J','S','V','E','F','/','f','Q']:#seven diseases samples\n",
    "                pos = qrs_spl[i] #corresponding position of peak value of QRS\n",
    "                if pos+right_len<=max_len and pos-left_len>=0:\n",
    "                    max_idx = pos+right_len#np.min([max_len, pos+trunc_len])\n",
    "                    min_idx = pos-left_len#np.max([0, pos-trunc_len])\n",
    "                    #for j, val in enumerate(lead_name):\n",
    "                        #QRS_value = signal[:,j][min_idx:max_idx]\n",
    "                        #data.append([QRS_value,labeltotext(qrs_sym[i]),val])#[QRS value, label, lead name]\n",
    "                    QRS_value = signal[:,0][min_idx:max_idx] #only one lead\n",
    "                    trData.append([QRS_value,labeltotext(qrs_sym[i])])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "trData = pd.DataFrame(np.array(trData))\n",
    "X_train = pd.DataFrame(trData[0].values.tolist())\n",
    "y_train = trData[1]\n",
    "print('The shape of trainset is: (%d,%d)'%(X_train.shape[0],X_train.shape[1]))\n",
    "print(trData[1].value_counts())\n",
    "#get testset\n",
    "teData = [] #[QRS value, label]\n",
    "for bt in [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]:#22 records for test\n",
    "    file = os.path.join(rootdir,str(bt))\n",
    "    try:\n",
    "        annotation = wfdb.rdann(file, 'atr') \n",
    "        qrs_spl = annotation.sample #numpy.ndarray\n",
    "        qrs_sym = annotation.symbol #list\n",
    "        record = wfdb.rdrecord(file)\n",
    "        signal = record.p_signal #numpy.ndarray\n",
    "        max_len = record.sig_len #length of samples\n",
    "        lead_name =  record.sig_name #names of lead channels,list\n",
    "        for i in range(annotation.ann_len):\n",
    "            if qrs_sym[i] in ['N','L','R','e','j','A','a','J','S','V','E','F','/','f','Q']:#seven diseases samples\n",
    "                pos = qrs_spl[i] #corresponding position of peak value of QRS\n",
    "                if pos+right_len<=max_len and pos-left_len>=0:\n",
    "                    max_idx = pos+right_len#np.min([max_len, pos+trunc_len])\n",
    "                    min_idx = pos-left_len#np.max([0, pos-trunc_len])\n",
    "                    #for j, val in enumerate(lead_name):\n",
    "                        #QRS_value = signal[:,j][min_idx:max_idx]\n",
    "                        #data.append([QRS_value,labeltotext(qrs_sym[i]),val])#[QRS value, label, lead name]\n",
    "                    QRS_value = signal[:,0][min_idx:max_idx] #only one lead\n",
    "                    teData.append([QRS_value,labeltotext(qrs_sym[i])])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "teData = pd.DataFrame(np.array(teData))\n",
    "X_test = pd.DataFrame(teData[0].values.tolist())\n",
    "y_test = teData[1]\n",
    "print('The shape of testset is: (%d,%d)'%(X_test.shape[0],X_test.shape[1]))\n",
    "print(teData[1].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed buliding index in 1 seconds\n",
      "Accuracy: 0.826814\n",
      "[[38424  2401  2534   854    32]\n",
      " [ 1758    42     8    29     0]\n",
      " [  495    79  2624    23     0]\n",
      " [  260     0   127     1     0]\n",
      " [    2     0     5     0     0]]\n",
      "Specificity: 0.868437\n",
      "Sensitivity of S: 0.022863\n",
      "Sensitivity of V: 0.814654\n",
      "Sensitivity of F: 0.002577\n",
      "Sensitivity of Q: 0.000000\n"
     ]
    }
   ],
   "source": [
    "#model: faiss+index\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(right_len+left_len) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(X_train, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "#performance\n",
    "X_test = np.ascontiguousarray(X_test, dtype=np.float32)\n",
    "scores, neighbors = gpu_index.search(X_test, k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(y_train)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(y_test.tolist(), y_pred))\n",
    "#confusion matrix\n",
    "labels = list(set(y_pred))\n",
    "cm = confusion_matrix(y_test.tolist(), y_pred, labels=labels ) #labels=['N','S','V','F','Q']\n",
    "print (cm)\n",
    "print ('Specificity: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of S: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of V: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of F: %.6f'%float(cm[3][3]/np.sum(cm[3])))\n",
    "print ('Sensitivity of Q: %.6f'%float(cm[4][4]/np.sum(cm[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/sklearn/neighbors/approximate.py:258: DeprecationWarning: LSHForest has poor performance and has been deprecated in 0.19. It will be removed in version 0.21.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.819872\n",
      "[[38020  1574  3375  1265    11]\n",
      " [ 1425    56   311    45     0]\n",
      " [  487    50  2664    20     0]\n",
      " [  345     1    37     5     0]\n",
      " [    4     0     2     0     1]]\n",
      "Specificity: 0.859306\n",
      "Sensitivity of S: 0.030484\n",
      "Sensitivity of V: 0.827072\n",
      "Sensitivity of F: 0.012887\n",
      "Sensitivity of Q: 0.142857\n"
     ]
    }
   ],
   "source": [
    "#model: LSH\n",
    "from sklearn.neighbors import LSHForest\n",
    "lshf = LSHForest(min_hash_match=4, n_neighbors=20, n_candidates=10, n_estimators =10, random_state=42) #hashcode=32\n",
    "lshf.fit(X_train)  \n",
    "distances, indices = lshf.kneighbors(X_test, n_neighbors=1)#top1\n",
    "y_pred = []\n",
    "for i in indices.flatten():\n",
    "    y_pred.append(np.array(y_train)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(y_test.tolist(), y_pred))\n",
    "#confusion matrix\n",
    "labels = list(set(y_pred))\n",
    "cm = confusion_matrix(y_test.tolist(), y_pred, labels=labels ) #labels=['N','S','V','F','Q']\n",
    "print (cm)\n",
    "print ('Specificity: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of S: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of V: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of F: %.6f'%float(cm[3][3]/np.sum(cm[3])))\n",
    "print ('Sensitivity of Q: %.6f'%float(cm[4][4]/np.sum(cm[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHR@5=0.195625, MAP@5=0.146205, MRR@5=0.760649\n",
      "MHR@10=0.197250, MAP@10=0.180814, MRR@10=0.569168\n",
      "MHR@15=0.179875, MAP@15=0.137433, MRR@15=0.531339\n",
      "MHR@20=0.198063, MAP@20=0.186761, MRR@20=0.553211\n"
     ]
    }
   ],
   "source": [
    "#model: BMF\n",
    "class BayesianMatrixFactorization():\n",
    "    \"\"\"\n",
    "    Bayesian Matrix Factorization model\n",
    "    R = PxQ\n",
    "    p ~ N(p|0, alpha^(-1)I)\n",
    "    q ~ N(q|0, alpha^(-1)I)\n",
    "    r = p @ q\n",
    "    t ~ N(r|p @ q, beta^(-1))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha_p:float=1., alpha_q:float=1., beta:float=1.):\n",
    "        \"\"\"\n",
    "        ----------\n",
    "        n_u, n_i: the number of users and items, respectively.\n",
    "        k : the number of latent factors\n",
    "        \"\"\"\n",
    "        self.alpha_p = alpha_p\n",
    "        self.alpha_q = alpha_q\n",
    "        self.beta = beta\n",
    "        #posterior of p,q \n",
    "        self.pos_mean_p = None\n",
    "        self.pos_precision_p = None\n",
    "        self.pos_mean_q = None\n",
    "        self.pos_precision_q = None\n",
    "\n",
    "    def fit(self, R:np.ndarray, k:int=5):\n",
    "        \"\"\"\n",
    "        bayesian update of parameters given training dataset\n",
    "        Parameters\n",
    "        ----------\n",
    "        R : (u,i) np.ndarray\n",
    "            training data independent variable, u is the number of users, i is the number of items.\n",
    "        k : int, the number of latent factors.\n",
    "        \"\"\"\n",
    "        #1. generate matrices P, Q\n",
    "        P = np.random.normal(0,self.alpha_p,(R.shape[0],k))#uxk\n",
    "        Q = np.random.normal(0,self.alpha_q,(R.shape[1],k))#ixk\n",
    "        #2.calculate the posterior with analytical solution\n",
    "        self.pos_precision_p = self.alpha_p + self.beta * Q @ Q.T # ixi\n",
    "        self.pos_mean_p = self.beta * R @ np.linalg.inv(self.pos_precision_p) @ Q # uxi,ixi,ixk -> uxk\n",
    "        self.pos_precision_q = self.alpha_q + self.beta * P @ P.T # uxu\n",
    "        self.pos_mean_q = self.beta * R.T @ np.linalg.inv(self.pos_precision_q) @ P # ixu,uxu,uxk -> ixk\n",
    "        \n",
    "    def predict(self, sample_size:int=None):\n",
    "        \"\"\"\n",
    "        return mean  of predictive distribution\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample_size : int, optional\n",
    "            number of samples to draw from the predictive distribution\n",
    "            (the default is None, no sampling from the distribution)\n",
    "        Returns\n",
    "        -------\n",
    "        R_pred : (u,i) np.ndarray\n",
    "            mean of the predictive distribution\n",
    "        R_pred_sample : (u,i,sample_size) np.ndarray\n",
    "            samples from the predictive distribution\n",
    "        \"\"\"\n",
    "        if sample_size is not None:\n",
    "            R_sample = []\n",
    "            for i in range(sample_size):\n",
    "                p_sample, q_sample = [], []\n",
    "                for k in range(self.pos_mean_p.shape[1]):#latent factors    \n",
    "                    mean_p = self.pos_mean_p[:,k]\n",
    "                    mean_q = self.pos_mean_q[:,k]\n",
    "                    p_sample_k = np.random.multivariate_normal(mean_p, np.linalg.inv(self.pos_precision_q), size=1)\n",
    "                    q_sample_k = np.random.multivariate_normal(mean_q, np.linalg.inv(self.pos_precision_p), size=1)\n",
    "                    p_sample.append(p_sample_k.flatten())\n",
    "                    q_sample.append(q_sample_k.flatten())\n",
    "                R_sample.append(np.dot(np.array(p_sample).T, np.array(q_sample)))\n",
    "            return  R_sample #uxi\n",
    "        \n",
    "        R_pred = self.pos_mean_p @ self.pos_mean_q.T #R = PxQ\n",
    "        return R_pred #uxi\n",
    "\n",
    "for topk in [5,10,15,20]:\n",
    "    bmf = BayesianMatrixFactorization()\n",
    "    bmf.fit(R=np.array(X_train), k=topk)\n",
    "    R_pred = bmf.predict()\n",
    "    #R_pred = np.array(bmf.predict(sample_size=10)).mean(axis=0)#sample weights.\n",
    "    # buliding index of trainset\n",
    "    #tstart = time.time()\n",
    "    cpu_index = faiss.IndexFlatL2(trunc_len*2) #\n",
    "    gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "    gpu_index.add(np.ascontiguousarray(R_pred, dtype=np.float32)) #add data(must be float32) to index\n",
    "    #elapsed = time.time() - tstart    \n",
    "    #print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "    #performance\n",
    "    X_test = np.ascontiguousarray(X_test, dtype=np.float32)\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    for i in range(y_test.shape[0]):\n",
    "        stype = np.array(y_test)[i]\n",
    "        scores, neighbors = gpu_index.search(X_test[i:i+1], k=topk)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in neighbors.flatten():\n",
    "            dtype = np.array(y_train)[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"MHR@{}={:.6f}, MAP@{}={:.6f}, MRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15 / 720 : loss = 986.149048"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:102: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 719 / 720 : loss = 801.465942Eopch:     1 mean_loss = 895.234918\n",
      " 719 / 720 : loss = 622.016052Eopch:     2 mean_loss = 710.581473\n",
      " 719 / 720 : loss = 458.642914Eopch:     3 mean_loss = 538.559606\n",
      " 719 / 720 : loss = 318.922821Eopch:     4 mean_loss = 386.437353\n",
      " 719 / 720 : loss = 208.740997Eopch:     5 mean_loss = 261.029209\n",
      " 719 / 720 : loss = 128.001282Eopch:     6 mean_loss = 165.658108\n",
      " 719 / 720 : loss = 74.2212452Eopch:     7 mean_loss = 99.096252\n",
      " 719 / 720 : loss = 25.279182Eopch:     9 mean_loss = 31.754674\n",
      " 719 / 720 : loss = 13.078069Eopch:    10 mean_loss = 18.094523\n",
      " 79 / 80 0 mHR@5=0.182500, mAP@5=0.182500, mRR@5=1.000000\n",
      "mHR@10=0.182500, mAP@10=0.182500, mRR@10=1.000000\n",
      "mHR@15=0.182500, mAP@15=0.182500, mRR@15=1.000000\n",
      "mHR@20=0.182500, mAP@20=0.182500, mRR@20=1.000000\n"
     ]
    }
   ],
   "source": [
    "def log_gaussian_loss(output, target, sigma, no_dim, sum_reduce=True):\n",
    "    exponent = -0.5*(target - output)**2/sigma**2\n",
    "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
    "    \n",
    "    if sum_reduce:\n",
    "        return -(log_coeff + exponent).sum()\n",
    "    else:\n",
    "        return -(log_coeff + exponent)\n",
    "    \n",
    "class gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def loglik(self, weights):\n",
    "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
    "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
    "        \n",
    "        return (exponent + log_coeff).sum()\n",
    "    \n",
    "class BayesLinear_Normalq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-3, -3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # sample gaussian noise for each weight\n",
    "        weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())      \n",
    "        # calculate the weight stds from the rho parameters\n",
    "        weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "        # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "        weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "    \n",
    "        output = torch.mm(x, weight_sample)\n",
    "            \n",
    "        # computing the KL loss term\n",
    "        #reference: https://github.com/jojonki/AutoEncoders/blob/master/kl_divergence_between_two_gaussians.pdf\n",
    "        prior_cov, varpost_cov = self.prior.sigma**2, weight_stds**2\n",
    "        KL_loss = 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*weight_stds.numel()\n",
    "        KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "        KL_loss = KL_loss + 0.5*((self.weight_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "        return output, KL_loss\n",
    "\n",
    "\n",
    "class BayesianNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=180, num_units=[128,32], output_dim=6):\n",
    "        super(BayesianNeuralNetwork, self).__init__()\n",
    "        \n",
    "        # network with Bayesian linear.\n",
    "        self.layer1 = BayesLinear_Normalq(input_dim, num_units[0], gaussian(0, 3))\n",
    "        self.layer2 = BayesLinear_Normalq(num_units[0], num_units[1], gaussian(0, 3))\n",
    "        self.layer3 = BayesLinear_Normalq(num_units[1], output_dim, gaussian(0, 3))\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "        # noise\n",
    "        self.log_noise = nn.Parameter(torch.cuda.FloatTensor([3]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        KL_loss_total = 0\n",
    "        #x = x.view(-1, self.input_dim)\n",
    "        x = x.view(x.size(0),-1) \n",
    "        #layer1\n",
    "        x, KL_loss = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        #layer2\n",
    "        x, KL_loss = self.layer2(x)\n",
    "        x = self.activation(x) \n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        #layer3\n",
    "        out, KL_loss = self.layer3(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        \n",
    "        return x, out, KL_loss_total\n",
    "\n",
    "\n",
    "#define model\n",
    "model = BayesianNeuralNetwork().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "batchSize = 100\n",
    "num_batches = len(y_train) // batchSize\n",
    "for epoch in range(10):#iteration\n",
    "    #train model \n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(y_train), (i+1)*batchSize])\n",
    "        X_batch = torch.from_numpy(np.array(X_train[min_idx: max_idx]).astype(np.float32)).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(np.array(y_train[min_idx: max_idx]).astype(np.float32)).type(torch.LongTensor).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        #forward\n",
    "        _, out, KL_loss = model(X_batch)#adjust channel to the second\n",
    "        out = F.log_softmax(out)\n",
    "        fit_loss = F.nll_loss(out, Y_batch)\n",
    "        loss = (KL_loss+fit_loss)/batchSize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    \n",
    "#release gpu memory\n",
    "#model = model.cpu()\n",
    "#torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "num_batches = len(y_train) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(y_train), (i+1)*batchSize])\n",
    "    X_batch = torch.from_numpy(np.array(X_train[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch, _, _ = model(X_batch)#forword\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teF = []\n",
    "num_batches = len(y_test) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(y_test), (i+1)*batchSize])\n",
    "    X_batch = torch.from_numpy(np.array(X_test[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch, _, _ = model(X_batch)#forword\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#Evaluate model\n",
    "cpu_index = faiss.IndexFlatL2(np.array(trF).shape[1]) #index with Faiss\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu  usable\n",
    "gpu_index.add(np.array(trF).astype('float32')) #add data to index\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = np.array(y_test)[i]\n",
    "        scores, neighbors = gpu_index.search(np.array(teF)[i:i+1].astype('float32'), k=topk)\n",
    "        #map_item_score = {}\n",
    "        #for j, trVal in enumerate(trF):\n",
    "        #    map_item_score[j] = pdist(np.vstack([teVal,trVal]),'cosine')\n",
    "        #ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors.flatten():\n",
    "            dtype = np.array(y_train)[i]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39 / 720 : loss = 1.102698"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 719 / 720 : loss = 0.255489Eopch:     1 mean_loss = 0.567560\n",
      " 719 / 720 : loss = 0.152731Eopch:     2 mean_loss = 0.270533\n",
      " 719 / 720 : loss = 0.113899Eopch:     3 mean_loss = 0.217814\n",
      " 719 / 720 : loss = 0.093981Eopch:     4 mean_loss = 0.187924\n",
      " 719 / 720 : loss = 0.080152Eopch:     5 mean_loss = 0.166353\n",
      " 719 / 720 : loss = 0.062912Eopch:     6 mean_loss = 0.149539\n",
      " 719 / 720 : loss = 0.048382Eopch:     7 mean_loss = 0.136564\n",
      " 719 / 720 : loss = 0.047562Eopch:     8 mean_loss = 0.126280\n",
      " 719 / 720 : loss = 0.032049Eopch:     9 mean_loss = 0.118871\n",
      " 719 / 720 : loss = 0.028015Eopch:    10 mean_loss = 0.113064\n",
      " 79 / 80 0 mHR@5=0.182500, mAP@5=0.182500, mRR@5=1.000000\n",
      "mHR@10=0.182500, mAP@10=0.182500, mRR@10=1.000000\n",
      "mHR@15=0.182500, mAP@15=0.182500, mRR@15=1.000000\n",
      "mHR@20=0.182500, mAP@20=0.182500, mRR@20=1.000000\n"
     ]
    }
   ],
   "source": [
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=180, num_units=[128,32], output_dim=6):\n",
    "        super(DeepNeuralNetwork, self).__init__()\n",
    "        \n",
    "        # network with  linear.\n",
    "        self.layer1 = nn.Linear(input_dim,num_units[0])\n",
    "        self.layer2 = nn.Linear(num_units[0],num_units[1])\n",
    "        self.layer3 = nn.Linear(num_units[1],output_dim)\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0),-1) \n",
    "        #layer1\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        #layer2\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation(x) \n",
    "        #layer3\n",
    "        out = self.layer3(x)\n",
    "        \n",
    "        return x, out\n",
    "\n",
    "\n",
    "#define model\n",
    "model = DeepNeuralNetwork().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "batchSize = 100\n",
    "num_batches = len(y_train) // batchSize\n",
    "for epoch in range(10):#iteration\n",
    "    #train model \n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(y_train), (i+1)*batchSize])\n",
    "        X_batch = torch.from_numpy(np.array(X_train[min_idx: max_idx]).astype(np.float32)).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(np.array(y_train[min_idx: max_idx]).astype(np.float32)).type(torch.LongTensor).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        #forward\n",
    "        _, out = model(X_batch)#adjust channel to the second\n",
    "        out = F.log_softmax(out)\n",
    "        loss = F.nll_loss(out, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    \n",
    "#release gpu memory\n",
    "#model = model.cpu()\n",
    "#torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "num_batches = len(y_train) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(y_train), (i+1)*batchSize])\n",
    "    X_batch = torch.from_numpy(np.array(X_train[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch, _ = model(X_batch)#forword\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teF = []\n",
    "num_batches = len(y_test) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(y_test), (i+1)*batchSize])\n",
    "    X_batch = torch.from_numpy(np.array(X_test[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch, _= model(X_batch)#forword\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#Evaluate model\n",
    "cpu_index = faiss.IndexFlatL2(np.array(trF).shape[1]) #index with Faiss\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu  usable\n",
    "gpu_index.add(np.array(trF).astype('float32')) #add data to index\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = np.array(y_test)[i]\n",
    "        scores, neighbors = gpu_index.search(np.array(teF)[i:i+1].astype('float32'), k=topk)\n",
    "        #map_item_score = {}\n",
    "        #for j, trVal in enumerate(trF):\n",
    "        #    map_item_score[j] = pdist(np.vstack([teVal,trVal]),'cosine')\n",
    "        #ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors.flatten():\n",
    "            dtype = np.array(y_train)[i]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 0 : loss = 1.788375\r",
      " 1 : loss = 1.762425\r",
      " 2 : loss = 1.743216\r",
      " 3 : loss = 1.726169\r",
      " 4 : loss = 1.708836\r",
      " 5 : loss = 1.691512\r",
      " 6 : loss = 1.674611\r",
      " 7 : loss = 1.656513\r",
      " 8 : loss = 1.637872\r",
      " 9 : loss = 1.61822torch.Size([2, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(10,180).cuda()\n",
    "y = torch.LongTensor([0,1,2,3,4,5,0,1,3,4]).cuda()\n",
    "model = DeepNeuralNetwork().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    _,out = model(x1)\n",
    "    out = F.log_softmax(out)\n",
    "    loss = F.nll_loss(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    sys.stdout.write('\\r {} : loss = {}'.format(epoch, float('%0.6f'%loss.item())))\n",
    "    #sys.stdout.flush()\n",
    "#output\n",
    "x2 = torch.rand(2,180).cuda()\n",
    "x2,_ = model(x2)\n",
    "#print (x2)\n",
    "print (x2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
