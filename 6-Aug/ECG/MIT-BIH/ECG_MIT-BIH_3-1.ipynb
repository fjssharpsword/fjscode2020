{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import itertools  \n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "from scipy.spatial.distance import pdist\n",
    "import cv2\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc \n",
    "from functools import reduce\n",
    "import wfdb#https://github.com/MIT-LCP/wfdb-python\n",
    "from wfdb import processing\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.set_device(0)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 / 20000 The length of train set is 20000\n",
      "20000 / 20000 The length of test set is 20000\n"
     ]
    }
   ],
   "source": [
    "#read train image with CV\n",
    "train_dir = '/data/tmpexec/ecg/train' #the path of images\n",
    "trI, trY = [],[]\n",
    "for iname in os.listdir(train_dir):\n",
    "    if iname.endswith(\".png\"):\n",
    "        try:\n",
    "            image_path = os.path.join(train_dir, iname)\n",
    "            itype = int(os.path.splitext(iname)[0].split(\"-\")[1])\n",
    "            img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(500,300,3)->(256,256,3)\n",
    "            trI.append(img)\n",
    "            trY.append(itype)\n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trY),20000))\n",
    "        sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(trY))\n",
    "#read test image with CV\n",
    "test_dir = '/data/tmpexec/ecg/test' #the path of images\n",
    "teI, teY = [],[]\n",
    "for iname in os.listdir(test_dir):\n",
    "    if iname.endswith(\".png\"):\n",
    "        try:\n",
    "            image_path = os.path.join(test_dir, iname)\n",
    "            itype = int(os.path.splitext(iname)[0].split(\"-\")[1])\n",
    "            img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(500,300,3)->(256,256,3)\n",
    "            teI.append(img)\n",
    "            teY.append(itype)\n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(teY),20000))\n",
    "        sys.stdout.flush()\n",
    "print('The length of test set is %d'%len(teY))\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs(spls=len(trY)):\n",
    "    idx_sf = random.sample(range(0, len(trY)),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 / 1000 : loss = 0.610888Eopch:     1 mean_loss = 0.378035\n",
      " 1000 / 1000 : loss = 0.628302Eopch:     2 mean_loss = 0.212210\n",
      " 1000 / 1000 : loss = 0.47035Eopch:     3 mean_loss = 0.146163\n",
      " 1000 / 1000 : loss = 0.341027Eopch:     4 mean_loss = 0.115741\n",
      " 1000 / 1000 : loss = 0.061092Eopch:     5 mean_loss = 0.095464\n",
      " 1000 / 1000 : loss = 0.040612Eopch:     6 mean_loss = 0.074126\n",
      " 1000 / 1000 : loss = 0.059186Eopch:     7 mean_loss = 0.063733\n",
      " 1000 / 1000 : loss = 0.042861Eopch:     8 mean_loss = 0.056828\n",
      " 1000 / 1000 : loss = 0.058322Eopch:     9 mean_loss = 0.048546\n",
      " 1000 / 1000 : loss = 0.070662Eopch:    10 mean_loss = 0.043846\n",
      "best_loss = 0.043846\n",
      " 1999 / 2000 Completed buliding index in 1 seconds\n",
      "Accuracy: 0.733600\n",
      "[[11225   479  2959   169    14]\n",
      " [  547   117   268    12     0]\n",
      " [  270   136  3321    54     7]\n",
      " [  254     9   141     9     1]\n",
      " [    1     1     6     0     0]]\n",
      "Specificity: 0.756096\n",
      "Sensitivity of S: 0.123941\n",
      "Sensitivity of V: 0.876716\n",
      "Sensitivity of F: 0.021739\n",
      "Sensitivity of Q: 0.000000\n"
     ]
    }
   ],
   "source": [
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ASHNet(nn.Module):\n",
    "    def __init__(self, code_size: int):\n",
    "        super().__init__()\n",
    "        #Resnet\n",
    "        self.net = nn.Sequential(\n",
    "            ResBlock(in_channels=3, out_channels=16),\n",
    "            ResBlock(in_channels=16, out_channels=16),\n",
    "            ResBlock(in_channels=16, out_channels=16, stride=2),\n",
    "        ) \n",
    "        #Attention \n",
    "        self.sa = SpatialAttention() \n",
    "        #fully connected\n",
    "        self.linear = nn.Sequential(\n",
    "            #nn.Linear(16*128*128, 4096),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.Linear(16*128*128, code_size),\n",
    "            #nn.ReLU(inplace=True) #nn.Tanh()#[-1,1]\n",
    "        )\n",
    "        \n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.sa(x)*x\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "            \n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.l1_loss = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    def forward(self,h1,h2,y):    \n",
    "        margin_val = self.margin * h1.shape[1]\n",
    "        squared_loss = torch.mean(self.mse_loss(h1, h2), dim=1)\n",
    "        # T1: 0.5 * (1 - y) * dist(x1, x2)\n",
    "        positive_pair_loss = (0.5 * (1 - y) * squared_loss)\n",
    "        mean_positive_pair_loss = torch.mean(positive_pair_loss)\n",
    "        # T2: 0.5 * y * max(margin - dist(x1, x2), 0)\n",
    "        zeros = torch.zeros_like(squared_loss)\n",
    "        marginMat = margin_val * torch.ones_like(squared_loss)\n",
    "        negative_pair_loss = 0.5 * y * torch.max(zeros, marginMat - squared_loss)\n",
    "        mean_negative_pair_loss = torch.mean(negative_pair_loss)\n",
    "\n",
    "        # T3: alpha(dst_l1(abs(x1), 1)) + dist_l1(abs(x2), 1)))\n",
    "        mean_value_regularization = self.alpha * (\n",
    "                self.l1_loss(torch.abs(h1), torch.ones_like(h1)) +\n",
    "                self.l1_loss(torch.abs(h2), torch.ones_like(h2)))\n",
    "\n",
    "        loss = mean_positive_pair_loss + mean_negative_pair_loss + mean_value_regularization\n",
    "        return loss\n",
    "    \n",
    "#define model\n",
    "hash_size=12\n",
    "model = ASHNet(code_size=hash_size).cuda()\n",
    "criterion  = HashLossFunc(margin=0.5).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "num_batches = len(trY_sf) // batchSize \n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "loss=loss.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(hash_size) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "#confusion matrix\n",
    "labels = list(set(y_pred))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels ) #labels=['N','S','V','F','Q']\n",
    "print (cm)\n",
    "print ('Specificity: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of S: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of V: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of F: %.6f'%float(cm[3][3]/np.sum(cm[3])))\n",
    "print ('Sensitivity of Q: %.6f'%float(cm[4][4]/np.sum(cm[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 / 1000 : loss = 1.709669Eopch:     1 mean_loss = 1.181406\n",
      " 1000 / 1000 : loss = 1.488833Eopch:     2 mean_loss = 0.758753\n",
      " 1000 / 1000 : loss = 1.488512Eopch:     3 mean_loss = 0.604034\n",
      " 1000 / 1000 : loss = 1.317083Eopch:     4 mean_loss = 0.517706\n",
      " 1000 / 1000 : loss = 1.421975Eopch:     5 mean_loss = 0.464073\n",
      " 1000 / 1000 : loss = 1.237505Eopch:     6 mean_loss = 0.414527\n",
      " 1000 / 1000 : loss = 1.077662Eopch:     7 mean_loss = 0.369818\n",
      " 1000 / 1000 : loss = 0.815454Eopch:     8 mean_loss = 0.346208\n",
      " 1000 / 1000 : loss = 0.797991Eopch:     9 mean_loss = 0.320465\n",
      " 1000 / 1000 : loss = 0.75585Eopch:    10 mean_loss = 0.296446\n",
      "best_loss = 0.296446\n",
      " 1999 / 2000 Completed buliding index in 1 seconds\n",
      "Accuracy: 0.719300\n",
      "[[11104  1033  2273   377    59]\n",
      " [  555   110   250    24     5]\n",
      " [  206   290  3151   119    22]\n",
      " [  223    30   136    21     4]\n",
      " [    2     2     3     1     0]]\n",
      "Specificity: 0.747946\n",
      "Sensitivity of S: 0.116525\n",
      "Sensitivity of V: 0.831837\n",
      "Sensitivity of F: 0.050725\n",
      "Sensitivity of Q: 0.000000\n"
     ]
    }
   ],
   "source": [
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ASHNet(nn.Module): #deep Hashint Network:DHNet\n",
    "    def __init__(self,inChannels=3,code_size=16):\n",
    "        super(ASHNet, self).__init__()\n",
    "        #(channels, Height, Width)\n",
    "        #layer1: Convolution, (3,256,256)->(16,256,256)\n",
    "        self.conv1 = nn.Conv2d(in_channels=inChannels, out_channels=16, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        #layer2: max pooling,(16,256,256)->(16,128,128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        #layer3: Channel and Spatial Attention Layer, (8,256,256)->(8,256,256)\n",
    "        self.sa = SpatialAttention()\n",
    "        #layer4: Convolution, (16,128,128)->(32,64,64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #layer5: mean pooling, (32,64,64)->(32,32,32)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(32)\n",
    "        #layer6: fully connected, 32*32*32->4096\n",
    "        self.fcl1 = nn.Linear(32*32*32,4096)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        #layer7: Hashing layer, 4096->16\n",
    "        self.fcl2 = nn.Linear(4096,code_size)\n",
    "              \n",
    "    def forward(self,x):\n",
    "        #input: (batch_size, in_channels, Height, Width)\n",
    "        #output: (batch_size, out_channels, Height, Width)\n",
    "        #layer1: convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        #layer2: max pooling\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        #layer3: Attention\n",
    "        x = self.sa(x) * x  #spatial\n",
    "        #layer4: Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu2(x)\n",
    "        #layer5: mean pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = self.bn4(x)\n",
    "        #layer6:fully connected layer\n",
    "        x = x.view(x.size(0),-1) #transfer three dims to one dim\n",
    "        x = self.fcl1(x)\n",
    "        x = self.relu3(x)\n",
    "        #layer7: Hashing layer\n",
    "        x = self.fcl2(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.l1_loss = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    def forward(self,h1,h2,y):    \n",
    "        margin_val = self.margin * h1.shape[1]\n",
    "        squared_loss = torch.mean(self.mse_loss(h1, h2), dim=1)\n",
    "        # T1: 0.5 * (1 - y) * dist(x1, x2)\n",
    "        positive_pair_loss = (0.5 * (1 - y) * squared_loss)\n",
    "        mean_positive_pair_loss = torch.mean(positive_pair_loss)\n",
    "        # T2: 0.5 * y * max(margin - dist(x1, x2), 0)\n",
    "        zeros = torch.zeros_like(squared_loss)\n",
    "        marginMat = margin_val * torch.ones_like(squared_loss)\n",
    "        negative_pair_loss = 0.5 * y * torch.max(zeros, marginMat - squared_loss)\n",
    "        mean_negative_pair_loss = torch.mean(negative_pair_loss)\n",
    "\n",
    "        # T3: alpha(dst_l1(abs(x1), 1)) + dist_l1(abs(x2), 1)))\n",
    "        mean_value_regularization = self.alpha * (\n",
    "                self.l1_loss(torch.abs(h1), torch.ones_like(h1)) +\n",
    "                self.l1_loss(torch.abs(h2), torch.ones_like(h2)))\n",
    "\n",
    "        loss = mean_positive_pair_loss + mean_negative_pair_loss + mean_value_regularization\n",
    "        return loss\n",
    "    \n",
    "#define model\n",
    "hash_size=36\n",
    "model = ASHNet(code_size=hash_size).cuda()\n",
    "criterion  = HashLossFunc(margin=0.5).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "num_batches = len(trY_sf) // batchSize \n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "loss=loss.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(hash_size) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "#confusion matrix\n",
    "labels = list(set(y_pred))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels ) #labels=['N','S','V','F','Q']\n",
    "print (cm)\n",
    "print ('Specificity: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of S: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of V: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of F: %.6f'%float(cm[3][3]/np.sum(cm[3])))\n",
    "print ('Sensitivity of Q: %.6f'%float(cm[4][4]/np.sum(cm[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of DS1 is: (20000,360)\n",
      "0    14846\n",
      "2     3788\n",
      "1      944\n",
      "3      414\n",
      "4        8\n",
      "Name: 1, dtype: int64\n",
      "The shape of DS2 is: (20000,360)\n",
      "0    14548\n",
      "2     3220\n",
      "1     1837\n",
      "3      388\n",
      "4        7\n",
      "Name: 1, dtype: int64\n",
      "Completed buliding index in 1 seconds\n",
      "Accuracy: 0.807300\n",
      "[[12802   419  1340   285     0]\n",
      " [  638   148   151     7     0]\n",
      " [  518    79  3158    33     0]\n",
      " [  277     1    98    38     0]\n",
      " [    6     0     2     0     0]]\n",
      "Specificity: 0.862320\n",
      "Sensitivity of S: 0.156780\n",
      "Sensitivity of V: 0.833685\n",
      "Sensitivity of F: 0.091787\n",
      "Sensitivity of Q: 0.000000\n"
     ]
    }
   ],
   "source": [
    "#Beats generation,\n",
    "#we defined a single ECG beat image by centering the Q-wave peak signal while\n",
    "#excluding the first and the last 20 ECG signals from the previous and afterward Q-wave peak signal\n",
    "#https://github.com/MIT-LCP/wfdb-python/blob/master/demo.ipynb\n",
    "#https://archive.physionet.org/physiobank/database/html/mitdbdir/mitdbdir.htm\n",
    "#http://www.tara.tcd.ie/bitstream/handle/2262/17623/automatic.pdf?sequence=1\n",
    "def labeltotext(val):\n",
    "    if val in ['N','L','R','e','j'] :\n",
    "        return 0 #N\n",
    "    elif val in ['A','a','J','S']:\n",
    "        return 1 #S\n",
    "    elif val in ['V','E']:\n",
    "        return 2 #V\n",
    "    elif val == 'F':\n",
    "        return 3 #F\n",
    "    elif val in ['/','f','Q']:\n",
    "        return 4 #Q\n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "rootdir = '/data/fjsdata/physionet/MIT-BIH/mitdb/'\n",
    "right_len = 180 #right sample length around of peak value of QRS\n",
    "left_len = 180 #left sample length around of peak value of QRS\n",
    "#get trainset\n",
    "trData = [] #[QRS value, label]\n",
    "for bt in [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]:#22 records for train\n",
    "    file = os.path.join(rootdir,str(bt))\n",
    "    try:\n",
    "        annotation = wfdb.rdann(file, 'atr') \n",
    "        qrs_spl = annotation.sample #numpy.ndarray\n",
    "        qrs_sym = annotation.symbol #list\n",
    "        record = wfdb.rdrecord(file)\n",
    "        signal = record.p_signal #numpy.ndarray\n",
    "        max_len = record.sig_len #length of samples\n",
    "        lead_name =  record.sig_name #names of lead channels,list\n",
    "        for i in range(annotation.ann_len):\n",
    "            if qrs_sym[i] in ['N','L','R','e','j','A','a','J','S','V','E','F','/','f','Q']:#seven diseases samples\n",
    "                pos = qrs_spl[i] #corresponding position of peak value of QRS\n",
    "                if pos+right_len<=max_len and pos-left_len>=0:\n",
    "                    max_idx = pos+right_len#np.min([max_len, pos+trunc_len])\n",
    "                    min_idx = pos-left_len#np.max([0, pos-trunc_len])\n",
    "                    QRS_value = signal[:,0][min_idx:max_idx] #only one lead\n",
    "                    trData.append([QRS_value,labeltotext(qrs_sym[i])])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "NOR = 14846 #normal samples    \n",
    "trData = pd.DataFrame(np.array(trData))\n",
    "NData =  trData[trData[1]==0].sample(n=NOR, random_state=1)\n",
    "UNData = trData[trData[1]!=0]\n",
    "trData = pd.concat([NData,UNData],axis=0).sample(frac=1) #shuffle\n",
    "X_DS1 = pd.DataFrame(trData[0].values.tolist())\n",
    "y_DS1 = trData[1]\n",
    "print('The shape of DS1 is: (%d,%d)'%(X_DS1.shape[0],X_DS1.shape[1]))\n",
    "print(trData[1].value_counts())\n",
    "\n",
    "#get testset\n",
    "teData = [] #[QRS value, label]\n",
    "for bt in [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]:#22 records for test\n",
    "    file = os.path.join(rootdir,str(bt))\n",
    "    try:\n",
    "        annotation = wfdb.rdann(file, 'atr') \n",
    "        qrs_spl = annotation.sample #numpy.ndarray\n",
    "        qrs_sym = annotation.symbol #list\n",
    "        record = wfdb.rdrecord(file)\n",
    "        signal = record.p_signal #numpy.ndarray\n",
    "        max_len = record.sig_len #length of samples\n",
    "        lead_name =  record.sig_name #names of lead channels,list\n",
    "        for i in range(annotation.ann_len):\n",
    "            if qrs_sym[i] in ['N','L','R','e','j','A','a','J','S','V','E','F','/','f','Q']:#seven diseases samples\n",
    "                pos = qrs_spl[i] #corresponding position of peak value of QRS\n",
    "                if pos+right_len<=max_len and pos-left_len>=0:\n",
    "                    max_idx = pos+right_len#np.min([max_len, pos+trunc_len])\n",
    "                    min_idx = pos-left_len#np.max([0, pos-trunc_len])\n",
    "                    QRS_value = signal[:,0][min_idx:max_idx] #only one lead\n",
    "                    teData.append([QRS_value,labeltotext(qrs_sym[i])])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "NOR = 14548 #normal samples    \n",
    "teData = pd.DataFrame(np.array(teData))\n",
    "NData =  teData[teData[1]==0].sample(n=NOR, random_state=1)\n",
    "UNData = teData[teData[1]!=0]\n",
    "teData = pd.concat([NData,UNData],axis=0).sample(frac=1) #shuffle\n",
    "X_DS2 = pd.DataFrame(teData[0].values.tolist())\n",
    "y_DS2 = teData[1]\n",
    "print('The shape of DS2 is: (%d,%d)'%(X_DS2.shape[0],X_DS2.shape[1]))\n",
    "print(teData[1].value_counts())\n",
    "\n",
    "#model: faiss+index\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(right_len+left_len) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(X_DS2, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(X_DS1, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(y_DS2)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(y_DS1.tolist(), y_pred))\n",
    "#confusion matrix\n",
    "labels = list(set(y_DS1))\n",
    "cm = confusion_matrix(y_DS1.tolist(), y_pred, labels=labels ) #labels=['N','S','V','F','Q']\n",
    "print (cm)\n",
    "print ('Specificity: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of S: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of V: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of F: %.6f'%float(cm[3][3]/np.sum(cm[3])))\n",
    "print ('Sensitivity of Q: %.6f'%float(cm[4][4]/np.sum(cm[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19999 / 20000 "
     ]
    }
   ],
   "source": [
    "#import ecg_plot#pip install ecg_plot, https://pypi.org/project/ecg-plot/\n",
    "#from scipy.misc import electrocardiogram \n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.electrocardiogram.html\n",
    "fs = 360\n",
    "for idx,row in X_DS2.iterrows():\n",
    "    label = np.array(y_DS2)[idx]\n",
    "    svpath = os.path.join('/data/tmpexec/ecg/train',str(idx)+'-'+str(label))\n",
    "    ecg = np.array(row)\n",
    "    time = np.arange(ecg.size) / fs\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.plot(time, ecg)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(svpath,dpi=100) #(500=5*100,300=3*100)\n",
    "    plt.close()\n",
    "    sys.stdout.write('\\r{} / {} '.format(idx,X_DS2.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "for idx,row in X_DS1.iterrows():\n",
    "    label = np.array(y_DS1)[idx]\n",
    "    svpath = os.path.join('/data/tmpexec/ecg/test',str(idx)+'-'+str(label))\n",
    "    ecg = np.array(row)\n",
    "    time = np.arange(ecg.size) / fs\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.plot(time, ecg)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(svpath,dpi=100) #(500=5*100,300=3*100)\n",
    "    plt.close()\n",
    "    sys.stdout.write('\\r{} / {} '.format(idx,X_DS1.shape[0]))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5144087076187134\n",
      "net.0.net.0.weight\n",
      "tensor([-0.1606, -0.0088, -0.1245], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "841.0537719726562\n",
      "net.0.net.0.weight\n",
      "tensor([-0.1614, -0.0095, -0.1237], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "41.54981994628906\n",
      "net.0.net.0.weight\n",
      "tensor([-0.1620, -0.0103, -0.1232], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "450.1275939941406\n",
      "net.0.net.0.weight\n",
      "tensor([-0.1628, -0.0112, -0.1226], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "445.1622619628906\n",
      "net.0.net.0.weight\n",
      "tensor([-0.1635, -0.0121, -0.1219], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "143.94607543945312\n",
      "net.0.net.0.weight\n",
      "tensor([-0.1644, -0.0130, -0.1210], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "48.680973052978516\n",
      "net.0.net.0.weight\n",
      "tensor([-0.1650, -0.0137, -0.1204], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "179.8726806640625\n",
      "net.0.net.0.weight\n",
      "tensor([-0.1652, -0.0141, -0.1202], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "234.72418212890625\n",
      "net.0.net.0.weight\n",
      "tensor([-0.1651, -0.0145, -0.1202], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "147.39199829101562\n",
      "net.0.net.0.weight\n",
      "tensor([-0.1650, -0.0148, -0.1204], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([[ 2.7477e+00, -2.4632e+00, -2.9840e+00,  3.0890e-01,  3.8006e+00,\n",
      "         -6.1986e-01,  6.5136e+00, -3.3675e+00, -2.4834e+00, -2.1026e+00,\n",
      "         -6.5516e-01,  1.7668e+00],\n",
      "        [ 2.6982e+00, -2.3219e+00, -3.5011e+00,  4.1121e-03,  5.0697e+00,\n",
      "         -4.9248e-01,  6.1669e+00, -2.3918e+00, -2.7758e+00, -2.5263e+00,\n",
      "         -1.7525e+00,  1.4101e+00],\n",
      "        [ 3.1446e+00, -2.1911e+00, -3.5946e+00,  2.9556e-01,  4.9989e+00,\n",
      "         -6.4306e-01,  6.2522e+00, -3.1699e+00, -2.6622e+00, -2.3753e+00,\n",
      "         -1.1795e+00,  1.7088e+00],\n",
      "        [ 2.9295e+00, -2.4793e+00, -3.0817e+00, -2.8429e-01,  4.7660e+00,\n",
      "         -4.7167e-01,  6.2641e+00, -2.8277e+00, -2.6191e+00, -1.9723e+00,\n",
      "         -1.4623e+00,  1.6268e+00],\n",
      "        [ 2.7981e+00, -2.5794e+00, -3.7477e+00,  5.5297e-01,  4.5579e+00,\n",
      "         -9.4613e-01,  5.9963e+00, -2.8378e+00, -1.8327e+00, -1.7878e+00,\n",
      "         -1.1493e+00,  1.9942e+00],\n",
      "        [ 2.7570e+00, -2.5782e+00, -3.8817e+00, -1.2903e-01,  4.5918e+00,\n",
      "         -4.8044e-01,  6.4231e+00, -2.8897e+00, -2.2929e+00, -1.8452e+00,\n",
      "         -9.0826e-01,  1.4463e+00],\n",
      "        [ 2.6674e+00, -2.6190e+00, -3.1760e+00, -1.0635e-01,  5.0479e+00,\n",
      "         -4.8941e-02,  6.7084e+00, -2.8087e+00, -2.4479e+00, -2.4873e+00,\n",
      "         -1.2247e+00,  1.5063e+00],\n",
      "        [ 2.8389e+00, -2.3216e+00, -3.3365e+00, -4.8494e-01,  4.9120e+00,\n",
      "         -1.0657e+00,  6.1194e+00, -2.7211e+00, -2.2662e+00, -2.4279e+00,\n",
      "         -1.4983e+00,  1.2675e+00],\n",
      "        [ 2.6789e+00, -2.6459e+00, -3.2841e+00,  6.6580e-02,  4.7347e+00,\n",
      "         -5.9310e-01,  6.3854e+00, -2.7872e+00, -3.1742e+00, -2.1932e+00,\n",
      "         -1.3579e+00,  1.5637e+00],\n",
      "        [ 2.9753e+00, -2.7093e+00, -3.8842e+00,  3.8895e-01,  4.7386e+00,\n",
      "         -9.3577e-01,  5.6084e+00, -2.6434e+00, -2.4993e+00, -1.9973e+00,\n",
      "         -1.1970e+00,  1.5849e+00]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1.],\n",
      "        [ 1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1.],\n",
      "        [ 1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1.],\n",
      "        [ 1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,  1.],\n",
      "        [ 1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1.],\n",
      "        [ 1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,  1.],\n",
      "        [ 1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,  1.],\n",
      "        [ 1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,  1.],\n",
      "        [ 1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1.],\n",
      "        [ 1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1.]],\n",
      "       device='cuda:0', grad_fn=<SignBackward>)\n",
      "torch.Size([10, 12])\n"
     ]
    }
   ],
   "source": [
    "#test network: valid\n",
    "x1 = torch.rand(10,3,512,512).cuda()\n",
    "x2 = torch.rand(10,3,512,512).cuda()\n",
    "y = torch.FloatTensor([0,1,1,0,1,0,0,0,1,1]).cuda()\n",
    "model = ASHNet(code_size=12).cuda()\n",
    "criterion  = HashLossFunc(margin=0.5).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out1 = model(x1)#out.grad_fn\n",
    "    out2 = model(x2)\n",
    "    loss = criterion(out1,out2,y)\n",
    "    print (loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #observe the variant of model.parameters\n",
    "    for i in model.named_parameters():\n",
    "        print(i[0])\n",
    "        print(i[1][0][0][0])\n",
    "        break\n",
    "#output\n",
    "x3 = torch.rand(10,3,512,512).cuda()\n",
    "out3 = model(x3)\n",
    "print (out3)\n",
    "out3 = torch.sign(out3) #Binarization,[-1,1]->{-1,1}\n",
    "print (out3)\n",
    "print (out3.size())\n",
    "model = model.cpu()\n",
    "criterion = criterion.cpu()\n",
    "loss = loss.cpu()\n",
    "x1= x1.cpu()\n",
    "x2 = x2.cpu()\n",
    "x3 = x3.cpu()\n",
    "y = y.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
