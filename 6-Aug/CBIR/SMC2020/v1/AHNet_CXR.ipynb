{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Develop Env: linux+cuda9+python3+opencv+pytorch\n",
    "2.Dataset: MIMIC-CXR https://mimic-cxr.mit.edu/about/access/\n",
    "        1)Label:No Finding,Enlarged Cardiomediastinum,Cardiomegaly,Airspace Opacity,Lung Lesion,\n",
    "               Edema,Consolidation,Pneumonia,Atelectasis,Pneumothorax,Pleural Effusion,Pleural Other,Fracture,Support Devices\n",
    "               0.0 - negative, 1.0 - positve, -1.0 - uncertain\n",
    "        2)Dataset: Pneumonia(frontal view) 0.0 - negative (3300), 1.0 - positve(3300), -1.0 - uncertain(3300), \n",
    "               9000 for train,1000 fot test\n",
    "3.Performance Metric: \n",
    "  1)MHR(Mean Hit Ratio):  for evaluating the precison of relevance retrieval;\n",
    "  2)MAP(Mean Average Precision): for evaluation the rank of relevance retrieval;\n",
    "  3)MRR(Mean Reciprocal Rank): for evaluation the first hit rank of relevance retrieval;\n",
    "  4)Memory consumption and Retrieval Speed.\n",
    "4.Algorithm: \n",
    "  1)Baseline: HNet\n",
    "  2)Attention: AHNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from scipy.spatial.distance import pdist\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "#import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.set_device(6)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 / 9000 The length of train set is 9000\n",
      "900 / 900 The length of train set is 900\n",
      "Completed buliding index in 5381 seconds\n"
     ]
    }
   ],
   "source": [
    "#1. Read data with List storage Data:[name,type],I:[img],Y[type]\n",
    "root_dir = '/data/fjsdata/physionet/MIMIC-CXR' #the path of images\n",
    "trainset = pd.read_csv(\"/data/fjsdata/physionet/MIMIC-CXR/CBIR_train.csv\" , sep=',')#load dataset\n",
    "testset = pd.read_csv(\"/data/fjsdata/physionet/MIMIC-CXR/CBIR_test.csv\" , sep=',')#load dataset\n",
    "tstart = time.time()\n",
    "#read train image with CV\n",
    "trData, trI, trY = [],[],[]\n",
    "for iname, itype in np.array(trainset).tolist():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, iname)\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (512, 512))#(1024,1024,3)->(512,512,3)\n",
    "        trData.append([iname,itype])\n",
    "        trI.append(img)\n",
    "        trY.append(itype)\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(trData),trainset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(trData))\n",
    "#read test image with CV\n",
    "teData, teI, teY = [],[],[]\n",
    "for iname, itype in np.array(testset).tolist():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, iname)\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (512, 512))#(1024,1024,3)->(512,512,3)\n",
    "        teData.append([iname,itype])\n",
    "        teI.append(img)\n",
    "        teY.append(itype)  \n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(teData),testset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(teData))\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.345176696777344\n",
      "conv1.weight\n",
      "tensor([-0.1740, -0.0418, -0.0278])\n",
      "15.80381965637207\n",
      "conv1.weight\n",
      "tensor([-0.1738, -0.0427, -0.0278])\n",
      "11.602569580078125\n",
      "conv1.weight\n",
      "tensor([-0.1732, -0.0436, -0.0281])\n",
      "7.364318370819092\n",
      "conv1.weight\n",
      "tensor([-0.1725, -0.0439, -0.0286])\n",
      "4.8878936767578125\n",
      "conv1.weight\n",
      "tensor([-0.1717, -0.0434, -0.0289])\n",
      "4.003961563110352\n",
      "conv1.weight\n",
      "tensor([-0.1709, -0.0428, -0.0292])\n",
      "2.91505765914917\n",
      "conv1.weight\n",
      "tensor([-0.1700, -0.0421, -0.0296])\n",
      "1.5746382474899292\n",
      "conv1.weight\n",
      "tensor([-0.1691, -0.0414, -0.0296])\n",
      "0.9257178902626038\n",
      "conv1.weight\n",
      "tensor([-0.1685, -0.0408, -0.0298])\n",
      "0.583267331123352\n",
      "conv1.weight\n",
      "tensor([-0.1678, -0.0404, -0.0297])\n",
      "tensor([[ 0.0644,  0.0225,  0.2352,  0.5156,  0.4740, -0.3895, -0.2621,\n",
      "          0.1878,  0.0210, -0.4247, -0.6259,  0.3071,  0.2720, -0.1768,\n",
      "         -0.0196, -0.1706],\n",
      "        [-0.1895, -0.0439, -0.2177, -0.2184, -0.1011, -0.3916, -0.0003,\n",
      "          0.1034,  0.0160, -0.0010,  0.1018, -0.0418, -0.3395,  0.1362,\n",
      "          0.1145, -0.1648],\n",
      "        [-0.1614,  0.2954,  0.2354, -0.2234, -0.1679, -0.1859, -0.4921,\n",
      "         -0.3704,  0.2812,  0.2474,  0.1399, -0.1982,  0.1976,  0.1559,\n",
      "          0.5307, -0.0958],\n",
      "        [-0.2879,  0.0896,  0.1030,  0.2533,  0.1425, -0.1947, -0.5889,\n",
      "         -0.0770, -0.0058,  0.3648,  0.1174, -0.1029,  0.3790,  0.0846,\n",
      "          0.3705,  0.1825],\n",
      "        [ 0.0935, -0.1038, -0.2705,  0.1358,  0.3160,  0.1017, -0.0231,\n",
      "          0.2655,  0.0079, -0.0950, -0.2495,  0.1403,  0.0224,  0.0603,\n",
      "         -0.1827,  0.1015],\n",
      "        [-0.0799,  0.1794,  0.2093, -0.0770,  0.3622, -0.1856, -0.2798,\n",
      "          0.1518, -0.0029, -0.1397,  0.0151,  0.3471,  0.3830, -0.1373,\n",
      "          0.0678, -0.1948],\n",
      "        [-0.1982,  0.0863,  0.3237,  0.1572, -0.0676,  0.0135, -0.2318,\n",
      "          0.0152, -0.0542,  0.1316,  0.0481, -0.0102,  0.2275, -0.0097,\n",
      "          0.0395,  0.0532],\n",
      "        [ 0.1869, -0.2660, -0.2963,  0.3252,  0.4521, -0.2574,  0.2074,\n",
      "         -0.1080,  0.0013, -0.4938, -0.5305,  0.2344, -0.1463, -0.1740,\n",
      "         -0.2848, -0.0801],\n",
      "        [ 0.1369, -0.3003, -0.5557,  0.0756,  0.1132, -0.1705, -0.1706,\n",
      "         -0.1357, -0.3055, -0.0807,  0.1380, -0.4401, -0.1030,  0.4499,\n",
      "          0.0442,  0.1144],\n",
      "        [ 0.1197,  0.2641,  0.1030, -0.1640,  0.1824, -0.4164, -0.3383,\n",
      "         -0.0682, -0.2098,  0.1510, -0.2971,  0.0395,  0.0339,  0.2151,\n",
      "          0.2042, -0.2155]])\n",
      "tensor([[ 1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
      "          1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,\n",
      "         -1.,  1.,  1., -1.],\n",
      "        [-1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
      "          1.,  1.,  1., -1.],\n",
      "        [-1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,  1., -1.,\n",
      "          1.,  1.,  1.,  1.],\n",
      "        [ 1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
      "          1.,  1., -1.,  1.],\n",
      "        [-1.,  1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,\n",
      "          1., -1.,  1., -1.],\n",
      "        [-1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,\n",
      "          1., -1.,  1.,  1.],\n",
      "        [ 1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
      "         -1., -1., -1., -1.],\n",
      "        [ 1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1.,  1., -1.,\n",
      "         -1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,\n",
      "          1.,  1.,  1., -1.]])\n",
      "torch.Size([10, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. define Attention-based Hashing network with pytorch\n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class AHNet(nn.Module): #deep Hashint Network:DHNet\n",
    "    def __init__(self,inChannels=3):\n",
    "        super(AHNet, self).__init__()\n",
    "        #(channels, Height, Width)\n",
    "        #layer1: Convolution, (3,512,512)->(8,256,256)\n",
    "        self.conv1 = nn.Conv2d(in_channels=inChannels, out_channels=8, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        #layer2: max pooling,(8,256,256)->(8,128,128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        #layer3: Spatial Attention Layer, (8,256,256)->(8,256,256)\n",
    "        self.sa = SpatialAttention()\n",
    "        #layer4: Convolution, (8,128,128)->(2,64,64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=2, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #layer5: mean pooling, (2,64,64)->(2,32,32)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(2)\n",
    "        #layer6: fully connected, 2*32*32->512\n",
    "        self.fcl1 = nn.Linear(2*32*32,512)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        #layer7: Hashing layer, 512->16\n",
    "        self.fcl2 = nn.Linear(512,16)#\n",
    "        self.tanh = nn.Tanh() #{-1,1}\n",
    "              \n",
    "    def forward(self,x):\n",
    "        #input: (batch_size, in_channels, Height, Width)\n",
    "        #output: (batch_size, out_channels, Height, Width)\n",
    "        #layer1: convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        #layer2: max pooling\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        #layer3: Attention\n",
    "        x = self.sa(x)*x\n",
    "        #layer4: Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu2(x)\n",
    "        #layer5: mean pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = self.bn4(x)\n",
    "        #layer6:fully connected\n",
    "        x = x.view(x.size(0),-1) #transfer three dims to one dim\n",
    "        x = self.fcl1(x)\n",
    "        x = self.relu3(x)\n",
    "        #layer7: Hashing layer\n",
    "        x = self.fcl2(x)\n",
    "        x = self.tanh(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "#https://pytorch-cn.readthedocs.io/zh/latest/    \n",
    "#https://github.com/filipradenovic/cnnimageretrieval-pytorch/blob/master/cirtorch/layers/functional.py\n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "    \n",
    "    def forward(self,h1,h2,y): \n",
    "        #h1=h2:NxD,y:N\n",
    "        dim = h1.shape[1]\n",
    "        euc_dist = F.pairwise_distance(h1, h2, p=2, eps=1e-06) # Calcualte Euclidean Distance\n",
    "        sim_term = 0.5*(1-y)*euc_dist #penalize the similar iamge pairs when y=0\n",
    "        unsim_term = 0.5*y*torch.clamp(self.margin*dim-euc_dist,0)#penalize the unsimlar image pairs when y =1\n",
    "        reg_term = self.alpha * ( torch.sum((torch.abs(h1)-1),dim=1) + torch.sum((torch.abs(h2)-1),dim=1) ) #regularization term\n",
    "        #loss = torch.mean(sim_term + unsim_term + reg_term) \n",
    "        loss = torch.sum(sim_term + unsim_term+ reg_term) \n",
    "        return loss\n",
    "\n",
    "#test network: valid\n",
    "x1 = torch.rand(10,3,512,512)#.cuda()\n",
    "x2 = torch.rand(10,3,512,512)#.cuda()\n",
    "y = torch.FloatTensor([0,1,1,0,1,0,0,0,1,1])#.cuda()\n",
    "model = AHNet()#.cuda()\n",
    "criterion  = HashLossFunc(margin=0.5)#.cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out1 = model(x1)#out.grad_fn\n",
    "    out2 = model(x2)\n",
    "    loss = criterion(out1,out2,y)\n",
    "    print (loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #observe the variant of model.parameters\n",
    "    for i in model.named_parameters():\n",
    "        print(i[0])\n",
    "        print(i[1][0][0][0])\n",
    "        break\n",
    "#output\n",
    "x3 = torch.rand(10,3,512,512)#.cuda()\n",
    "out3 = model(x3)\n",
    "print (out3)\n",
    "out3 = torch.sign(out3) #Binarization,[-1,1]->{-1,1}\n",
    "print (out3)\n",
    "print (out3.size())\n",
    "del x1,x2,x3,out1,out2,out3,model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 900 / 900 : loss = 15.139267Eopch:     1 mean_loss = 18.710916\n",
      " 900 / 900 : loss = 10.103222Eopch:     2 mean_loss = 18.803743\n",
      "best_loss = 18.710916\n",
      " 89 / 90 0 mHR@5=0.342222, mAP@5=0.326693, mRR@5=0.899352\n",
      "mHR@10=0.347222, mAP@10=0.318613, mRR@10=0.791531\n",
      "mHR@15=0.350889, mAP@15=0.312002, mRR@15=0.720993\n",
      "mHR@20=0.354056, mAP@20=0.306752, mRR@20=0.684521\n"
     ]
    }
   ],
   "source": [
    "#3.train and evaluate model\n",
    "def onlineGenImgPairs(batchSize):\n",
    "    idx_sf = random.sample(range(0, len(trY)),2*batchSize)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "        \n",
    "#define model\n",
    "model = AHNet().cuda()\n",
    "criterion  = HashLossFunc(margin=0.5).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "for epoch in range(2):#iteration: len(trY)/2\n",
    "    batchSize = 10\n",
    "    batches = len(trY)//batchSize\n",
    "    losses = []\n",
    "    for batch in range(batches):\n",
    "        #grad vanish\n",
    "        optimizer.zero_grad() \n",
    "        #genenate training images pair\n",
    "        trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs(batchSize)\n",
    "        I1_batch = torch.from_numpy(trI1_sf).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(batch+1, batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(best_net(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(best_net(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#train data with list: trData, trI, trF, trY\n",
    "#test data with list: teData, teI, teF, teY\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        map_item_score = {}\n",
    "        for j, trVal in enumerate(trF):\n",
    "            map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in ranklist:\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
