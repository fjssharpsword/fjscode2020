{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Develop Env: linux+cuda9+python3+opencv+pytorch\n",
    "2.Dataset: ASOCT-Cataract with 32560 images, Dataset Statistics: \n",
    "        1)OD:oculus dextrus, OS: oculus sinister\n",
    "           OD    18901\n",
    "           OS    13659\n",
    "        2)Structure: N, C, P train set: (29297, 6) test set: (3255, 6)\n",
    "           N-Level: 1.0-6.0 \n",
    "           C-Level: 1.0-5.0 if C=0.0(562) denotes this sample with no label \n",
    "           P-Level: 1.0-6.0 if C=0.0(7354) denotes this sample with no label\n",
    "        3)train set and test set:  OD, P-Level(Micro ROI) \n",
    "           1.0(2381),2.0(1407),3.0(1677),4.0(1494),5.0(781)-7000 for train ,700 for test     \n",
    "3.Performance Metric: \n",
    "  1)MHR(Mean Hit Ratio):  for evaluating the precison of relevance retrieval;\n",
    "  2)MAP(Mean Average Precision): for evaluation the rank of relevance retrieval;\n",
    "  3)MRR(Mean Reciprocal Rank): for evaluation the first hit rank of relevance retrieval;\n",
    "  4)Memory consumption and Retrieval Speed.\n",
    "4.Algorithm: \n",
    "  1)Baseline: LSH,SDH,DSH,DRH\n",
    "  2)Attention: ASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from scipy.spatial.distance import pdist\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "#import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.set_device(6)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468 / 7000 C020_20180514_100234_R_CASIA2_LGC_004.jpg:/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New/C020_20180514_100234_R_CASIA2_LGC_004.jpg\n",
      "568 / 7000 C020_20180514_100234_R_CASIA2_LGC_000.jpg:/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New/C020_20180514_100234_R_CASIA2_LGC_000.jpg\n",
      "1871 / 7000 C020_20180514_100234_R_CASIA2_LGC_002.jpg:/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New/C020_20180514_100234_R_CASIA2_LGC_002.jpg\n",
      "1929 / 7000 C020_20180514_100234_R_CASIA2_LGC_008.jpg:/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New/C020_20180514_100234_R_CASIA2_LGC_008.jpg\n",
      "6996 / 7000 The length of train set is 6996\n",
      "700 / 700 The length of train set is 700\n"
     ]
    }
   ],
   "source": [
    "#Read data with List storage N:[name],I:[img],Y[type]\n",
    "image_dir = '/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New' #the path of images\n",
    "trainset = pd.read_csv(\"/data/fjsdata/ASOCT/Cataract/CBIR_MICCAI_train.csv\" , sep=',')#load dataset\n",
    "testset = pd.read_csv(\"/data/fjsdata/ASOCT/Cataract/CBIR_MICCAI_test.csv\" , sep=',')#load testset\n",
    "\n",
    "#read train image with CV\n",
    "trN, trI, trY = [],[],[]\n",
    "for iname, itype in np.array(trainset).tolist():#column: name,id,lr,N,C,P  \n",
    "    if iname.endswith(\".jpg\"):\n",
    "        try:\n",
    "            image_path = os.path.join(image_dir, iname)\n",
    "            img = cv2.resize(cv2.imread(image_path).astype(np.float32), (512, 512))\n",
    "            trN.append(iname)\n",
    "            trI.append(img)\n",
    "            trY.append(itype)\n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trN),len(trainset)))\n",
    "        sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(trN))\n",
    "#read test image with CV\n",
    "teN, teI, teY = [],[],[]\n",
    "for iname, itype in np.array(testset).tolist():#column: name,id,lr,N,C,P  \n",
    "    if iname.endswith(\".jpg\"):\n",
    "        try:\n",
    "            image_path = os.path.join(image_dir, iname)\n",
    "            img = cv2.resize(cv2.imread(image_path).astype(np.float32), (512, 512))\n",
    "            teN.append(iname)\n",
    "            teI.append(img)\n",
    "            teY.append(itype)\n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(teN),len(testset)))\n",
    "        sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(teN))\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs(spls=len(trY)):\n",
    "    idx_sf = random.sample(range(0, len(trY)),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#2 Hashing Network:HNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 349 / 349 : loss = 10.352543Eopch:     1 mean_loss = 17.459815\n",
      " 349 / 349 : loss = 11.689684Eopch:     2 mean_loss = 15.949930\n",
      " 349 / 349 : loss = 12.925333Eopch:     3 mean_loss = 15.708146\n",
      " 349 / 349 : loss = 10.979179Eopch:     4 mean_loss = 15.474466\n",
      " 349 / 349 : loss = 14.879833Eopch:     5 mean_loss = 17.029373\n",
      " 349 / 349 : loss = 13.217655Eopch:     6 mean_loss = 16.392042\n",
      " 349 / 349 : loss = 13.484409Eopch:     7 mean_loss = 16.394535\n",
      " 349 / 349 : loss = 13.126215Eopch:     8 mean_loss = 16.015646\n",
      " 349 / 349 : loss = 14.028273Eopch:     9 mean_loss = 15.937943\n",
      " 349 / 349 : loss = 14.788277Eopch:    10 mean_loss = 16.190952\n",
      " 349 / 349 : loss = 12.163811Eopch:    11 mean_loss = 15.897008\n",
      " 349 / 349 : loss = 13.881802Eopch:    12 mean_loss = 15.750173\n",
      " 349 / 349 : loss = 13.007825Eopch:    13 mean_loss = 15.485516\n",
      " 349 / 349 : loss = 14.352207Eopch:    14 mean_loss = 15.457333\n",
      " 349 / 349 : loss = 12.224516Eopch:    15 mean_loss = 15.417593\n",
      " 349 / 349 : loss = 10.889629Eopch:    16 mean_loss = 15.486691\n",
      " 349 / 349 : loss = 11.530146Eopch:    17 mean_loss = 15.421472\n",
      " 349 / 349 : loss = 12.324202Eopch:    18 mean_loss = 15.185121\n",
      " 349 / 349 : loss = 9.7132236Eopch:    19 mean_loss = 15.231728\n",
      " 349 / 349 : loss = 12.835232Eopch:    20 mean_loss = 15.224627\n",
      " 349 / 349 : loss = 12.276155Eopch:    21 mean_loss = 15.173637\n",
      " 349 / 349 : loss = 10.426976Eopch:    22 mean_loss = 15.338917\n",
      " 349 / 349 : loss = 9.7451817Eopch:    23 mean_loss = 15.363057\n",
      " 349 / 349 : loss = 12.109936Eopch:    24 mean_loss = 15.346857\n",
      " 349 / 349 : loss = 13.217957Eopch:    25 mean_loss = 15.303049\n",
      " 349 / 349 : loss = 11.724422Eopch:    26 mean_loss = 14.962675\n",
      " 349 / 349 : loss = 9.3066046Eopch:    27 mean_loss = 14.885656\n",
      " 349 / 349 : loss = 11.216665Eopch:    28 mean_loss = 15.056973\n",
      " 349 / 349 : loss = 12.147966Eopch:    29 mean_loss = 14.978217\n",
      " 349 / 349 : loss = 12.371903Eopch:    30 mean_loss = 15.022482\n",
      " 349 / 349 : loss = 14.693529Eopch:    31 mean_loss = 15.389026\n",
      " 349 / 349 : loss = 14.170221Eopch:    32 mean_loss = 15.039288\n",
      " 349 / 349 : loss = 15.577777Eopch:    33 mean_loss = 15.069284\n",
      " 349 / 349 : loss = 13.364669Eopch:    34 mean_loss = 14.727366\n",
      " 349 / 349 : loss = 12.312309Eopch:    35 mean_loss = 14.854001\n",
      " 349 / 349 : loss = 16.989893Eopch:    36 mean_loss = 14.899165\n",
      " 349 / 349 : loss = 14.564793Eopch:    37 mean_loss = 14.842075\n",
      " 349 / 349 : loss = 12.423725Eopch:    38 mean_loss = 16.133223\n",
      " 349 / 349 : loss = 14.303645Eopch:    39 mean_loss = 17.268630\n",
      " 349 / 349 : loss = 13.852951Eopch:    40 mean_loss = 17.419691\n",
      " 349 / 349 : loss = 12.997976Eopch:    41 mean_loss = 17.141997\n",
      " 349 / 349 : loss = 13.363164Eopch:    42 mean_loss = 17.432695\n",
      " 349 / 349 : loss = 11.807936Eopch:    43 mean_loss = 16.466353\n",
      " 349 / 349 : loss = 13.248626Eopch:    44 mean_loss = 15.978382\n",
      " 349 / 349 : loss = 12.931556Eopch:    45 mean_loss = 16.002917\n",
      " 349 / 349 : loss = 13.188344Eopch:    46 mean_loss = 15.715394\n",
      " 349 / 349 : loss = 11.413185Eopch:    47 mean_loss = 15.998753\n",
      " 349 / 349 : loss = 11.669973Eopch:    48 mean_loss = 16.278824\n",
      " 349 / 349 : loss = 11.369057Eopch:    49 mean_loss = 16.117638\n",
      " 349 / 349 : loss = 9.9748952Eopch:    50 mean_loss = 15.962419\n",
      " 349 / 349 : loss = 10.914809Eopch:    51 mean_loss = 15.985473\n",
      " 349 / 349 : loss = 13.768268Eopch:    52 mean_loss = 15.904125\n",
      " 349 / 349 : loss = 11.634174Eopch:    53 mean_loss = 15.849167\n",
      " 349 / 349 : loss = 13.753761Eopch:    54 mean_loss = 16.410380\n",
      " 349 / 349 : loss = 12.576773Eopch:    55 mean_loss = 16.174734\n",
      " 349 / 349 : loss = 14.049293Eopch:    56 mean_loss = 15.774748\n",
      " 349 / 349 : loss = 13.475814Eopch:    57 mean_loss = 16.141327\n",
      " 349 / 349 : loss = 13.411755Eopch:    58 mean_loss = 15.961026\n",
      " 349 / 349 : loss = 9.3706839Eopch:    59 mean_loss = 16.142864\n",
      " 349 / 349 : loss = 10.774524Eopch:    60 mean_loss = 16.395982\n",
      " 349 / 349 : loss = 10.613619Eopch:    61 mean_loss = 16.131624\n",
      " 349 / 349 : loss = 8.8372183Eopch:    62 mean_loss = 15.917719\n",
      " 349 / 349 : loss = 9.0131154Eopch:    63 mean_loss = 15.630997\n",
      " 349 / 349 : loss = 11.819673Eopch:    64 mean_loss = 15.532241\n",
      " 349 / 349 : loss = 10.767186Eopch:    65 mean_loss = 15.673725\n",
      " 349 / 349 : loss = 10.437268Eopch:    66 mean_loss = 15.638175\n",
      " 349 / 349 : loss = 10.106672Eopch:    67 mean_loss = 15.628251\n",
      " 349 / 349 : loss = 11.912158Eopch:    68 mean_loss = 15.773383\n",
      " 349 / 349 : loss = 11.095767Eopch:    69 mean_loss = 15.714611\n",
      " 349 / 349 : loss = 8.4551661Eopch:    70 mean_loss = 15.748722\n",
      " 349 / 349 : loss = 10.853425Eopch:    71 mean_loss = 15.669985\n",
      " 349 / 349 : loss = 12.327664Eopch:    72 mean_loss = 15.850510\n",
      " 349 / 349 : loss = 13.078231Eopch:    73 mean_loss = 15.808232\n",
      " 349 / 349 : loss = 13.554711Eopch:    74 mean_loss = 15.989373\n",
      " 349 / 349 : loss = 13.274442Eopch:    75 mean_loss = 16.178855\n",
      " 349 / 349 : loss = 10.836415Eopch:    76 mean_loss = 16.337224\n",
      " 349 / 349 : loss = 12.390326Eopch:    77 mean_loss = 16.014329\n",
      " 349 / 349 : loss = 11.670744Eopch:    78 mean_loss = 16.094451\n",
      " 349 / 349 : loss = 9.6574232Eopch:    79 mean_loss = 16.172061\n",
      " 349 / 349 : loss = 11.016516Eopch:    80 mean_loss = 17.085859\n",
      " 349 / 349 : loss = 9.4195113Eopch:    81 mean_loss = 16.234516\n",
      " 349 / 349 : loss = 10.516596Eopch:    82 mean_loss = 15.806322\n",
      " 349 / 349 : loss = 11.328851Eopch:    83 mean_loss = 15.885179\n",
      " 349 / 349 : loss = 11.402993Eopch:    84 mean_loss = 15.880050\n",
      " 349 / 349 : loss = 10.231133Eopch:    85 mean_loss = 15.715698\n",
      " 349 / 349 : loss = 11.043303Eopch:    86 mean_loss = 15.793786\n",
      " 349 / 349 : loss = 10.779408Eopch:    87 mean_loss = 15.689545\n",
      " 349 / 349 : loss = 10.025241Eopch:    88 mean_loss = 15.630309\n",
      " 349 / 349 : loss = 9.1172395Eopch:    89 mean_loss = 15.572547\n",
      " 349 / 349 : loss = 11.522352Eopch:    90 mean_loss = 15.599307\n",
      " 349 / 349 : loss = 12.259581Eopch:    91 mean_loss = 15.731643\n",
      " 349 / 349 : loss = 10.985656Eopch:    92 mean_loss = 16.014049\n",
      " 349 / 349 : loss = 10.227154Eopch:    93 mean_loss = 15.786335\n",
      " 349 / 349 : loss = 10.936411Eopch:    94 mean_loss = 15.741859\n",
      " 349 / 349 : loss = 10.152157Eopch:    95 mean_loss = 15.880438\n",
      " 349 / 349 : loss = 12.190251Eopch:    96 mean_loss = 16.122343\n",
      " 349 / 349 : loss = 11.947489Eopch:    97 mean_loss = 15.699612\n",
      " 349 / 349 : loss = 9.3803674Eopch:    98 mean_loss = 15.506856\n",
      " 349 / 349 : loss = 10.733402Eopch:    99 mean_loss = 15.307933\n",
      " 349 / 349 : loss = 10.528222Eopch:   100 mean_loss = 15.345610\n",
      " 69 / 70 0 mHR@5=0.346571, mAP@5=0.306290, mRR@5=0.784513\n",
      "mHR@10=0.343286, mAP@10=0.279631, mRR@10=0.635309\n",
      "mHR@15=0.348286, mAP@15=0.263594, mRR@15=0.572960\n",
      "mHR@20=0.340929, mAP@20=0.248711, mRR@20=0.547012\n"
     ]
    }
   ],
   "source": [
    "#define Hashing network with pytorch\n",
    "class HNet(nn.Module): \n",
    "    def __init__(self,inChannels=3, outHashcode=16):\n",
    "        super(HNet, self).__init__()\n",
    "        #(channels, Height, Width)\n",
    "        #layer1: Convolution, (3,512,512)->(8,256,256)\n",
    "        self.conv1 = nn.Conv2d(in_channels=inChannels, out_channels=8, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        #layer2: max pooling,(8,256,256)->(8,128,128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        #layer: spatial attention, pass\n",
    "        #layer3: Convolution, (8,128,128)->(2,64,64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=2, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #layer4: mean pooling, (2,64,64)->(2,32,32)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(2)\n",
    "        #layer5: fully connected, 2*32*32->512\n",
    "        self.fcl1 = nn.Linear(2*32*32,512)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        #layer6: Hashing layer, 512->16\n",
    "        self.fcl2 = nn.Linear(512,outHashcode)#\n",
    "        self.tanh = nn.Tanh() #{-1,1}\n",
    "              \n",
    "    def forward(self,x):\n",
    "        #input: (batch_size, in_channels, Height, Width)\n",
    "        #output: (batch_size, out_channels, Height, Width)\n",
    "        #layer1: convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        #layer2: max pooling\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        #layer: attention,pass\n",
    "        #layer3: Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu2(x)\n",
    "        #layer4: mean pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = self.bn4(x)\n",
    "        #layer5:fully connected\n",
    "        x = x.view(x.size(0),-1) #transfer three dims to one dim\n",
    "        x = self.fcl1(x)\n",
    "        x = self.relu3(x)\n",
    "        #layer6: Hashing layer\n",
    "        x = self.fcl2(x)\n",
    "        x = self.tanh(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "#https://pytorch-cn.readthedocs.io/zh/latest/    \n",
    "#https://github.com/filipradenovic/cnnimageretrieval-pytorch/blob/master/cirtorch/layers/functional.py\n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "    \n",
    "    def forward(self,h1,h2,y): \n",
    "        #h1=h2:NxD,y:N\n",
    "        dim = h1.shape[1]\n",
    "        euc_dist = F.pairwise_distance(h1, h2, p=2, eps=1e-06) # Calcualte Euclidean Distance\n",
    "        sim_term = 0.5*(1-y)*euc_dist #penalize the similar iamge pairs when y=0\n",
    "        unsim_term = 0.5*y*torch.clamp(self.margin*dim-euc_dist,0)#penalize the unsimlar image pairs when y =1\n",
    "        reg_term = self.alpha * ( torch.sum((torch.abs(h1)-1),dim=1) + torch.sum((torch.abs(h2)-1),dim=1) ) #regularization term\n",
    "        #loss = torch.mean(sim_term + unsim_term + reg_term) \n",
    "        loss = torch.sum(sim_term + unsim_term+ reg_term) \n",
    "        return loss\n",
    "\n",
    "#define model\n",
    "model = HNet(outHashcode=32).cuda()\n",
    "criterion  = HashLossFunc(margin=0.3).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "#best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "num_batches = len(trY_sf) // batchSize\n",
    "for epoch in range(100):#iteration\n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad() #grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    #if np.mean(losses) < best_loss:\n",
    "    #    best_loss = np.mean(losses)\n",
    "    #    best_net = copy.deepcopy(model)\n",
    "#print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "#model = model.cpu()\n",
    "#torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(model(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(model(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#train data with list: trData, trI, trF, trY\n",
    "#test data with list: teData, teI, teF, teY\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        map_item_score = {}\n",
    "        for j, trVal in enumerate(trF):\n",
    "            map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in ranklist:\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#3 Attention-based Hashing Network:AHNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 349 / 349 : loss = 13.414903Eopch:     1 mean_loss = 17.435256\n",
      " 349 / 349 : loss = 12.970074Eopch:     2 mean_loss = 16.205311\n",
      " 349 / 349 : loss = 14.306896Eopch:     3 mean_loss = 16.042361\n",
      " 349 / 349 : loss = 11.092731Eopch:     4 mean_loss = 15.816894\n",
      " 349 / 349 : loss = 13.495705Eopch:     5 mean_loss = 15.547955\n",
      " 349 / 349 : loss = 13.273634Eopch:     6 mean_loss = 15.613994\n",
      " 349 / 349 : loss = 13.100557Eopch:     7 mean_loss = 15.243671\n",
      " 349 / 349 : loss = 14.732826Eopch:     8 mean_loss = 15.319182\n",
      " 349 / 349 : loss = 12.596115Eopch:     9 mean_loss = 15.325578\n",
      " 349 / 349 : loss = 14.584191Eopch:    10 mean_loss = 15.366641\n",
      " 349 / 349 : loss = 11.804696Eopch:    11 mean_loss = 15.494835\n",
      " 349 / 349 : loss = 12.335512Eopch:    12 mean_loss = 15.201933\n",
      " 349 / 349 : loss = 13.149352Eopch:    13 mean_loss = 15.088060\n",
      " 349 / 349 : loss = 11.298193Eopch:    14 mean_loss = 15.230922\n",
      " 349 / 349 : loss = 9.7115844Eopch:    15 mean_loss = 15.141446\n",
      " 349 / 349 : loss = 8.9976999Eopch:    16 mean_loss = 14.711649\n",
      " 349 / 349 : loss = 12.504793Eopch:    17 mean_loss = 14.941945\n",
      " 349 / 349 : loss = 10.390207Eopch:    18 mean_loss = 15.110613\n",
      " 349 / 349 : loss = 12.049111Eopch:    19 mean_loss = 15.080190\n",
      " 349 / 349 : loss = 11.990431Eopch:    21 mean_loss = 14.911280\n",
      " 349 / 349 : loss = 11.358884Eopch:    22 mean_loss = 14.997626\n",
      " 349 / 349 : loss = 8.8607894Eopch:    23 mean_loss = 14.787117\n",
      " 349 / 349 : loss = 9.3486167Eopch:    24 mean_loss = 14.716857\n",
      " 349 / 349 : loss = 9.5117077Eopch:    25 mean_loss = 14.912884\n",
      " 349 / 349 : loss = 10.074301Eopch:    26 mean_loss = 14.719017\n",
      " 349 / 349 : loss = 10.203182Eopch:    27 mean_loss = 14.395817\n",
      " 349 / 349 : loss = 11.478764Eopch:    28 mean_loss = 14.423407\n",
      " 349 / 349 : loss = 9.9855755Eopch:    29 mean_loss = 14.427378\n",
      " 349 / 349 : loss = 9.8508927Eopch:    30 mean_loss = 14.558859\n",
      " 349 / 349 : loss = 8.9406096Eopch:    31 mean_loss = 14.436439\n",
      " 349 / 349 : loss = 10.326749Eopch:    32 mean_loss = 14.269015\n",
      " 349 / 349 : loss = 10.017754Eopch:    33 mean_loss = 14.352403\n",
      " 349 / 349 : loss = 9.2441258Eopch:    34 mean_loss = 14.091548\n",
      " 349 / 349 : loss = 9.7722427Eopch:    35 mean_loss = 14.145138\n",
      " 349 / 349 : loss = 8.3790897Eopch:    36 mean_loss = 14.236136\n",
      " 349 / 349 : loss = 10.220033Eopch:    37 mean_loss = 14.204275\n",
      " 349 / 349 : loss = 9.8358871Eopch:    38 mean_loss = 14.285035\n",
      " 349 / 349 : loss = 7.3789827Eopch:    39 mean_loss = 14.159572\n",
      " 349 / 349 : loss = 9.9223198Eopch:    40 mean_loss = 14.191755\n",
      " 349 / 349 : loss = 8.2761936Eopch:    41 mean_loss = 14.450681\n",
      " 349 / 349 : loss = 11.701315Eopch:    42 mean_loss = 14.119658\n",
      " 349 / 349 : loss = 9.5530852Eopch:    43 mean_loss = 14.399163\n",
      " 349 / 349 : loss = 9.4857584Eopch:    44 mean_loss = 14.409539\n",
      " 349 / 349 : loss = 8.7969895Eopch:    45 mean_loss = 14.030305\n",
      " 349 / 349 : loss = 11.401803Eopch:    46 mean_loss = 14.196004\n",
      " 349 / 349 : loss = 12.260761Eopch:    47 mean_loss = 14.246885\n",
      " 349 / 349 : loss = 12.245274Eopch:    48 mean_loss = 14.202043\n",
      " 349 / 349 : loss = 11.645626Eopch:    49 mean_loss = 14.199459\n",
      " 349 / 349 : loss = 11.272234Eopch:    50 mean_loss = 14.261739\n",
      " 349 / 349 : loss = 13.008577Eopch:    51 mean_loss = 14.241810\n",
      " 349 / 349 : loss = 9.1696417Eopch:    52 mean_loss = 13.874202\n",
      " 349 / 349 : loss = 8.2832544Eopch:    53 mean_loss = 14.136711\n",
      " 349 / 349 : loss = 6.6498883Eopch:    54 mean_loss = 14.209080\n",
      " 349 / 349 : loss = 12.784505Eopch:    55 mean_loss = 13.903748\n",
      " 349 / 349 : loss = 9.0044623Eopch:    56 mean_loss = 13.752699\n",
      " 349 / 349 : loss = 9.2831337Eopch:    57 mean_loss = 13.812201\n",
      " 349 / 349 : loss = 8.0986734Eopch:    58 mean_loss = 13.842755\n",
      " 349 / 349 : loss = 9.4542733Eopch:    59 mean_loss = 13.751477\n",
      " 349 / 349 : loss = 10.636383Eopch:    60 mean_loss = 14.047069\n",
      " 349 / 349 : loss = 9.0179216Eopch:    61 mean_loss = 13.704946\n",
      " 349 / 349 : loss = 9.7662575Eopch:    62 mean_loss = 13.685416\n",
      " 349 / 349 : loss = 9.1877839Eopch:    63 mean_loss = 13.369323\n",
      " 349 / 349 : loss = 9.5366135Eopch:    64 mean_loss = 13.631351\n",
      " 349 / 349 : loss = 10.493475Eopch:    65 mean_loss = 13.504852\n",
      " 349 / 349 : loss = 8.8964525Eopch:    66 mean_loss = 13.594296\n",
      " 349 / 349 : loss = 8.3122254Eopch:    67 mean_loss = 13.666080\n",
      " 349 / 349 : loss = 8.6054847Eopch:    68 mean_loss = 13.770021\n",
      " 349 / 349 : loss = 9.2847689Eopch:    69 mean_loss = 13.887738\n",
      " 349 / 349 : loss = 8.8839449Eopch:    70 mean_loss = 13.529687\n",
      " 349 / 349 : loss = 7.9351111Eopch:    71 mean_loss = 13.427337\n",
      " 349 / 349 : loss = 13.638434Eopch:    72 mean_loss = 13.344924\n",
      " 349 / 349 : loss = 10.337234Eopch:    73 mean_loss = 13.491907\n",
      " 349 / 349 : loss = 10.953141Eopch:    74 mean_loss = 13.279181\n",
      " 349 / 349 : loss = 7.8970557Eopch:    75 mean_loss = 13.124108\n",
      " 349 / 349 : loss = 11.420253Eopch:    76 mean_loss = 12.952863\n",
      " 349 / 349 : loss = 11.128489Eopch:    77 mean_loss = 12.883785\n",
      " 349 / 349 : loss = 10.958584Eopch:    78 mean_loss = 12.974801\n",
      " 349 / 349 : loss = 10.500197Eopch:    79 mean_loss = 12.886568\n",
      " 349 / 349 : loss = 11.463032Eopch:    80 mean_loss = 12.586093\n",
      " 349 / 349 : loss = 8.8842391Eopch:    81 mean_loss = 12.613346\n",
      " 349 / 349 : loss = 9.3604224Eopch:    82 mean_loss = 12.699504\n",
      " 349 / 349 : loss = 8.1716184Eopch:    83 mean_loss = 12.463477\n",
      " 349 / 349 : loss = 10.157264Eopch:    84 mean_loss = 12.405470\n",
      " 349 / 349 : loss = 10.463215Eopch:    85 mean_loss = 12.251293\n",
      " 349 / 349 : loss = 8.7768961Eopch:    86 mean_loss = 12.138339\n",
      " 349 / 349 : loss = 9.2309727Eopch:    87 mean_loss = 12.165197\n",
      " 349 / 349 : loss = 7.2775049Eopch:    88 mean_loss = 11.865987\n",
      " 349 / 349 : loss = 9.7935242Eopch:    89 mean_loss = 11.759019\n",
      " 349 / 349 : loss = 10.166218Eopch:    90 mean_loss = 12.015638\n",
      " 349 / 349 : loss = 5.9649398Eopch:    91 mean_loss = 11.750155\n",
      " 349 / 349 : loss = 5.5244182Eopch:    92 mean_loss = 11.532699\n",
      " 349 / 349 : loss = 7.5096419Eopch:    93 mean_loss = 11.367731\n",
      " 349 / 349 : loss = 5.8440498Eopch:    94 mean_loss = 11.213991\n",
      " 349 / 349 : loss = 9.3366272Eopch:    95 mean_loss = 11.421675\n",
      " 349 / 349 : loss = 3.8735911Eopch:    96 mean_loss = 10.906111\n",
      " 349 / 349 : loss = 8.0037617Eopch:    97 mean_loss = 10.816038\n",
      " 349 / 349 : loss = 5.6381914Eopch:    98 mean_loss = 10.674111\n",
      " 349 / 349 : loss = 4.5965251Eopch:    99 mean_loss = 10.583064\n",
      " 349 / 349 : loss = 4.4796557Eopch:   100 mean_loss = 10.287692\n",
      " 69 / 70 0 mHR@5=0.362571, mAP@5=0.322657, mRR@5=0.811370\n",
      "mHR@10=0.361714, mAP@10=0.305498, mRR@10=0.727949\n",
      "mHR@15=0.354000, mAP@15=0.291184, mRR@15=0.686901\n",
      "mHR@20=0.347571, mAP@20=0.279270, mRR@20=0.648097\n"
     ]
    }
   ],
   "source": [
    "#define Attention-based Hashing network with pytorch\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_channels=in_planes, out_channels=in_planes // 2, kernel_size=1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_channels=in_planes // 2, out_channels=in_planes, kernel_size=1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class AHNet(nn.Module): #deep Hashint Network:DHNet\n",
    "    def __init__(self,inChannels=3,outHashcode=16):\n",
    "        super(AHNet, self).__init__()\n",
    "        #(channels, Height, Width)\n",
    "        #layer1: Convolution, (3,512,512)->(8,256,256)\n",
    "        self.conv1 = nn.Conv2d(in_channels=inChannels, out_channels=8, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        #layer2: max pooling,(8,256,256)->(8,128,128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        #layer3: Channel and Spatial Attention Layer, (8,256,256)->(8,256,256)\n",
    "        self.ca = ChannelAttention(8)\n",
    "        #self.sa = SpatialAttention()\n",
    "        #layer4: Convolution, (8,128,128)->(2,64,64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=2, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #layer5: mean pooling, (2,64,64)->(2,32,32)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(2)\n",
    "        #layer6: fully connected, 2*32*32->512\n",
    "        self.fcl1 = nn.Linear(2*32*32,512)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        #layer7: Hashing layer, 512->16\n",
    "        self.fcl2 = nn.Linear(512,outHashcode)#\n",
    "        self.tanh = nn.Tanh() #{-1,1}\n",
    "              \n",
    "    def forward(self,x):\n",
    "        #input: (batch_size, in_channels, Height, Width)\n",
    "        #output: (batch_size, out_channels, Height, Width)\n",
    "        #layer1: convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        #layer2: max pooling\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        #layer3: Attention\n",
    "        x = self.ca(x) * x  #channel\n",
    "        #x = self.sa(x) * x  #spatial\n",
    "        #layer4: Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu2(x)\n",
    "        #layer5: mean pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = self.bn4(x)\n",
    "        #layer6:fully connected\n",
    "        x = x.view(x.size(0),-1) #transfer three dims to one dim\n",
    "        x = self.fcl1(x)\n",
    "        x = self.relu3(x)\n",
    "        #layer7: Hashing layer\n",
    "        x = self.fcl2(x)\n",
    "        x = self.tanh(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "#https://pytorch-cn.readthedocs.io/zh/latest/    \n",
    "#https://github.com/filipradenovic/cnnimageretrieval-pytorch/blob/master/cirtorch/layers/functional.py\n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "    \n",
    "    def forward(self,h1,h2,y): \n",
    "        #h1=h2:NxD,y:N\n",
    "        dim = h1.shape[1]\n",
    "        euc_dist = F.pairwise_distance(h1, h2, p=2, eps=1e-06) # Calcualte Euclidean Distance\n",
    "        sim_term = 0.5*(1-y)*euc_dist #penalize the similar iamge pairs when y=0\n",
    "        unsim_term = 0.5*y*torch.clamp(self.margin*dim-euc_dist,0)#penalize the unsimlar image pairs when y =1\n",
    "        reg_term = self.alpha * ( torch.sum((torch.abs(h1)-1),dim=1) + torch.sum((torch.abs(h2)-1),dim=1) ) #regularization term\n",
    "        #loss = torch.mean(sim_term + unsim_term + reg_term) \n",
    "        loss = torch.sum(sim_term + unsim_term+ reg_term) \n",
    "        return loss\n",
    "    \n",
    "#define model\n",
    "model = AHNet(outHashcode=32).cuda()\n",
    "criterion  = HashLossFunc(margin=0.3).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "#best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "num_batches = len(trY_sf) // batchSize\n",
    "for epoch in range(100):#iteration\n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    #if np.mean(losses) < best_loss:\n",
    "     #   best_loss = np.mean(losses)\n",
    "     #   best_net = copy.deepcopy(model)\n",
    "#print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "#model = model.cpu()\n",
    "#torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize+1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(model(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(model(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#train data with list: trData, trI, trF, trY\n",
    "#test data with list: teData, teI, teF, teY\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        map_item_score = {}\n",
    "        for j, trVal in enumerate(trF):\n",
    "            map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in ranklist:\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    8381\n",
      "0.0    4309\n",
      "3.0    1677\n",
      "4.0    1494\n",
      "2.0    1407\n",
      "5.0     781\n",
      "3.5     383\n",
      "2.5     189\n",
      "6.0     155\n",
      "1.5      64\n",
      "4.5      61\n",
      "Name: P, dtype: int64\n",
      "7000\n",
      "700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Read data with List storage Data:[name],I:[img],Y[type]\n",
    "image_dir = '/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New' #the path of images\n",
    "traindf = pd.read_csv(\"/data/fjsdata/ASOCT/Cataract/CBIR_Cataract_ncp_train.csv\" , sep=',')#load dataset\n",
    "testdf = pd.read_csv(\"/data/fjsdata/ASOCT/Cataract/CBIR_Cataract_ncp_test.csv\" , sep=',')#load testset\n",
    "df = pd.concat([traindf, testdf], axis=0)\n",
    "#print (df.columns)\n",
    "df = df[df['lr']=='OD']\n",
    "print (df['P'].value_counts())\n",
    "df = df[['name','P']]\n",
    "trainset,testset = [], []\n",
    "ds =df[df['P']==1.0].sample(2381).sample(frac=1) #8381-2381-238\n",
    "testset.extend(np.array(ds).tolist()[0:238-40])\n",
    "trainset.extend(np.array(ds).tolist()[238:])\n",
    "ds =df[df['P']==2.0].sample(frac=1) #1407-136\n",
    "testset.extend(np.array(ds).tolist()[0:136])\n",
    "trainset.extend(np.array(ds).tolist()[136:])\n",
    "ds =df[df['P']==3.0].sample(frac=1) #1677-167\n",
    "testset.extend(np.array(ds).tolist()[0:167])\n",
    "trainset.extend(np.array(ds).tolist()[167:])\n",
    "ds =df[df['P']==4.0].sample(frac=1) #1494-149\n",
    "testset.extend(np.array(ds).tolist()[0:149])\n",
    "trainset.extend(np.array(ds).tolist()[149:])\n",
    "ds =df[df['P']==5.0].sample(frac=1) #781-50\n",
    "testset.extend(np.array(ds).tolist()[0:50])\n",
    "trainset.extend(np.array(ds).tolist()[50:])\n",
    "print (len(trainset))\n",
    "print (len(testset))\n",
    "pd.DataFrame(trainset).to_csv('/data/fjsdata/ASOCT/Cataract/CBIR_MICCAI_train.csv',index=False)\n",
    "pd.DataFrame(testset).to_csv('/data/fjsdata/ASOCT/Cataract/CBIR_MICCAI_test.csv',index=False)\n",
    "del ds,traindf,testdf,df\n",
    "gc.collect() #release cpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
