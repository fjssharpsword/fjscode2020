{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Develop Env: linux+cuda9+python3+opencv+pytorch\n",
    "2.Dataset: ASOCT-Cataract with 32560 images, Dataset Statistics: \n",
    "        1)OD:oculus dextrus, OS: oculus sinister\n",
    "           OD    18901\n",
    "           OS    13659\n",
    "        2)Structure: N, C, P train set: (29297, 6) test set: (3255, 6)\n",
    "           N-Level: 1.0-6.0 \n",
    "           C-Level: 1.0-5.0 if C=0.0(562) denotes this sample with no label \n",
    "           P-Level: 1.0-6.0 if C=0.0(7354) denotes this sample with no label\n",
    "        3)train set and test set:  OD, P-Level(Micro ROI) \n",
    "           1.0(2381),2.0(1407),3.0(1677),4.0(1494),5.0(781)-7000 for train ,700 for test     \n",
    "3.Performance Metric: \n",
    "  1)MHR(Mean Hit Ratio):  for evaluating the precison of relevance retrieval;\n",
    "  2)MAP(Mean Average Precision): for evaluation the rank of relevance retrieval;\n",
    "  3)MRR(Mean Reciprocal Rank): for evaluation the first hit rank of relevance retrieval;\n",
    "  4)Memory consumption and Retrieval Speed.\n",
    "4.Algorithm: \n",
    "  1)Baseline: HNet\n",
    "  2)Attention: AHNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from scipy.spatial.distance import pdist\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "#import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.set_device(6)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468 / 7000 C020_20180514_100234_R_CASIA2_LGC_004.jpg:/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New/C020_20180514_100234_R_CASIA2_LGC_004.jpg\n",
      "568 / 7000 C020_20180514_100234_R_CASIA2_LGC_000.jpg:/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New/C020_20180514_100234_R_CASIA2_LGC_000.jpg\n",
      "1871 / 7000 C020_20180514_100234_R_CASIA2_LGC_002.jpg:/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New/C020_20180514_100234_R_CASIA2_LGC_002.jpg\n",
      "1929 / 7000 C020_20180514_100234_R_CASIA2_LGC_008.jpg:/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New/C020_20180514_100234_R_CASIA2_LGC_008.jpg\n",
      "6996 / 7000 The length of train set is 6996\n",
      "700 / 700 The length of train set is 700\n"
     ]
    }
   ],
   "source": [
    "#Read data with List storage N:[name],I:[img],Y[type]\n",
    "image_dir = '/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New' #the path of images\n",
    "trainset = pd.read_csv(\"/data/fjsdata/ASOCT/Cataract/CBIR_MICCAI_train.csv\" , sep=',')#load dataset\n",
    "testset = pd.read_csv(\"/data/fjsdata/ASOCT/Cataract/CBIR_MICCAI_test.csv\" , sep=',')#load testset\n",
    "\n",
    "#read train image with CV\n",
    "trN, trI, trY = [],[],[]\n",
    "for iname, itype in np.array(trainset).tolist():#column: name,id,lr,N,C,P  \n",
    "    if iname.endswith(\".jpg\"):\n",
    "        try:\n",
    "            image_path = os.path.join(image_dir, iname)\n",
    "            img = cv2.resize(cv2.imread(image_path).astype(np.float32), (512, 512))\n",
    "            trN.append(iname)\n",
    "            trI.append(img)\n",
    "            trY.append(itype)\n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trN),len(trainset)))\n",
    "        sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(trN))\n",
    "#read test image with CV\n",
    "teN, teI, teY = [],[],[]\n",
    "for iname, itype in np.array(testset).tolist():#column: name,id,lr,N,C,P  \n",
    "    if iname.endswith(\".jpg\"):\n",
    "        try:\n",
    "            image_path = os.path.join(image_dir, iname)\n",
    "            img = cv2.resize(cv2.imread(image_path).astype(np.float32), (512, 512))\n",
    "            teN.append(iname)\n",
    "            teI.append(img)\n",
    "            teY.append(itype)\n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(teN),len(testset)))\n",
    "        sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(teN))\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs(spls=len(trY)):\n",
    "    idx_sf = random.sample(range(0, len(trY)),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#2 Hashing Network:HNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 349 / 349 : loss = 22.066423Eopch:     1 mean_loss = 17.566976\n",
      " 349 / 349 : loss = 22.022745Eopch:     2 mean_loss = 16.305806\n",
      " 349 / 349 : loss = 20.772488Eopch:     3 mean_loss = 16.194819\n",
      " 349 / 349 : loss = 21.853693Eopch:     4 mean_loss = 16.132962\n",
      " 349 / 349 : loss = 19.990168Eopch:     5 mean_loss = 15.674733\n",
      " 349 / 349 : loss = 20.286651Eopch:     6 mean_loss = 15.788922\n",
      " 349 / 349 : loss = 20.473982Eopch:     7 mean_loss = 15.602843\n",
      " 349 / 349 : loss = 18.921017Eopch:     8 mean_loss = 15.600062\n",
      " 349 / 349 : loss = 20.485531Eopch:     9 mean_loss = 15.911162\n",
      " 349 / 349 : loss = 17.932779Eopch:    10 mean_loss = 15.688822\n",
      " 349 / 349 : loss = 18.359922Eopch:    11 mean_loss = 15.746835\n",
      " 349 / 349 : loss = 18.878979Eopch:    12 mean_loss = 15.334322\n",
      " 349 / 349 : loss = 17.966068Eopch:    13 mean_loss = 15.347233\n",
      " 349 / 349 : loss = 16.425714Eopch:    14 mean_loss = 15.431356\n",
      " 349 / 349 : loss = 17.225279Eopch:    15 mean_loss = 15.418440\n",
      " 349 / 349 : loss = 16.469667Eopch:    16 mean_loss = 15.224069\n",
      " 349 / 349 : loss = 17.580442Eopch:    17 mean_loss = 15.129279\n",
      " 349 / 349 : loss = 19.784849Eopch:    18 mean_loss = 15.284969\n",
      " 349 / 349 : loss = 16.940826Eopch:    19 mean_loss = 15.083818\n",
      " 349 / 349 : loss = 16.745689Eopch:    20 mean_loss = 14.975285\n",
      " 349 / 349 : loss = 17.539925Eopch:    21 mean_loss = 14.936189\n",
      " 349 / 349 : loss = 18.647812Eopch:    22 mean_loss = 15.014026\n",
      " 349 / 349 : loss = 18.959196Eopch:    23 mean_loss = 14.998017\n",
      " 349 / 349 : loss = 19.365915Eopch:    24 mean_loss = 14.788481\n",
      " 349 / 349 : loss = 19.063103Eopch:    25 mean_loss = 15.204912\n",
      " 349 / 349 : loss = 19.614496Eopch:    26 mean_loss = 15.029828\n",
      " 349 / 349 : loss = 19.551926Eopch:    27 mean_loss = 15.082532\n",
      " 349 / 349 : loss = 17.946774Eopch:    28 mean_loss = 14.871617\n",
      " 349 / 349 : loss = 18.198509Eopch:    29 mean_loss = 14.895923\n",
      " 349 / 349 : loss = 17.797138Eopch:    30 mean_loss = 14.823628\n",
      " 349 / 349 : loss = 17.641571Eopch:    31 mean_loss = 14.840901\n",
      " 349 / 349 : loss = 18.287481Eopch:    32 mean_loss = 15.156244\n",
      " 349 / 349 : loss = 17.898718Eopch:    33 mean_loss = 15.233338\n",
      " 349 / 349 : loss = 18.200554Eopch:    34 mean_loss = 15.198904\n",
      " 349 / 349 : loss = 17.094494Eopch:    35 mean_loss = 14.969659\n",
      " 349 / 349 : loss = 16.359198Eopch:    36 mean_loss = 14.841125\n",
      " 349 / 349 : loss = 16.645857Eopch:    37 mean_loss = 14.920751\n",
      " 349 / 349 : loss = 16.257763Eopch:    38 mean_loss = 14.754789\n",
      " 349 / 349 : loss = 16.972483Eopch:    39 mean_loss = 14.844587\n",
      " 349 / 349 : loss = 18.634399Eopch:    40 mean_loss = 14.637029\n",
      " 349 / 349 : loss = 16.009626Eopch:    41 mean_loss = 14.612057\n",
      " 349 / 349 : loss = 15.236092Eopch:    42 mean_loss = 14.727859\n",
      " 349 / 349 : loss = 16.790602Eopch:    43 mean_loss = 14.601152\n",
      " 349 / 349 : loss = 18.544228Eopch:    44 mean_loss = 14.577619\n",
      " 349 / 349 : loss = 16.393261Eopch:    45 mean_loss = 14.683006\n",
      " 349 / 349 : loss = 15.894695Eopch:    46 mean_loss = 14.276372\n",
      " 349 / 349 : loss = 16.490679Eopch:    47 mean_loss = 14.226090\n",
      " 349 / 349 : loss = 17.229534Eopch:    48 mean_loss = 14.163609\n",
      " 349 / 349 : loss = 16.953224Eopch:    49 mean_loss = 14.525184\n",
      " 349 / 349 : loss = 18.627699Eopch:    50 mean_loss = 14.398612\n",
      " 349 / 349 : loss = 19.205276Eopch:    51 mean_loss = 14.289860\n",
      " 349 / 349 : loss = 18.580355Eopch:    52 mean_loss = 14.602147\n",
      " 349 / 349 : loss = 18.618122Eopch:    53 mean_loss = 14.194609\n",
      " 349 / 349 : loss = 17.833639Eopch:    54 mean_loss = 14.042385\n",
      " 349 / 349 : loss = 18.039164Eopch:    55 mean_loss = 14.137238\n",
      " 349 / 349 : loss = 19.766314Eopch:    56 mean_loss = 14.123091\n",
      " 349 / 349 : loss = 17.823923Eopch:    57 mean_loss = 14.118501\n",
      " 349 / 349 : loss = 18.436633Eopch:    58 mean_loss = 14.296230\n",
      " 349 / 349 : loss = 18.254656Eopch:    59 mean_loss = 14.216455\n",
      " 349 / 349 : loss = 17.592167Eopch:    60 mean_loss = 14.246804\n",
      " 349 / 349 : loss = 16.446169Eopch:    61 mean_loss = 14.059724\n",
      " 349 / 349 : loss = 17.745594Eopch:    62 mean_loss = 14.023843\n",
      " 349 / 349 : loss = 18.389273Eopch:    63 mean_loss = 13.941553\n",
      " 349 / 349 : loss = 18.297188Eopch:    64 mean_loss = 13.955699\n",
      " 349 / 349 : loss = 18.967932Eopch:    65 mean_loss = 14.013308\n",
      " 349 / 349 : loss = 18.865437Eopch:    66 mean_loss = 14.097098\n",
      " 349 / 349 : loss = 19.229368Eopch:    67 mean_loss = 14.198125\n",
      " 349 / 349 : loss = 17.327097Eopch:    68 mean_loss = 13.985813\n",
      " 349 / 349 : loss = 19.431818Eopch:    69 mean_loss = 14.437726\n",
      " 349 / 349 : loss = 16.805717Eopch:    70 mean_loss = 13.913979\n",
      " 349 / 349 : loss = 16.392017Eopch:    71 mean_loss = 14.037101\n",
      " 349 / 349 : loss = 16.283091Eopch:    72 mean_loss = 13.884403\n",
      " 349 / 349 : loss = 14.945065Eopch:    73 mean_loss = 13.612502\n",
      " 349 / 349 : loss = 16.073862Eopch:    74 mean_loss = 13.756563\n",
      " 349 / 349 : loss = 16.244572Eopch:    75 mean_loss = 13.676350\n",
      " 349 / 349 : loss = 14.718124Eopch:    76 mean_loss = 13.416915\n",
      " 349 / 349 : loss = 16.080132Eopch:    77 mean_loss = 13.174745\n",
      " 349 / 349 : loss = 16.040319Eopch:    78 mean_loss = 13.298290\n",
      " 349 / 349 : loss = 16.377499Eopch:    79 mean_loss = 13.286745\n",
      " 349 / 349 : loss = 16.137629Eopch:    80 mean_loss = 13.273508\n",
      " 349 / 349 : loss = 15.369308Eopch:    81 mean_loss = 13.004588\n",
      " 349 / 349 : loss = 15.247387Eopch:    82 mean_loss = 12.815625\n",
      " 349 / 349 : loss = 14.927569Eopch:    83 mean_loss = 13.049758\n",
      " 349 / 349 : loss = 15.430985Eopch:    84 mean_loss = 13.235650\n",
      " 349 / 349 : loss = 16.118155Eopch:    85 mean_loss = 13.027427\n",
      " 349 / 349 : loss = 14.116809Eopch:    86 mean_loss = 12.813294\n",
      " 349 / 349 : loss = 12.832313Eopch:    87 mean_loss = 12.792967\n",
      " 349 / 349 : loss = 13.197157Eopch:    88 mean_loss = 12.609990\n",
      " 349 / 349 : loss = 11.760715Eopch:    89 mean_loss = 12.594182\n",
      " 349 / 349 : loss = 10.341804Eopch:    90 mean_loss = 12.480746\n",
      " 349 / 349 : loss = 13.009406Eopch:    91 mean_loss = 12.499603\n",
      " 349 / 349 : loss = 11.267006Eopch:    92 mean_loss = 12.123053\n",
      " 349 / 349 : loss = 12.194567Eopch:    93 mean_loss = 11.934052\n",
      " 349 / 349 : loss = 10.751324Eopch:    94 mean_loss = 11.942366\n",
      " 349 / 349 : loss = 9.8430565Eopch:    95 mean_loss = 11.872554\n",
      " 349 / 349 : loss = 9.7666193Eopch:    96 mean_loss = 11.794343\n",
      " 349 / 349 : loss = 10.702666Eopch:    97 mean_loss = 11.588078\n",
      " 349 / 349 : loss = 8.6104341Eopch:    98 mean_loss = 11.344637\n",
      " 349 / 349 : loss = 8.6442769Eopch:    99 mean_loss = 11.332589\n",
      " 349 / 349 : loss = 9.6267821Eopch:   100 mean_loss = 11.109922\n",
      "best_loss = 11.109922\n",
      " 69 / 70 0 mHR@5=0.371429, mAP@5=0.314195, mRR@5=0.752365\n",
      "mHR@10=0.364143, mAP@10=0.276631, mRR@10=0.655813\n",
      "mHR@15=0.355810, mAP@15=0.254039, mRR@15=0.625208\n",
      "mHR@20=0.343500, mAP@20=0.235923, mRR@20=0.596431\n"
     ]
    }
   ],
   "source": [
    "#define Hashing network with pytorch\n",
    "class HNet(nn.Module): \n",
    "    def __init__(self,inChannels=3, outHashcode=16):\n",
    "        super(HNet, self).__init__()\n",
    "        #(channels, Height, Width)\n",
    "        #layer1: Convolution, (3,512,512)->(8,256,256)\n",
    "        self.conv1 = nn.Conv2d(in_channels=inChannels, out_channels=8, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        #layer2: max pooling,(8,256,256)->(8,128,128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        #layer: spatial attention, pass\n",
    "        #layer3: Convolution, (8,128,128)->(2,64,64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=2, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #layer4: mean pooling, (2,64,64)->(2,32,32)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(2)\n",
    "        #layer5: fully connected, 2*32*32->512\n",
    "        self.fcl1 = nn.Linear(2*32*32,512)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        #layer6: Hashing layer, 512->16\n",
    "        self.fcl2 = nn.Linear(512,outHashcode)#\n",
    "        self.tanh = nn.Tanh() #{-1,1}\n",
    "              \n",
    "    def forward(self,x):\n",
    "        #input: (batch_size, in_channels, Height, Width)\n",
    "        #output: (batch_size, out_channels, Height, Width)\n",
    "        #layer1: convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        #layer2: max pooling\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        #layer: attention,pass\n",
    "        #layer3: Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu2(x)\n",
    "        #layer4: mean pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = self.bn4(x)\n",
    "        #layer5:fully connected\n",
    "        x = x.view(x.size(0),-1) #transfer three dims to one dim\n",
    "        x = self.fcl1(x)\n",
    "        x = self.relu3(x)\n",
    "        #layer6: Hashing layer\n",
    "        x = self.fcl2(x)\n",
    "        x = self.tanh(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "#https://pytorch-cn.readthedocs.io/zh/latest/    \n",
    "#https://github.com/filipradenovic/cnnimageretrieval-pytorch/blob/master/cirtorch/layers/functional.py\n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "    \n",
    "    def forward(self,h1,h2,y): \n",
    "        #h1=h2:NxD,y:N\n",
    "        dim = h1.shape[1]\n",
    "        euc_dist = F.pairwise_distance(h1, h2, p=2, eps=1e-06) # Calcualte Euclidean Distance\n",
    "        sim_term = 0.5*(1-y)*euc_dist #penalize the similar iamge pairs when y=0\n",
    "        unsim_term = 0.5*y*torch.clamp(self.margin*dim-euc_dist,0)#penalize the unsimlar image pairs when y =1\n",
    "        reg_term = self.alpha * ( torch.sum((torch.abs(h1)-1),dim=1) + torch.sum((torch.abs(h2)-1),dim=1) ) #regularization term\n",
    "        #loss = torch.mean(sim_term + unsim_term + reg_term) \n",
    "        loss = torch.sum(sim_term + unsim_term+ reg_term) \n",
    "        return loss\n",
    "\n",
    "#define model\n",
    "model = HNet(outHashcode=32).cuda()\n",
    "criterion  = HashLossFunc(margin=0.3).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "num_batches = len(trY_sf) // batchSize\n",
    "for epoch in range(100):#iteration\n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad() #grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(best_net(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(best_net(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#train data with list: trData, trI, trF, trY\n",
    "#test data with list: teData, teI, teF, teY\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        map_item_score = {}\n",
    "        for j, trVal in enumerate(trF):\n",
    "            map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in ranklist:\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#3 Attention-based Hashing Network:AHNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 349 / 349 : loss = 16.492819Eopch:     1 mean_loss = 17.419442\n",
      " 349 / 349 : loss = 19.996469Eopch:     2 mean_loss = 16.484739\n",
      " 349 / 349 : loss = 21.370365Eopch:     3 mean_loss = 17.181245\n",
      " 349 / 349 : loss = 20.443871Eopch:     4 mean_loss = 16.764355\n",
      " 349 / 349 : loss = 20.505417Eopch:     5 mean_loss = 16.650235\n",
      " 349 / 349 : loss = 19.654013Eopch:     6 mean_loss = 16.533890\n",
      " 349 / 349 : loss = 21.646709Eopch:     7 mean_loss = 16.381284\n",
      " 349 / 349 : loss = 20.153263Eopch:     8 mean_loss = 16.070623\n",
      " 349 / 349 : loss = 22.150024Eopch:     9 mean_loss = 15.893461\n",
      " 349 / 349 : loss = 20.635471Eopch:    10 mean_loss = 16.207966\n",
      " 349 / 349 : loss = 19.438141Eopch:    11 mean_loss = 16.095785\n",
      " 349 / 349 : loss = 20.364534Eopch:    12 mean_loss = 16.388130\n",
      " 349 / 349 : loss = 21.267626Eopch:    13 mean_loss = 15.893841\n",
      " 349 / 349 : loss = 19.714793Eopch:    14 mean_loss = 15.940588\n",
      " 349 / 349 : loss = 21.177565Eopch:    15 mean_loss = 16.313077\n",
      " 349 / 349 : loss = 20.718987Eopch:    16 mean_loss = 16.032325\n",
      " 349 / 349 : loss = 19.575218Eopch:    17 mean_loss = 16.103909\n",
      " 349 / 349 : loss = 21.628006Eopch:    18 mean_loss = 15.829299\n",
      " 349 / 349 : loss = 21.939978Eopch:    19 mean_loss = 16.014818\n",
      " 349 / 349 : loss = 21.564964Eopch:    20 mean_loss = 15.880432\n",
      " 349 / 349 : loss = 21.158171Eopch:    21 mean_loss = 15.551392\n",
      " 349 / 349 : loss = 20.056622Eopch:    22 mean_loss = 15.524793\n",
      " 349 / 349 : loss = 21.349716Eopch:    23 mean_loss = 15.399839\n",
      " 349 / 349 : loss = 22.136379Eopch:    24 mean_loss = 15.593196\n",
      " 349 / 349 : loss = 19.505083Eopch:    25 mean_loss = 15.704069\n",
      " 349 / 349 : loss = 20.160759Eopch:    26 mean_loss = 15.540850\n",
      " 349 / 349 : loss = 20.242081Eopch:    27 mean_loss = 15.567777\n",
      " 349 / 349 : loss = 21.262016Eopch:    28 mean_loss = 15.363956\n",
      " 349 / 349 : loss = 19.929425Eopch:    29 mean_loss = 15.537755\n",
      " 349 / 349 : loss = 22.023087Eopch:    30 mean_loss = 15.616483\n",
      " 349 / 349 : loss = 21.755533Eopch:    31 mean_loss = 15.825912\n",
      " 349 / 349 : loss = 22.408715Eopch:    32 mean_loss = 15.787068\n",
      " 349 / 349 : loss = 22.556217Eopch:    33 mean_loss = 15.539104\n",
      " 349 / 349 : loss = 20.673317Eopch:    34 mean_loss = 15.402454\n",
      " 349 / 349 : loss = 21.670667Eopch:    35 mean_loss = 15.363397\n",
      " 349 / 349 : loss = 21.566748Eopch:    36 mean_loss = 15.487197\n",
      " 349 / 349 : loss = 17.448387Eopch:    37 mean_loss = 15.291209\n",
      " 349 / 349 : loss = 23.563404Eopch:    38 mean_loss = 15.290106\n",
      " 349 / 349 : loss = 22.309833Eopch:    39 mean_loss = 15.444901\n",
      " 349 / 349 : loss = 21.480455Eopch:    40 mean_loss = 15.022472\n",
      " 349 / 349 : loss = 20.942516Eopch:    41 mean_loss = 14.983056\n",
      " 349 / 349 : loss = 21.544815Eopch:    42 mean_loss = 14.911851\n",
      " 349 / 349 : loss = 22.467973Eopch:    43 mean_loss = 15.135323\n",
      " 349 / 349 : loss = 20.096287Eopch:    44 mean_loss = 15.070509\n",
      " 349 / 349 : loss = 18.597677Eopch:    45 mean_loss = 14.927496\n",
      " 349 / 349 : loss = 18.065323Eopch:    46 mean_loss = 14.913452\n",
      " 349 / 349 : loss = 19.927168Eopch:    47 mean_loss = 14.877357\n",
      " 349 / 349 : loss = 18.925112Eopch:    48 mean_loss = 14.642832\n",
      " 349 / 349 : loss = 19.248577Eopch:    49 mean_loss = 14.787187\n",
      " 349 / 349 : loss = 20.450974Eopch:    50 mean_loss = 14.813903\n",
      " 349 / 349 : loss = 19.851185Eopch:    51 mean_loss = 14.709955\n",
      " 349 / 349 : loss = 20.565792Eopch:    52 mean_loss = 14.520569\n",
      " 349 / 349 : loss = 19.926889Eopch:    53 mean_loss = 14.676070\n",
      " 349 / 349 : loss = 19.999378Eopch:    54 mean_loss = 14.677745\n",
      " 349 / 349 : loss = 20.363855Eopch:    55 mean_loss = 14.628932\n",
      " 349 / 349 : loss = 20.502378Eopch:    56 mean_loss = 14.434759\n",
      " 349 / 349 : loss = 19.422383Eopch:    57 mean_loss = 14.417846\n",
      " 349 / 349 : loss = 20.892529Eopch:    58 mean_loss = 14.369114\n",
      " 349 / 349 : loss = 22.453789Eopch:    59 mean_loss = 14.391543\n",
      " 349 / 349 : loss = 20.961868Eopch:    60 mean_loss = 14.438054\n",
      " 349 / 349 : loss = 20.534971Eopch:    61 mean_loss = 14.677727\n",
      " 349 / 349 : loss = 19.631317Eopch:    62 mean_loss = 14.486736\n",
      " 349 / 349 : loss = 20.494104Eopch:    63 mean_loss = 14.548344\n",
      " 349 / 349 : loss = 20.928764Eopch:    64 mean_loss = 14.256534\n",
      " 349 / 349 : loss = 21.196987Eopch:    65 mean_loss = 14.438484\n",
      " 349 / 349 : loss = 20.855043Eopch:    66 mean_loss = 14.500044\n",
      " 349 / 349 : loss = 20.564997Eopch:    67 mean_loss = 14.142987\n",
      " 349 / 349 : loss = 20.805101Eopch:    68 mean_loss = 14.155025\n",
      " 349 / 349 : loss = 21.143642Eopch:    69 mean_loss = 14.116688\n",
      " 349 / 349 : loss = 20.145323Eopch:    70 mean_loss = 14.101644\n",
      " 349 / 349 : loss = 21.063368Eopch:    71 mean_loss = 14.098601\n",
      " 349 / 349 : loss = 20.346161Eopch:    72 mean_loss = 13.997913\n",
      " 349 / 349 : loss = 20.650902Eopch:    73 mean_loss = 13.958817\n",
      " 349 / 349 : loss = 20.622826Eopch:    74 mean_loss = 13.918736\n",
      " 349 / 349 : loss = 18.224646Eopch:    75 mean_loss = 13.795721\n",
      " 349 / 349 : loss = 19.514158Eopch:    76 mean_loss = 14.041583\n",
      " 349 / 349 : loss = 19.042212Eopch:    77 mean_loss = 13.978314\n",
      " 349 / 349 : loss = 18.393032Eopch:    78 mean_loss = 14.047351\n",
      " 349 / 349 : loss = 19.527327Eopch:    79 mean_loss = 14.035681\n",
      " 349 / 349 : loss = 19.182643Eopch:    80 mean_loss = 14.075489\n",
      " 349 / 349 : loss = 19.767557Eopch:    81 mean_loss = 13.992108\n",
      " 349 / 349 : loss = 20.561369Eopch:    82 mean_loss = 13.943827\n",
      " 349 / 349 : loss = 19.680786Eopch:    83 mean_loss = 14.097974\n",
      " 349 / 349 : loss = 19.335127Eopch:    84 mean_loss = 13.909069\n",
      " 349 / 349 : loss = 22.002678Eopch:    85 mean_loss = 13.874733\n",
      " 349 / 349 : loss = 18.843523Eopch:    86 mean_loss = 13.744297\n",
      " 349 / 349 : loss = 19.569294Eopch:    87 mean_loss = 13.576770\n",
      " 349 / 349 : loss = 20.224354Eopch:    88 mean_loss = 13.549137\n",
      " 349 / 349 : loss = 20.382385Eopch:    89 mean_loss = 13.576388\n",
      " 349 / 349 : loss = 19.296247Eopch:    90 mean_loss = 13.531018\n",
      " 349 / 349 : loss = 19.551027Eopch:    91 mean_loss = 13.499338\n",
      " 349 / 349 : loss = 20.862362Eopch:    92 mean_loss = 13.416658\n",
      " 349 / 349 : loss = 20.621227Eopch:    93 mean_loss = 13.507438\n",
      " 349 / 349 : loss = 20.806334Eopch:    94 mean_loss = 13.475452\n",
      " 349 / 349 : loss = 19.482347Eopch:    95 mean_loss = 13.309492\n",
      " 349 / 349 : loss = 19.716499Eopch:    96 mean_loss = 13.002601\n",
      " 349 / 349 : loss = 17.072515Eopch:    97 mean_loss = 12.815537\n",
      " 349 / 349 : loss = 17.615681Eopch:    98 mean_loss = 12.998508\n",
      " 349 / 349 : loss = 17.093361Eopch:    99 mean_loss = 12.906073\n",
      " 349 / 349 : loss = 18.149611Eopch:   100 mean_loss = 12.996042\n",
      "best_loss = 12.815537\n",
      " 69 / 70 0 mHR@5=0.434571, mAP@5=0.383243, mRR@5=0.798817\n",
      "mHR@10=0.422000, mAP@10=0.342649, mRR@10=0.698165\n",
      "mHR@15=0.409333, mAP@15=0.314456, mRR@15=0.657843\n",
      "mHR@20=0.397571, mAP@20=0.293660, mRR@20=0.636344\n"
     ]
    }
   ],
   "source": [
    "#define Attention-based Hashing network with pytorch\n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class AHNet(nn.Module): #deep Hashint Network:DHNet\n",
    "    def __init__(self,inChannels=3,outHashcode=16):\n",
    "        super(AHNet, self).__init__()\n",
    "        #(channels, Height, Width)\n",
    "        #layer1: Convolution, (3,512,512)->(8,256,256)\n",
    "        self.conv1 = nn.Conv2d(in_channels=inChannels, out_channels=8, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        #layer2: max pooling,(8,256,256)->(8,128,128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        #layer3: Spatial Attention Layer, (8,256,256)->(8,256,256)\n",
    "        self.sa = SpatialAttention()\n",
    "        #layer4: Convolution, (8,128,128)->(2,64,64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=2, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #layer5: mean pooling, (2,64,64)->(2,32,32)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(2)\n",
    "        #layer6: fully connected, 2*32*32->512\n",
    "        self.fcl1 = nn.Linear(2*32*32,512)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        #layer7: Hashing layer, 512->16\n",
    "        self.fcl2 = nn.Linear(512,outHashcode)#\n",
    "        self.tanh = nn.Tanh() #{-1,1}\n",
    "              \n",
    "    def forward(self,x):\n",
    "        #input: (batch_size, in_channels, Height, Width)\n",
    "        #output: (batch_size, out_channels, Height, Width)\n",
    "        #layer1: convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        #layer2: max pooling\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        #layer3: Attention\n",
    "        x = self.sa(x)*x\n",
    "        #layer4: Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu2(x)\n",
    "        #layer5: mean pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = self.bn4(x)\n",
    "        #layer6:fully connected\n",
    "        x = x.view(x.size(0),-1) #transfer three dims to one dim\n",
    "        x = self.fcl1(x)\n",
    "        x = self.relu3(x)\n",
    "        #layer7: Hashing layer\n",
    "        x = self.fcl2(x)\n",
    "        x = self.tanh(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "#https://pytorch-cn.readthedocs.io/zh/latest/    \n",
    "#https://github.com/filipradenovic/cnnimageretrieval-pytorch/blob/master/cirtorch/layers/functional.py\n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "    \n",
    "    def forward(self,h1,h2,y): \n",
    "        #h1=h2:NxD,y:N\n",
    "        dim = h1.shape[1]\n",
    "        euc_dist = F.pairwise_distance(h1, h2, p=2, eps=1e-06) # Calcualte Euclidean Distance\n",
    "        sim_term = 0.5*(1-y)*euc_dist #penalize the similar iamge pairs when y=0\n",
    "        unsim_term = 0.5*y*torch.clamp(self.margin*dim-euc_dist,0)#penalize the unsimlar image pairs when y =1\n",
    "        reg_term = self.alpha * ( torch.sum((torch.abs(h1)-1),dim=1) + torch.sum((torch.abs(h2)-1),dim=1) ) #regularization term\n",
    "        #loss = torch.mean(sim_term + unsim_term + reg_term) \n",
    "        loss = torch.sum(sim_term + unsim_term+ reg_term) \n",
    "        return loss\n",
    "    \n",
    "#define model\n",
    "model = AHNet(outHashcode=32).cuda()\n",
    "criterion  = HashLossFunc(margin=0.3).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "num_batches = len(trY_sf) // batchSize\n",
    "for epoch in range(100):#iteration\n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize+1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(best_net(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(best_net(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#train data with list: trData, trI, trF, trY\n",
    "#test data with list: teData, teI, teF, teY\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        map_item_score = {}\n",
    "        for j, trVal in enumerate(trF):\n",
    "            map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in ranklist:\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    8381\n",
      "0.0    4309\n",
      "3.0    1677\n",
      "4.0    1494\n",
      "2.0    1407\n",
      "5.0     781\n",
      "3.5     383\n",
      "2.5     189\n",
      "6.0     155\n",
      "1.5      64\n",
      "4.5      61\n",
      "Name: P, dtype: int64\n",
      "7000\n",
      "700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Read data with List storage Data:[name],I:[img],Y[type]\n",
    "image_dir = '/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New' #the path of images\n",
    "traindf = pd.read_csv(\"/data/fjsdata/ASOCT/Cataract/CBIR_Cataract_ncp_train.csv\" , sep=',')#load dataset\n",
    "testdf = pd.read_csv(\"/data/fjsdata/ASOCT/Cataract/CBIR_Cataract_ncp_test.csv\" , sep=',')#load testset\n",
    "df = pd.concat([traindf, testdf], axis=0)\n",
    "#print (df.columns)\n",
    "df = df[df['lr']=='OD']\n",
    "print (df['P'].value_counts())\n",
    "df = df[['name','P']]\n",
    "trainset,testset = [], []\n",
    "ds =df[df['P']==1.0].sample(2381).sample(frac=1) #8381-2381-238\n",
    "testset.extend(np.array(ds).tolist()[0:238-40])\n",
    "trainset.extend(np.array(ds).tolist()[238:])\n",
    "ds =df[df['P']==2.0].sample(frac=1) #1407-136\n",
    "testset.extend(np.array(ds).tolist()[0:136])\n",
    "trainset.extend(np.array(ds).tolist()[136:])\n",
    "ds =df[df['P']==3.0].sample(frac=1) #1677-167\n",
    "testset.extend(np.array(ds).tolist()[0:167])\n",
    "trainset.extend(np.array(ds).tolist()[167:])\n",
    "ds =df[df['P']==4.0].sample(frac=1) #1494-149\n",
    "testset.extend(np.array(ds).tolist()[0:149])\n",
    "trainset.extend(np.array(ds).tolist()[149:])\n",
    "ds =df[df['P']==5.0].sample(frac=1) #781-50\n",
    "testset.extend(np.array(ds).tolist()[0:50])\n",
    "trainset.extend(np.array(ds).tolist()[50:])\n",
    "print (len(trainset))\n",
    "print (len(testset))\n",
    "pd.DataFrame(trainset).to_csv('/data/fjsdata/ASOCT/Cataract/CBIR_MICCAI_train.csv',index=False)\n",
    "pd.DataFrame(testset).to_csv('/data/fjsdata/ASOCT/Cataract/CBIR_MICCAI_test.csv',index=False)\n",
    "del ds,traindf,testdf,df\n",
    "gc.collect() #release cpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
