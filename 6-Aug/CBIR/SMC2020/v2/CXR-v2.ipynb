{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Develop Env: linux+cuda9+python3+opencv+pytorch\n",
    "2.Dataset: MIMIC-CXR https://mimic-cxr.mit.edu/about/access/\n",
    "        1)Label:No Finding,Enlarged Cardiomediastinum,Cardiomegaly,Airspace Opacity,Lung Lesion,\n",
    "               Edema,Consolidation,Pneumonia,Atelectasis,Pneumothorax,Pleural Effusion,Pleural Other,Fracture,Support Devices\n",
    "               0.0 - negative, 1.0 - positve, -1.0 - uncertain\n",
    "        2)Dataset: Pneumonia(frontal view) 0.0 - negative (3300), 1.0 - positve(3300), -1.0 - uncertain(3300), \n",
    "               9000 for train,1000 fot test\n",
    "3.Performance Metric: \n",
    "  1)MHR(Mean Hit Ratio):  for evaluating the precison of relevance retrieval;\n",
    "  2)MAP(Mean Average Precision): for evaluation the rank of relevance retrieval;\n",
    "  3)MRR(Mean Reciprocal Rank): for evaluation the first hit rank of relevance retrieval;\n",
    "  4)Memory consumption and Retrieval Speed.\n",
    "4.Algorithm: \n",
    "  1)Baseline: HNet\n",
    "  2)Attention: AHNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "9.0.176\n",
      "GeForce RTX 2080 Ti\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from scipy.spatial.distance import pdist\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "print (torch.cuda.is_available())\n",
    "print (torch.version.cuda)\n",
    "print (torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "torch.cuda.set_device(7)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 / 9000 The length of train set is 9000\n",
      "900 / 900 The length of train set is 900\n",
      "Completed buliding index in 1702 seconds\n"
     ]
    }
   ],
   "source": [
    "#1. Read data with List storage Name:[name],I:[img],Y[type]\n",
    "root_dir = '/data/fjsdata/physionet/MIMIC-CXR' #the path of images\n",
    "trainset = pd.read_csv(\"/data/fjsdata/physionet/MIMIC-CXR/CBIR_train.csv\" , sep=',')#load dataset\n",
    "testset = pd.read_csv(\"/data/fjsdata/physionet/MIMIC-CXR/CBIR_test.csv\" , sep=',')#load dataset\n",
    "tstart = time.time()\n",
    "#read train image with CV\n",
    "trN, trI, trY = [],[],[]\n",
    "for iname, itype in np.array(trainset).tolist():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, iname)\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (512, 512))#(1024,1024,3)->(512,512,3)\n",
    "        trN.append(iname)\n",
    "        trI.append(img)\n",
    "        trY.append(itype)\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(trN),trainset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(trN))\n",
    "#read test image with CV\n",
    "teN, teI, teY = [],[],[]\n",
    "for iname, itype in np.array(testset).tolist():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, iname)\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (512, 512))#(1024,1024,3)->(512,512,3)\n",
    "        teN.append(iname)\n",
    "        teI.append(img)\n",
    "        teY.append(itype)  \n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(teN),testset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(teN))\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs(spls=len(trY)):\n",
    "    idx_sf = random.sample(range(0, len(trY)),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#2 Hashing Network:HNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 450 / 450 : loss = 20.753569Eopch:     1 mean_loss = 20.779539\n",
      " 450 / 450 : loss = 22.686249Eopch:     2 mean_loss = 19.773121\n",
      " 450 / 450 : loss = 25.414377Eopch:     3 mean_loss = 19.411851\n",
      " 450 / 450 : loss = 23.817625Eopch:     4 mean_loss = 19.150364\n",
      " 450 / 450 : loss = 22.570183Eopch:     5 mean_loss = 19.226598\n",
      " 450 / 450 : loss = 22.139744Eopch:     6 mean_loss = 19.008948\n",
      " 450 / 450 : loss = 22.422085Eopch:     7 mean_loss = 18.942513\n",
      " 450 / 450 : loss = 21.859486Eopch:     8 mean_loss = 18.985912\n",
      " 450 / 450 : loss = 23.426498Eopch:     9 mean_loss = 18.723805\n",
      " 450 / 450 : loss = 22.714871Eopch:    10 mean_loss = 18.704162\n",
      " 450 / 450 : loss = 23.939526Eopch:    11 mean_loss = 18.810273\n",
      " 450 / 450 : loss = 22.518263Eopch:    12 mean_loss = 18.803040\n",
      " 450 / 450 : loss = 22.079067Eopch:    13 mean_loss = 18.696478\n",
      " 450 / 450 : loss = 22.759193Eopch:    14 mean_loss = 19.081631\n",
      " 450 / 450 : loss = 21.495045Eopch:    15 mean_loss = 18.874965\n",
      " 450 / 450 : loss = 25.023487Eopch:    16 mean_loss = 18.646773\n",
      " 450 / 450 : loss = 22.697193Eopch:    17 mean_loss = 18.396921\n",
      " 450 / 450 : loss = 23.113777Eopch:    18 mean_loss = 18.499332\n",
      " 450 / 450 : loss = 23.861181Eopch:    19 mean_loss = 18.684631\n",
      " 450 / 450 : loss = 22.489056Eopch:    20 mean_loss = 18.633835\n",
      " 450 / 450 : loss = 25.876404Eopch:    21 mean_loss = 18.878154\n",
      " 450 / 450 : loss = 22.837158Eopch:    22 mean_loss = 18.698713\n",
      " 450 / 450 : loss = 21.544912Eopch:    23 mean_loss = 18.465840\n",
      " 450 / 450 : loss = 22.676441Eopch:    24 mean_loss = 18.437228\n",
      " 450 / 450 : loss = 21.405766Eopch:    25 mean_loss = 18.523226\n",
      " 450 / 450 : loss = 22.818756Eopch:    26 mean_loss = 18.497270\n",
      " 450 / 450 : loss = 20.880648Eopch:    27 mean_loss = 18.517507\n",
      " 450 / 450 : loss = 22.751617Eopch:    28 mean_loss = 18.509359\n",
      " 450 / 450 : loss = 23.011675Eopch:    29 mean_loss = 18.296734\n",
      " 450 / 450 : loss = 20.667738Eopch:    30 mean_loss = 18.430487\n",
      " 450 / 450 : loss = 22.139788Eopch:    31 mean_loss = 18.473897\n",
      " 450 / 450 : loss = 21.809056Eopch:    32 mean_loss = 18.619967\n",
      " 450 / 450 : loss = 21.388952Eopch:    33 mean_loss = 18.786994\n",
      " 450 / 450 : loss = 21.974651Eopch:    34 mean_loss = 18.336578\n",
      " 450 / 450 : loss = 22.064693Eopch:    35 mean_loss = 18.453090\n",
      " 450 / 450 : loss = 22.768509Eopch:    36 mean_loss = 18.453898\n",
      " 450 / 450 : loss = 22.309895Eopch:    37 mean_loss = 18.506481\n",
      " 450 / 450 : loss = 22.606682Eopch:    38 mean_loss = 18.273242\n",
      " 450 / 450 : loss = 21.015406Eopch:    39 mean_loss = 18.413451\n",
      " 450 / 450 : loss = 21.009548Eopch:    40 mean_loss = 18.345576\n",
      " 450 / 450 : loss = 21.611221Eopch:    41 mean_loss = 18.408780\n",
      " 450 / 450 : loss = 21.049641Eopch:    42 mean_loss = 18.117233\n",
      " 450 / 450 : loss = 19.931196Eopch:    43 mean_loss = 18.094135\n",
      " 450 / 450 : loss = 22.012604Eopch:    45 mean_loss = 18.172143\n",
      " 450 / 450 : loss = 21.450871Eopch:    46 mean_loss = 18.255086\n",
      " 450 / 450 : loss = 22.422025Eopch:    47 mean_loss = 18.195911\n",
      " 450 / 450 : loss = 23.068481Eopch:    48 mean_loss = 18.298485\n",
      " 450 / 450 : loss = 20.599764Eopch:    49 mean_loss = 18.271977\n",
      " 450 / 450 : loss = 21.698986Eopch:    50 mean_loss = 18.141770\n",
      " 450 / 450 : loss = 20.800777Eopch:    51 mean_loss = 18.120892\n",
      " 450 / 450 : loss = 19.840988Eopch:    52 mean_loss = 18.165310\n",
      " 450 / 450 : loss = 23.092739Eopch:    53 mean_loss = 18.264900\n",
      " 450 / 450 : loss = 20.927118Eopch:    54 mean_loss = 18.245761\n",
      " 450 / 450 : loss = 21.759335Eopch:    55 mean_loss = 18.057134\n",
      " 450 / 450 : loss = 22.199844Eopch:    56 mean_loss = 18.191229\n",
      " 450 / 450 : loss = 20.468639Eopch:    57 mean_loss = 18.176865\n",
      " 450 / 450 : loss = 23.519283Eopch:    58 mean_loss = 18.257659\n",
      " 450 / 450 : loss = 21.530693Eopch:    59 mean_loss = 18.121047\n",
      " 450 / 450 : loss = 23.203978Eopch:    60 mean_loss = 18.101285\n",
      " 450 / 450 : loss = 23.013775Eopch:    61 mean_loss = 18.115460\n",
      " 450 / 450 : loss = 21.095531Eopch:    62 mean_loss = 18.259166\n",
      " 450 / 450 : loss = 21.315319Eopch:    63 mean_loss = 18.413613\n",
      " 450 / 450 : loss = 20.947634Eopch:    64 mean_loss = 18.177209\n",
      " 450 / 450 : loss = 22.670574Eopch:    65 mean_loss = 18.123955\n",
      " 450 / 450 : loss = 23.207003Eopch:    66 mean_loss = 18.324529\n",
      " 450 / 450 : loss = 22.710808Eopch:    67 mean_loss = 18.098304\n",
      " 450 / 450 : loss = 23.902393Eopch:    68 mean_loss = 18.353705\n",
      " 450 / 450 : loss = 22.673859Eopch:    69 mean_loss = 18.265586\n",
      " 450 / 450 : loss = 22.780214Eopch:    70 mean_loss = 18.288978\n",
      " 450 / 450 : loss = 21.691338Eopch:    71 mean_loss = 18.326233\n",
      " 450 / 450 : loss = 22.779434Eopch:    72 mean_loss = 17.982814\n",
      " 450 / 450 : loss = 23.089169Eopch:    73 mean_loss = 18.122734\n",
      " 450 / 450 : loss = 22.557556Eopch:    74 mean_loss = 17.959575\n",
      " 450 / 450 : loss = 23.011356Eopch:    75 mean_loss = 18.009532\n",
      " 450 / 450 : loss = 22.555191Eopch:    76 mean_loss = 18.186050\n",
      " 450 / 450 : loss = 23.437904Eopch:    77 mean_loss = 18.071084\n",
      " 450 / 450 : loss = 19.250319Eopch:    78 mean_loss = 18.082941\n",
      " 450 / 450 : loss = 19.613403Eopch:    79 mean_loss = 17.983665\n",
      " 450 / 450 : loss = 19.806217Eopch:    80 mean_loss = 18.029433\n",
      " 450 / 450 : loss = 21.038477Eopch:    81 mean_loss = 17.841233\n",
      " 450 / 450 : loss = 23.937094Eopch:    82 mean_loss = 17.966862\n",
      " 450 / 450 : loss = 21.698572Eopch:    83 mean_loss = 17.976378\n",
      " 450 / 450 : loss = 21.184578Eopch:    84 mean_loss = 17.863359\n",
      " 450 / 450 : loss = 21.157452Eopch:    85 mean_loss = 17.869509\n",
      " 450 / 450 : loss = 20.998825Eopch:    86 mean_loss = 17.926890\n",
      " 450 / 450 : loss = 21.926403Eopch:    87 mean_loss = 18.041429\n",
      " 450 / 450 : loss = 21.225704Eopch:    88 mean_loss = 18.004037\n",
      " 450 / 450 : loss = 21.473232Eopch:    89 mean_loss = 17.951193\n",
      " 450 / 450 : loss = 22.820663Eopch:    90 mean_loss = 17.968776\n",
      " 450 / 450 : loss = 22.401058Eopch:    91 mean_loss = 17.990700\n",
      " 450 / 450 : loss = 23.419249Eopch:    92 mean_loss = 17.971058\n",
      " 450 / 450 : loss = 21.048637Eopch:    93 mean_loss = 17.817879\n",
      " 450 / 450 : loss = 19.891385Eopch:    94 mean_loss = 17.746813\n",
      " 450 / 450 : loss = 21.513489Eopch:    95 mean_loss = 17.743067\n",
      " 450 / 450 : loss = 19.770006Eopch:    96 mean_loss = 17.667963\n",
      " 450 / 450 : loss = 21.512043Eopch:    97 mean_loss = 17.703197\n",
      " 450 / 450 : loss = 21.397203Eopch:    98 mean_loss = 17.724789\n",
      " 450 / 450 : loss = 20.640263Eopch:    99 mean_loss = 17.604835\n",
      " 450 / 450 : loss = 19.387691Eopch:   100 mean_loss = 17.627072\n",
      "best_loss = 17.604835\n",
      " 89 / 90 0 mHR@5=0.331556, mAP@5=0.282467, mRR@5=0.739549\n",
      "mHR@10=0.337667, mAP@10=0.259801, mRR@10=0.625151\n",
      "mHR@15=0.343259, mAP@15=0.249356, mRR@15=0.580161\n",
      "mHR@20=0.342500, mAP@20=0.238503, mRR@20=0.545828\n"
     ]
    }
   ],
   "source": [
    "#define Hashing network with pytorch\n",
    "class HNet(nn.Module): \n",
    "    def __init__(self,inChannels=3, outHashcode=16):\n",
    "        super(HNet, self).__init__()\n",
    "        #(channels, Height, Width)\n",
    "        #layer1: Convolution, (3,512,512)->(8,256,256)\n",
    "        self.conv1 = nn.Conv2d(in_channels=inChannels, out_channels=8, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        #layer2: max pooling,(8,256,256)->(8,128,128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        #layer: spatial attention, pass\n",
    "        #layer3: Convolution, (8,128,128)->(2,64,64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=2, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #layer4: mean pooling, (2,64,64)->(2,32,32)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(2)\n",
    "        #layer5: fully connected, 2*32*32->512\n",
    "        self.fcl1 = nn.Linear(2*32*32,512)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        #layer6: Hashing layer, 512->16\n",
    "        self.fcl2 = nn.Linear(512,outHashcode)#\n",
    "        self.tanh = nn.Tanh() #{-1,1}\n",
    "              \n",
    "    def forward(self,x):\n",
    "        #input: (batch_size, in_channels, Height, Width)\n",
    "        #output: (batch_size, out_channels, Height, Width)\n",
    "        #layer1: convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        #layer2: max pooling\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        #layer: attention,pass\n",
    "        #layer3: Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu2(x)\n",
    "        #layer4: mean pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = self.bn4(x)\n",
    "        #layer5:fully connected\n",
    "        x = x.view(x.size(0),-1) #transfer three dims to one dim\n",
    "        x = self.fcl1(x)\n",
    "        x = self.relu3(x)\n",
    "        #layer6: Hashing layer\n",
    "        x = self.fcl2(x)\n",
    "        x = self.tanh(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "#https://pytorch-cn.readthedocs.io/zh/latest/    \n",
    "#https://github.com/filipradenovic/cnnimageretrieval-pytorch/blob/master/cirtorch/layers/functional.py\n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "    \n",
    "    def forward(self,h1,h2,y): \n",
    "        #h1=h2:NxD,y:N\n",
    "        dim = h1.shape[1]\n",
    "        euc_dist = F.pairwise_distance(h1, h2, p=2, eps=1e-06) # Calcualte Euclidean Distance\n",
    "        sim_term = 0.5*(1-y)*euc_dist #penalize the similar iamge pairs when y=0\n",
    "        unsim_term = 0.5*y*torch.clamp(self.margin*dim-euc_dist,0)#penalize the unsimlar image pairs when y =1\n",
    "        reg_term = self.alpha * ( torch.sum((torch.abs(h1)-1),dim=1) + torch.sum((torch.abs(h2)-1),dim=1) ) #regularization term\n",
    "        #loss = torch.mean(sim_term + unsim_term + reg_term) \n",
    "        loss = torch.sum(sim_term + unsim_term+ reg_term) \n",
    "        return loss\n",
    "\n",
    "#define model\n",
    "model = HNet(outHashcode=32).cuda()\n",
    "criterion  = HashLossFunc(margin=0.3).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "num_batches = len(trY_sf) // batchSize\n",
    "for epoch in range(100):#iteration\n",
    "    losses = []\n",
    "    for i in range(num_batches):   \n",
    "        optimizer.zero_grad() #grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(best_net(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(best_net(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#train data with list: trData, trI, trF, trY\n",
    "#test data with list: teData, teI, teF, teY\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        map_item_score = {}\n",
    "        for j, trVal in enumerate(trF):\n",
    "            map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in ranklist:\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#3 Attention-based Hashing Network:AHNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 450 / 450 : loss = 27.219505Eopch:     1 mean_loss = 21.167239\n",
      " 450 / 450 : loss = 24.781539Eopch:     2 mean_loss = 20.102185\n",
      " 450 / 450 : loss = 24.754116Eopch:     3 mean_loss = 19.836653\n",
      " 450 / 450 : loss = 25.758965Eopch:     4 mean_loss = 19.797267\n",
      " 450 / 450 : loss = 26.395975Eopch:     5 mean_loss = 19.706234\n",
      " 450 / 450 : loss = 27.093191Eopch:     6 mean_loss = 19.662256\n",
      " 450 / 450 : loss = 24.192425Eopch:     7 mean_loss = 19.812892\n",
      " 450 / 450 : loss = 23.147552Eopch:     9 mean_loss = 21.167607\n",
      " 450 / 450 : loss = 23.290432Eopch:    10 mean_loss = 20.121827\n",
      " 450 / 450 : loss = 23.735516Eopch:    11 mean_loss = 19.788363\n",
      " 450 / 450 : loss = 23.262829Eopch:    12 mean_loss = 19.568516\n",
      " 450 / 450 : loss = 23.717112Eopch:    13 mean_loss = 19.641504\n",
      " 450 / 450 : loss = 22.541138Eopch:    14 mean_loss = 19.323304\n",
      " 450 / 450 : loss = 22.510509Eopch:    15 mean_loss = 19.508521\n",
      " 450 / 450 : loss = 22.337872Eopch:    16 mean_loss = 19.511593\n",
      " 450 / 450 : loss = 21.815596Eopch:    17 mean_loss = 19.486257\n",
      " 450 / 450 : loss = 22.807022Eopch:    18 mean_loss = 19.229702\n",
      " 450 / 450 : loss = 23.399334Eopch:    19 mean_loss = 19.347901\n",
      " 450 / 450 : loss = 22.364199Eopch:    20 mean_loss = 19.076558\n",
      " 450 / 450 : loss = 22.027266Eopch:    21 mean_loss = 19.235757\n",
      " 450 / 450 : loss = 22.877079Eopch:    22 mean_loss = 19.223126\n",
      " 450 / 450 : loss = 23.146564Eopch:    23 mean_loss = 19.273631\n",
      " 450 / 450 : loss = 22.553965Eopch:    24 mean_loss = 19.220994\n",
      " 450 / 450 : loss = 21.932907Eopch:    25 mean_loss = 19.094739\n",
      " 450 / 450 : loss = 23.605928Eopch:    26 mean_loss = 19.049688\n",
      " 450 / 450 : loss = 22.367559Eopch:    27 mean_loss = 19.050245\n",
      " 450 / 450 : loss = 22.965839Eopch:    28 mean_loss = 19.276266\n",
      " 450 / 450 : loss = 22.861195Eopch:    29 mean_loss = 19.136047\n",
      " 450 / 450 : loss = 22.866776Eopch:    30 mean_loss = 19.151203\n",
      " 450 / 450 : loss = 22.322823Eopch:    31 mean_loss = 18.972182\n",
      " 450 / 450 : loss = 23.444172Eopch:    32 mean_loss = 18.953550\n",
      " 450 / 450 : loss = 23.546316Eopch:    33 mean_loss = 19.253227\n",
      " 450 / 450 : loss = 22.289907Eopch:    34 mean_loss = 19.033364\n",
      " 450 / 450 : loss = 22.808136Eopch:    35 mean_loss = 19.180122\n",
      " 450 / 450 : loss = 23.060284Eopch:    36 mean_loss = 19.086305\n",
      " 450 / 450 : loss = 21.772924Eopch:    37 mean_loss = 19.001409\n",
      " 450 / 450 : loss = 21.775955Eopch:    38 mean_loss = 18.987129\n",
      " 450 / 450 : loss = 21.809015Eopch:    39 mean_loss = 19.057855\n",
      " 450 / 450 : loss = 22.917366Eopch:    40 mean_loss = 18.999856\n",
      " 450 / 450 : loss = 22.380475Eopch:    41 mean_loss = 18.945058\n",
      " 450 / 450 : loss = 22.716753Eopch:    42 mean_loss = 19.039980\n",
      " 450 / 450 : loss = 23.515245Eopch:    43 mean_loss = 18.859163\n",
      " 450 / 450 : loss = 22.594349Eopch:    44 mean_loss = 18.936322\n",
      " 450 / 450 : loss = 23.414356Eopch:    45 mean_loss = 18.963005\n",
      " 450 / 450 : loss = 24.344044Eopch:    46 mean_loss = 18.712171\n",
      " 450 / 450 : loss = 22.038147Eopch:    47 mean_loss = 18.948233\n",
      " 450 / 450 : loss = 19.895054Eopch:    48 mean_loss = 19.146077\n",
      " 450 / 450 : loss = 20.933496Eopch:    49 mean_loss = 19.039752\n",
      " 450 / 450 : loss = 22.576012Eopch:    50 mean_loss = 18.954311\n",
      " 450 / 450 : loss = 20.762413Eopch:    51 mean_loss = 18.966271\n",
      " 450 / 450 : loss = 22.224796Eopch:    52 mean_loss = 18.972219\n",
      " 450 / 450 : loss = 20.604076Eopch:    53 mean_loss = 18.964828\n",
      " 450 / 450 : loss = 19.639263Eopch:    54 mean_loss = 18.925379\n",
      " 450 / 450 : loss = 21.050816Eopch:    55 mean_loss = 19.274479\n",
      " 450 / 450 : loss = 18.943048Eopch:    56 mean_loss = 19.306106\n",
      " 450 / 450 : loss = 19.411026Eopch:    57 mean_loss = 19.299189\n",
      " 450 / 450 : loss = 21.418449Eopch:    58 mean_loss = 19.297055\n",
      " 450 / 450 : loss = 20.047718Eopch:    59 mean_loss = 19.314551\n",
      " 450 / 450 : loss = 20.717491Eopch:    60 mean_loss = 19.188564\n",
      " 450 / 450 : loss = 20.621267Eopch:    61 mean_loss = 18.998851\n",
      " 450 / 450 : loss = 21.879688Eopch:    62 mean_loss = 18.861149\n",
      " 450 / 450 : loss = 22.108721Eopch:    63 mean_loss = 18.830841\n",
      " 450 / 450 : loss = 22.918098Eopch:    64 mean_loss = 19.269258\n",
      " 450 / 450 : loss = 22.682678Eopch:    65 mean_loss = 19.050057\n",
      " 450 / 450 : loss = 23.523935Eopch:    66 mean_loss = 18.953101\n",
      " 450 / 450 : loss = 23.824381Eopch:    67 mean_loss = 18.964662\n",
      " 450 / 450 : loss = 22.309626Eopch:    68 mean_loss = 19.114290\n",
      " 450 / 450 : loss = 21.923635Eopch:    69 mean_loss = 18.924820\n",
      " 450 / 450 : loss = 21.717648Eopch:    70 mean_loss = 18.756184\n",
      " 450 / 450 : loss = 22.396286Eopch:    71 mean_loss = 18.808909\n",
      " 450 / 450 : loss = 22.639946Eopch:    72 mean_loss = 18.740244\n",
      " 450 / 450 : loss = 23.446402Eopch:    73 mean_loss = 18.835660\n",
      " 450 / 450 : loss = 22.306156Eopch:    74 mean_loss = 18.976555\n",
      " 450 / 450 : loss = 23.528507Eopch:    75 mean_loss = 18.977192\n",
      " 450 / 450 : loss = 22.423103Eopch:    76 mean_loss = 18.818527\n",
      " 450 / 450 : loss = 22.803417Eopch:    78 mean_loss = 19.131494\n",
      " 450 / 450 : loss = 22.728846Eopch:    79 mean_loss = 18.982348\n",
      " 450 / 450 : loss = 22.093554Eopch:    80 mean_loss = 19.045082\n",
      " 450 / 450 : loss = 22.371752Eopch:    81 mean_loss = 18.789973\n",
      " 450 / 450 : loss = 22.784878Eopch:    82 mean_loss = 18.799186\n",
      " 450 / 450 : loss = 23.542187Eopch:    83 mean_loss = 19.155935\n",
      " 450 / 450 : loss = 23.213566Eopch:    84 mean_loss = 18.902847\n",
      " 450 / 450 : loss = 23.452463Eopch:    85 mean_loss = 18.812428\n",
      " 450 / 450 : loss = 22.020969Eopch:    86 mean_loss = 18.751485\n",
      " 450 / 450 : loss = 22.416317Eopch:    87 mean_loss = 18.715002\n",
      " 450 / 450 : loss = 22.874485Eopch:    88 mean_loss = 18.756905\n",
      " 450 / 450 : loss = 22.235207Eopch:    89 mean_loss = 18.773293\n",
      " 450 / 450 : loss = 22.358624Eopch:    90 mean_loss = 19.254989\n",
      " 450 / 450 : loss = 22.984283Eopch:    91 mean_loss = 19.169400\n",
      " 450 / 450 : loss = 21.986675Eopch:    92 mean_loss = 19.049390\n",
      " 450 / 450 : loss = 22.787411Eopch:    93 mean_loss = 19.056129\n",
      " 450 / 450 : loss = 22.229685Eopch:    94 mean_loss = 18.767007\n",
      " 450 / 450 : loss = 23.506292Eopch:    95 mean_loss = 18.781587\n",
      " 450 / 450 : loss = 23.973286Eopch:    96 mean_loss = 18.758449\n",
      " 450 / 450 : loss = 23.367931Eopch:    97 mean_loss = 18.982849\n",
      " 450 / 450 : loss = 23.830435Eopch:    98 mean_loss = 19.235665\n",
      " 450 / 450 : loss = 22.898924Eopch:    99 mean_loss = 19.118581\n",
      " 450 / 450 : loss = 24.565258Eopch:   100 mean_loss = 19.043618\n",
      "best_loss = 18.712171\n",
      " 89 / 90 0 mHR@5=0.354667, mAP@5=0.295456, mRR@5=0.726425\n",
      "mHR@10=0.354778, mAP@10=0.269561, mRR@10=0.628683\n",
      "mHR@15=0.354000, mAP@15=0.255083, mRR@15=0.585321\n"
     ]
    }
   ],
   "source": [
    "#define Attention-based Hashing network with pytorch\n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class AHNet(nn.Module): #deep Hashint Network:DHNet\n",
    "    def __init__(self,inChannels=3,outHashcode=16):\n",
    "        super(AHNet, self).__init__()\n",
    "        #(channels, Height, Width)\n",
    "        #layer1: Convolution, (3,512,512)->(8,256,256)\n",
    "        self.conv1 = nn.Conv2d(in_channels=inChannels, out_channels=8, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        #layer2: max pooling,(8,256,256)->(8,128,128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        #layer3: Spatial Attention Layer, (8,256,256)->(8,256,256)\n",
    "        self.sa = SpatialAttention()\n",
    "        #layer4: Convolution, (8,128,128)->(2,64,64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=2, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #layer5: mean pooling, (2,64,64)->(2,32,32)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(2)\n",
    "        #layer6: fully connected, 2*32*32->512\n",
    "        self.fcl1 = nn.Linear(2*32*32,512)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        #layer7: Hashing layer, 512->16\n",
    "        self.fcl2 = nn.Linear(512,outHashcode)#\n",
    "        self.tanh = nn.Tanh() #{-1,1}\n",
    "              \n",
    "    def forward(self,x):\n",
    "        #input: (batch_size, in_channels, Height, Width)\n",
    "        #output: (batch_size, out_channels, Height, Width)\n",
    "        #layer1: convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        #layer2: max pooling\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        #layer3: Attention\n",
    "        x = self.sa(x)*x\n",
    "        #layer4: Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu2(x)\n",
    "        #layer5: mean pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = self.bn4(x)\n",
    "        #layer6:fully connected\n",
    "        x = x.view(x.size(0),-1) #transfer three dims to one dim\n",
    "        x = self.fcl1(x)\n",
    "        x = self.relu3(x)\n",
    "        #layer7: Hashing layer\n",
    "        x = self.fcl2(x)\n",
    "        x = self.tanh(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "#https://pytorch-cn.readthedocs.io/zh/latest/    \n",
    "#https://github.com/filipradenovic/cnnimageretrieval-pytorch/blob/master/cirtorch/layers/functional.py\n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "    \n",
    "    def forward(self,h1,h2,y): \n",
    "        #h1=h2:NxD,y:N\n",
    "        dim = h1.shape[1]\n",
    "        euc_dist = F.pairwise_distance(h1, h2, p=2, eps=1e-06) # Calcualte Euclidean Distance\n",
    "        sim_term = 0.5*(1-y)*euc_dist #penalize the similar iamge pairs when y=0\n",
    "        unsim_term = 0.5*y*torch.clamp(self.margin*dim-euc_dist,0)#penalize the unsimlar image pairs when y =1\n",
    "        reg_term = self.alpha * ( torch.sum((torch.abs(h1)-1),dim=1) + torch.sum((torch.abs(h2)-1),dim=1) ) #regularization term\n",
    "        #loss = torch.mean(sim_term + unsim_term + reg_term) \n",
    "        loss = torch.sum(sim_term + unsim_term+ reg_term) \n",
    "        return loss\n",
    "    \n",
    "#define model\n",
    "model = AHNet(outHashcode=32).cuda()\n",
    "criterion  = HashLossFunc(margin=0.3).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "num_batches = len(trY_sf) // batchSize\n",
    "for epoch in range(100):#iteration\n",
    "    losses = []\n",
    "    for i in range(num_batches):   \n",
    "        optimizer.zero_grad() #grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(best_net(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(best_net(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#train data with list: trData, trI, trF, trY\n",
    "#test data with list: teData, teI, teF, teY\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        map_item_score = {}\n",
    "        for j, trVal in enumerate(trF):\n",
    "            map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in ranklist:\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate dataset\n",
    "dataset = pd.read_csv(\"/data/fjsdata/physionet/MIMIC-CXR/train.csv\" , sep=',')\n",
    "#print (dataset.columns)\n",
    "#print (dataset['view'].value_counts())\n",
    "dataset = dataset[dataset['view']=='frontal']#only get frontal view\n",
    "dataset = dataset[['path','Pneumonia']]\n",
    "#print (dataset['Pneumonia'].value_counts())\n",
    "trainset, testset = [], []\n",
    "ds =dataset[dataset['Pneumonia']==0.0].sample(3300).sample(frac=1)#26296-3300, negative\n",
    "testset.extend(np.array(ds).tolist()[0:300])\n",
    "trainset.extend(np.array(ds).tolist()[300:])\n",
    "ds =dataset[dataset['Pneumonia']==1.0].sample(3300).sample(frac=1)#18327-3300, positive\n",
    "testset.extend(np.array(ds).tolist()[0:300])\n",
    "trainset.extend(np.array(ds).tolist()[300:])\n",
    "ds =dataset[dataset['Pneumonia']==-1.0].sample(3300).sample(frac=1)#19681-3300, positive\n",
    "testset.extend(np.array(ds).tolist()[0:300])\n",
    "trainset.extend(np.array(ds).tolist()[300:])\n",
    "print (len(trainset))\n",
    "print (len(testset))\n",
    "pd.DataFrame(trainset).to_csv('/data/fjsdata/physionet/MIMIC-CXR/CBIR_train.csv',index=False)\n",
    "pd.DataFrame(testset).to_csv('/data/fjsdata/physionet/MIMIC-CXR/CBIR_test.csv',index=False)\n",
    "del ds,dataset\n",
    "gc.collect() #release cpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
