{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Develop Env: linux+cuda9+python3+opencv+pytorch\n",
    "2.Dataset: Fundus-DR https://www.kaggle.com/c/diabetic-retinopathy-detection/data\n",
    "        1)Label:0 - No DR,1 - Mild,2 - Moderate,3 - Severe,4 - Proliferative DR\n",
    "        2)Dataset: right eye, 0(3376),1(1231),2(2590),3(448),4(355), 7200 for train, 800 for test    \n",
    "3.Performance Metric: \n",
    "  1)MHR(Mean Hit Ratio):  for evaluating the precison of relevance retrieval;\n",
    "  2)MAP(Mean Average Precision): for evaluation the rank of relevance retrieval;\n",
    "  3)MRR(Mean Reciprocal Rank): for evaluation the first hit rank of relevance retrieval;\n",
    "  4)Memory consumption and Retrieval Speed.\n",
    "4.Algorithm: \n",
    "  1)Baseline: HNet\n",
    "  2)Attention: AHNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from scipy.spatial.distance import pdist\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "#import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.set_device(5)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200 / 7200 The length of train set is 7200\n",
      "748 / 800 "
     ]
    }
   ],
   "source": [
    "#1. Read data with List storage Name:[name],I:[img],Y[type]\n",
    "root_dir = '/data/fjsdata/fundus/kaggle_DR/train/' #the path of images\n",
    "trainset = pd.read_csv(\"/data/fjsdata/fundus/kaggle_DR/CBIR_train.csv\" , sep=',')#load dataset\n",
    "testset = pd.read_csv(\"/data/fjsdata/fundus/kaggle_DR/CBIR_test.csv\" , sep=',')#load dataset\n",
    "tstart = time.time()\n",
    "#read train image with CV\n",
    "trN, trI, trY = [],[],[]\n",
    "for iname, itype in np.array(trainset).tolist():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, iname+'.jpeg')\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (512, 512))#(1024,1024,3)->(512,512,3)\n",
    "        trN.append(iname)\n",
    "        trI.append(img)\n",
    "        trY.append(itype)  \n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(trN),trainset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(trN))\n",
    "#read test image with CV\n",
    "teN, teI, teY = [],[],[]\n",
    "for iname, itype in np.array(testset).tolist():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, iname+'.jpeg')\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (512, 512))#(1024,1024,3)->(512,512,3)\n",
    "        teN.append(iname)\n",
    "        teI.append(img)\n",
    "        teY.append(itype)  \n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(teN),testset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(teN))\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs(spls=len(trY)):\n",
    "    idx_sf = random.sample(range(0, len(trY)),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#2 Hashing Network:HNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 360 / 360 : loss = 10.508908Eopch:     1 mean_loss = 18.601788\n",
      " 360 / 360 : loss = 10.830853Eopch:     2 mean_loss = 18.005839\n",
      " 360 / 360 : loss = 9.4087773Eopch:     3 mean_loss = 18.384498\n",
      " 360 / 360 : loss = 13.852285Eopch:     4 mean_loss = 18.218214\n",
      " 360 / 360 : loss = 19.430454Eopch:     5 mean_loss = 18.000938\n",
      " 360 / 360 : loss = 14.515945Eopch:     6 mean_loss = 17.964972\n",
      " 360 / 360 : loss = 10.560654Eopch:     7 mean_loss = 18.094134\n",
      " 360 / 360 : loss = 14.166309Eopch:     8 mean_loss = 17.995287\n",
      " 360 / 360 : loss = 14.819332Eopch:     9 mean_loss = 17.928391\n",
      " 360 / 360 : loss = 16.939133Eopch:    10 mean_loss = 18.656372\n",
      " 360 / 360 : loss = 15.251892Eopch:    11 mean_loss = 18.415165\n",
      " 360 / 360 : loss = 13.906714Eopch:    12 mean_loss = 17.778392\n",
      " 360 / 360 : loss = 16.302053Eopch:    13 mean_loss = 18.324900\n",
      " 360 / 360 : loss = 13.901804Eopch:    14 mean_loss = 17.557915\n",
      " 360 / 360 : loss = 14.631466Eopch:    15 mean_loss = 17.862555\n",
      " 360 / 360 : loss = 15.975847Eopch:    16 mean_loss = 17.973379\n",
      " 360 / 360 : loss = 15.649207Eopch:    17 mean_loss = 18.151605\n",
      " 360 / 360 : loss = 10.748846Eopch:    18 mean_loss = 17.911886\n",
      " 360 / 360 : loss = 12.463402Eopch:    19 mean_loss = 17.867951\n",
      " 360 / 360 : loss = 13.974626Eopch:    20 mean_loss = 17.822418\n",
      " 360 / 360 : loss = 14.315523Eopch:    21 mean_loss = 17.788473\n",
      " 360 / 360 : loss = 17.658684Eopch:    22 mean_loss = 19.325739\n",
      " 360 / 360 : loss = 18.692585Eopch:    23 mean_loss = 22.329826\n",
      " 360 / 360 : loss = 17.323606Eopch:    24 mean_loss = 22.625817\n",
      " 360 / 360 : loss = 12.336088Eopch:    25 mean_loss = 21.309547\n",
      " 360 / 360 : loss = 13.865234Eopch:    26 mean_loss = 20.887124\n",
      " 360 / 360 : loss = 10.661642Eopch:    27 mean_loss = 21.399365\n",
      " 360 / 360 : loss = 11.831072Eopch:    28 mean_loss = 20.245203\n",
      " 360 / 360 : loss = 13.208338Eopch:    29 mean_loss = 19.146116\n",
      " 360 / 360 : loss = 11.271914Eopch:    30 mean_loss = 18.740980\n",
      " 360 / 360 : loss = 11.387699Eopch:    31 mean_loss = 19.133609\n",
      " 360 / 360 : loss = 10.093964Eopch:    32 mean_loss = 19.725183\n",
      " 360 / 360 : loss = 10.055031Eopch:    33 mean_loss = 20.179384\n",
      " 360 / 360 : loss = 13.111864Eopch:    34 mean_loss = 20.421780\n",
      " 360 / 360 : loss = 14.674068Eopch:    35 mean_loss = 19.530269\n",
      " 360 / 360 : loss = 17.803391Eopch:    36 mean_loss = 18.507448\n",
      " 360 / 360 : loss = 15.751184Eopch:    37 mean_loss = 18.541206\n",
      " 360 / 360 : loss = 19.964348Eopch:    38 mean_loss = 19.244226\n",
      " 360 / 360 : loss = 19.459347Eopch:    39 mean_loss = 19.261545\n",
      " 360 / 360 : loss = 20.098053Eopch:    40 mean_loss = 18.971355\n",
      " 360 / 360 : loss = 18.188032Eopch:    41 mean_loss = 18.637921\n",
      " 360 / 360 : loss = 17.825718Eopch:    42 mean_loss = 18.520014\n",
      " 360 / 360 : loss = 22.458202Eopch:    43 mean_loss = 18.563434\n",
      " 360 / 360 : loss = 20.333799Eopch:    44 mean_loss = 18.647074\n",
      " 360 / 360 : loss = 18.374449Eopch:    45 mean_loss = 18.573597\n",
      " 360 / 360 : loss = 20.282742Eopch:    46 mean_loss = 18.527265\n",
      " 360 / 360 : loss = 14.765471Eopch:    47 mean_loss = 18.312324\n",
      " 360 / 360 : loss = 16.887897Eopch:    48 mean_loss = 19.004137\n",
      " 360 / 360 : loss = 17.919388Eopch:    49 mean_loss = 19.188675\n",
      " 360 / 360 : loss = 14.244316Eopch:    50 mean_loss = 18.504706\n",
      " 360 / 360 : loss = 13.980556Eopch:    51 mean_loss = 18.334799\n",
      " 360 / 360 : loss = 16.491329Eopch:    52 mean_loss = 18.435528\n",
      " 360 / 360 : loss = 13.642392Eopch:    53 mean_loss = 18.358379\n",
      " 360 / 360 : loss = 14.460037Eopch:    54 mean_loss = 18.685363\n",
      " 360 / 360 : loss = 14.472783Eopch:    55 mean_loss = 19.074064\n",
      " 360 / 360 : loss = 15.707485Eopch:    56 mean_loss = 19.067157\n",
      " 360 / 360 : loss = 14.543748Eopch:    57 mean_loss = 19.561545\n",
      " 360 / 360 : loss = 15.000805Eopch:    58 mean_loss = 19.201404\n",
      " 360 / 360 : loss = 16.295406Eopch:    59 mean_loss = 18.674880\n",
      " 360 / 360 : loss = 15.980043Eopch:    60 mean_loss = 18.873870\n",
      " 360 / 360 : loss = 15.263942Eopch:    61 mean_loss = 18.788299\n",
      " 360 / 360 : loss = 15.783009Eopch:    62 mean_loss = 18.187703\n",
      " 360 / 360 : loss = 19.622585Eopch:    63 mean_loss = 18.402077\n",
      " 360 / 360 : loss = 20.926781Eopch:    64 mean_loss = 18.714331\n",
      " 360 / 360 : loss = 20.922199Eopch:    65 mean_loss = 18.581513\n",
      " 360 / 360 : loss = 24.252419Eopch:    66 mean_loss = 19.538264\n",
      " 360 / 360 : loss = 23.004566Eopch:    67 mean_loss = 18.901467\n",
      " 360 / 360 : loss = 21.264099Eopch:    68 mean_loss = 19.430651\n",
      " 360 / 360 : loss = 15.354572Eopch:    69 mean_loss = 18.396726\n",
      " 360 / 360 : loss = 16.689613Eopch:    70 mean_loss = 18.459036\n",
      " 360 / 360 : loss = 20.011123Eopch:    71 mean_loss = 18.740038\n",
      " 360 / 360 : loss = 18.974918Eopch:    72 mean_loss = 18.391013\n",
      " 360 / 360 : loss = 17.479431Eopch:    73 mean_loss = 18.302730\n",
      " 360 / 360 : loss = 18.826054Eopch:    74 mean_loss = 18.370175\n",
      " 360 / 360 : loss = 16.355333Eopch:    75 mean_loss = 18.934709\n",
      " 360 / 360 : loss = 17.885326Eopch:    76 mean_loss = 19.087153\n",
      " 360 / 360 : loss = 17.311209Eopch:    77 mean_loss = 18.179574\n",
      " 360 / 360 : loss = 12.653544Eopch:    78 mean_loss = 18.290611\n",
      " 360 / 360 : loss = 19.258057Eopch:    79 mean_loss = 18.838064\n",
      " 360 / 360 : loss = 13.865728Eopch:    80 mean_loss = 19.531645\n",
      " 360 / 360 : loss = 14.846051Eopch:    81 mean_loss = 18.664229\n",
      " 360 / 360 : loss = 12.209206Eopch:    82 mean_loss = 19.088287\n",
      " 360 / 360 : loss = 10.153407Eopch:    83 mean_loss = 19.324382\n",
      " 360 / 360 : loss = 11.404846Eopch:    84 mean_loss = 19.074489\n",
      " 360 / 360 : loss = 12.526115Eopch:    85 mean_loss = 18.882962\n",
      " 360 / 360 : loss = 11.860259Eopch:    86 mean_loss = 18.783466\n",
      " 360 / 360 : loss = 13.135502Eopch:    87 mean_loss = 18.645625\n",
      " 360 / 360 : loss = 12.095566Eopch:    88 mean_loss = 18.421793\n",
      " 360 / 360 : loss = 16.949133Eopch:    89 mean_loss = 18.485297\n",
      " 360 / 360 : loss = 15.543736Eopch:    90 mean_loss = 18.415662\n",
      " 360 / 360 : loss = 16.439278Eopch:    91 mean_loss = 18.780533\n",
      " 360 / 360 : loss = 12.314445Eopch:    93 mean_loss = 18.537232\n",
      " 360 / 360 : loss = 18.543985Eopch:    94 mean_loss = 18.756207\n",
      " 360 / 360 : loss = 20.901756Eopch:    95 mean_loss = 18.755773\n",
      " 360 / 360 : loss = 15.077165Eopch:    96 mean_loss = 18.841393\n",
      " 360 / 360 : loss = 12.973429Eopch:    97 mean_loss = 18.500157\n",
      " 360 / 360 : loss = 12.474823Eopch:    98 mean_loss = 18.568051\n",
      " 360 / 360 : loss = 13.108274Eopch:    99 mean_loss = 18.400453\n",
      " 360 / 360 : loss = 14.683525Eopch:   100 mean_loss = 18.125028\n",
      " 79 / 80 0 mHR@5=0.420500, mAP@5=0.417612, mRR@5=0.982949\n",
      "mHR@10=0.419875, mAP@10=0.415352, mRR@10=0.970802\n",
      "mHR@15=0.420167, mAP@15=0.412543, mRR@15=0.925061\n",
      "mHR@20=0.420750, mAP@20=0.410535, mRR@20=0.904473\n"
     ]
    }
   ],
   "source": [
    "#define Hashing network with pytorch\n",
    "class HNet(nn.Module): \n",
    "    def __init__(self,inChannels=3, outHashcode=16):\n",
    "        super(HNet, self).__init__()\n",
    "        #(channels, Height, Width)\n",
    "        #layer1: Convolution, (3,512,512)->(8,256,256)\n",
    "        self.conv1 = nn.Conv2d(in_channels=inChannels, out_channels=8, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        #layer2: max pooling,(8,256,256)->(8,128,128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        #layer: spatial attention, pass\n",
    "        #layer3: Convolution, (8,128,128)->(2,64,64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=2, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #layer4: mean pooling, (2,64,64)->(2,32,32)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(2)\n",
    "        #layer5: fully connected, 2*32*32->512\n",
    "        self.fcl1 = nn.Linear(2*32*32,512)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        #layer6: Hashing layer, 512->16\n",
    "        self.fcl2 = nn.Linear(512,outHashcode)#\n",
    "        self.tanh = nn.Tanh() #{-1,1}\n",
    "              \n",
    "    def forward(self,x):\n",
    "        #input: (batch_size, in_channels, Height, Width)\n",
    "        #output: (batch_size, out_channels, Height, Width)\n",
    "        #layer1: convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        #layer2: max pooling\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        #layer: attention,pass\n",
    "        #layer3: Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu2(x)\n",
    "        #layer4: mean pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = self.bn4(x)\n",
    "        #layer5:fully connected\n",
    "        x = x.view(x.size(0),-1) #transfer three dims to one dim\n",
    "        x = self.fcl1(x)\n",
    "        x = self.relu3(x)\n",
    "        #layer6: Hashing layer\n",
    "        x = self.fcl2(x)\n",
    "        x = self.tanh(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "#https://pytorch-cn.readthedocs.io/zh/latest/    \n",
    "#https://github.com/filipradenovic/cnnimageretrieval-pytorch/blob/master/cirtorch/layers/functional.py\n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "    \n",
    "    def forward(self,h1,h2,y): \n",
    "        #h1=h2:NxD,y:N\n",
    "        dim = h1.shape[1]\n",
    "        euc_dist = F.pairwise_distance(h1, h2, p=2, eps=1e-06) # Calcualte Euclidean Distance\n",
    "        sim_term = 0.5*(1-y)*euc_dist #penalize the similar iamge pairs when y=0\n",
    "        unsim_term = 0.5*y*torch.clamp(self.margin*dim-euc_dist,0)#penalize the unsimlar image pairs when y =1\n",
    "        reg_term = self.alpha * ( torch.sum((torch.abs(h1)-1),dim=1) + torch.sum((torch.abs(h2)-1),dim=1) ) #regularization term\n",
    "        #loss = torch.mean(sim_term + unsim_term + reg_term) \n",
    "        loss = torch.sum(sim_term + unsim_term+ reg_term) \n",
    "        return loss\n",
    "\n",
    "#define model\n",
    "model = HNet(outHashcode=16).cuda()\n",
    "criterion  = HashLossFunc(margin=0.5).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "#best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "num_batches = len(trY_sf) // batchSize\n",
    "for epoch in range(100):#iteration\n",
    "    losses = []\n",
    "    for i in range(num_batches): \n",
    "        optimizer.zero_grad() #grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    #if np.mean(losses) < best_loss:\n",
    "    #    best_loss = np.mean(losses)\n",
    "    #    best_net = copy.deepcopy(model)\n",
    "#print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "#model = model.cpu()\n",
    "#torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(model(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(model(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#train data with list: trData, trI, trF, trY\n",
    "#test data with list: teData, teI, teF, teY\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        map_item_score = {}\n",
    "        for j, trVal in enumerate(trF):\n",
    "            map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in ranklist:\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#3 Attention-based Hashing Network:AHNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 360 / 360 : loss = 18.012304Eopch:     1 mean_loss = 20.496477\n",
      " 360 / 360 : loss = 17.352184Eopch:     2 mean_loss = 22.143610\n",
      " 360 / 360 : loss = 17.239742Eopch:     3 mean_loss = 22.490541\n",
      " 360 / 360 : loss = 16.017452Eopch:     4 mean_loss = 22.915594\n",
      " 360 / 360 : loss = 16.550526Eopch:     5 mean_loss = 22.995458\n",
      " 360 / 360 : loss = 18.382084Eopch:     6 mean_loss = 23.045223\n",
      " 360 / 360 : loss = 19.171746Eopch:     7 mean_loss = 23.057716\n",
      " 360 / 360 : loss = 21.999998Eopch:     8 mean_loss = 23.458775\n",
      " 360 / 360 : loss = 19.819669Eopch:     9 mean_loss = 23.154140\n",
      " 360 / 360 : loss = 21.001526Eopch:    10 mean_loss = 23.502885\n",
      " 360 / 360 : loss = 21.789242Eopch:    11 mean_loss = 23.849968\n",
      " 360 / 360 : loss = 21.525826Eopch:    12 mean_loss = 22.827615\n",
      " 360 / 360 : loss = 20.862729Eopch:    13 mean_loss = 21.249288\n",
      " 360 / 360 : loss = 21.538021Eopch:    14 mean_loss = 23.495001\n",
      " 360 / 360 : loss = 20.247911Eopch:    15 mean_loss = 22.474646\n",
      " 360 / 360 : loss = 24.337097Eopch:    16 mean_loss = 22.229460\n",
      " 360 / 360 : loss = 24.270554Eopch:    17 mean_loss = 21.562996\n",
      " 360 / 360 : loss = 24.307873Eopch:    19 mean_loss = 22.500665\n",
      " 360 / 360 : loss = 20.945511Eopch:    20 mean_loss = 21.705124\n",
      " 360 / 360 : loss = 19.755361Eopch:    21 mean_loss = 21.675753\n",
      " 360 / 360 : loss = 23.339127Eopch:    22 mean_loss = 20.860662\n",
      " 360 / 360 : loss = 18.339199Eopch:    23 mean_loss = 21.099543\n",
      " 360 / 360 : loss = 15.904354Eopch:    24 mean_loss = 21.350273\n",
      " 360 / 360 : loss = 21.729567Eopch:    25 mean_loss = 21.649671\n",
      " 360 / 360 : loss = 21.581171Eopch:    26 mean_loss = 21.951668\n",
      " 360 / 360 : loss = 20.351164Eopch:    27 mean_loss = 21.444354\n",
      " 360 / 360 : loss = 20.697395Eopch:    28 mean_loss = 21.427057\n",
      " 360 / 360 : loss = 21.376287Eopch:    29 mean_loss = 21.144876\n",
      " 360 / 360 : loss = 13.493748Eopch:    30 mean_loss = 21.079668\n",
      " 360 / 360 : loss = 16.386131Eopch:    31 mean_loss = 20.886211\n",
      " 360 / 360 : loss = 15.808496Eopch:    32 mean_loss = 21.163770\n",
      " 360 / 360 : loss = 15.983615Eopch:    33 mean_loss = 21.344461\n",
      " 360 / 360 : loss = 17.160748Eopch:    34 mean_loss = 21.721621\n",
      " 360 / 360 : loss = 17.695107Eopch:    35 mean_loss = 21.647575\n",
      " 360 / 360 : loss = 17.857864Eopch:    36 mean_loss = 21.826057\n",
      " 360 / 360 : loss = 15.029441Eopch:    37 mean_loss = 21.513873\n",
      " 360 / 360 : loss = 15.124465Eopch:    38 mean_loss = 21.298374\n",
      " 360 / 360 : loss = 18.073305Eopch:    39 mean_loss = 21.448564\n",
      " 360 / 360 : loss = 18.599667Eopch:    40 mean_loss = 22.425115\n",
      " 360 / 360 : loss = 18.512043Eopch:    41 mean_loss = 22.444417\n",
      " 360 / 360 : loss = 19.526958Eopch:    42 mean_loss = 22.154757\n",
      " 360 / 360 : loss = 21.597238Eopch:    43 mean_loss = 21.765413\n",
      " 360 / 360 : loss = 19.735188Eopch:    44 mean_loss = 21.741136\n",
      " 360 / 360 : loss = 19.512348Eopch:    45 mean_loss = 21.348168\n",
      " 360 / 360 : loss = 18.730736Eopch:    46 mean_loss = 21.837400\n",
      " 360 / 360 : loss = 18.022942Eopch:    47 mean_loss = 21.809125\n",
      " 360 / 360 : loss = 14.942818Eopch:    48 mean_loss = 21.690482\n",
      " 360 / 360 : loss = 15.122267Eopch:    49 mean_loss = 21.613757\n",
      " 360 / 360 : loss = 17.945183Eopch:    50 mean_loss = 21.145988\n",
      " 360 / 360 : loss = 16.626326Eopch:    51 mean_loss = 21.159182\n",
      " 360 / 360 : loss = 16.673221Eopch:    52 mean_loss = 21.097597\n",
      " 360 / 360 : loss = 14.443652Eopch:    53 mean_loss = 20.751989\n",
      " 360 / 360 : loss = 14.902618Eopch:    54 mean_loss = 20.539807\n",
      " 360 / 360 : loss = 18.896828Eopch:    55 mean_loss = 20.786080\n",
      " 360 / 360 : loss = 15.295703Eopch:    56 mean_loss = 20.495462\n",
      " 360 / 360 : loss = 15.753965Eopch:    57 mean_loss = 21.393672\n",
      " 360 / 360 : loss = 17.719219Eopch:    58 mean_loss = 21.261032\n",
      " 360 / 360 : loss = 16.654079Eopch:    59 mean_loss = 21.289141\n",
      " 360 / 360 : loss = 17.587036Eopch:    60 mean_loss = 20.274401\n",
      " 360 / 360 : loss = 17.006048Eopch:    61 mean_loss = 20.486145\n",
      " 360 / 360 : loss = 15.753257Eopch:    62 mean_loss = 20.288199\n",
      " 360 / 360 : loss = 20.223852Eopch:    63 mean_loss = 20.138755\n",
      " 360 / 360 : loss = 17.168179Eopch:    64 mean_loss = 20.531178\n",
      " 360 / 360 : loss = 20.349716Eopch:    65 mean_loss = 20.618369\n",
      " 360 / 360 : loss = 15.515857Eopch:    66 mean_loss = 20.678063\n",
      " 360 / 360 : loss = 19.108212Eopch:    67 mean_loss = 20.480409\n",
      " 360 / 360 : loss = 16.415972Eopch:    68 mean_loss = 20.875895\n",
      " 360 / 360 : loss = 19.251083Eopch:    69 mean_loss = 20.926605\n",
      " 360 / 360 : loss = 19.008327Eopch:    70 mean_loss = 21.268627\n",
      " 360 / 360 : loss = 17.790377Eopch:    71 mean_loss = 20.916199\n",
      " 360 / 360 : loss = 18.303339Eopch:    72 mean_loss = 20.607153\n",
      " 360 / 360 : loss = 17.163767Eopch:    73 mean_loss = 20.577477\n",
      " 360 / 360 : loss = 18.589001Eopch:    74 mean_loss = 20.463428\n",
      " 360 / 360 : loss = 15.981092Eopch:    75 mean_loss = 20.357078\n",
      " 360 / 360 : loss = 17.810425Eopch:    76 mean_loss = 20.497999\n",
      " 360 / 360 : loss = 17.769712Eopch:    77 mean_loss = 20.402831\n",
      " 360 / 360 : loss = 16.983135Eopch:    78 mean_loss = 20.252460\n",
      " 360 / 360 : loss = 16.350969Eopch:    79 mean_loss = 20.855693\n",
      " 360 / 360 : loss = 16.904762Eopch:    80 mean_loss = 21.408524\n",
      " 360 / 360 : loss = 18.464071Eopch:    81 mean_loss = 21.422707\n",
      " 360 / 360 : loss = 15.574588Eopch:    82 mean_loss = 20.841670\n",
      " 360 / 360 : loss = 15.097248Eopch:    83 mean_loss = 20.905425\n",
      " 360 / 360 : loss = 17.121473Eopch:    84 mean_loss = 20.639762\n",
      " 360 / 360 : loss = 16.283989Eopch:    85 mean_loss = 20.629707\n",
      " 360 / 360 : loss = 16.820108Eopch:    86 mean_loss = 20.405736\n",
      " 360 / 360 : loss = 16.453407Eopch:    87 mean_loss = 20.209938\n",
      " 360 / 360 : loss = 16.040947Eopch:    88 mean_loss = 20.434515\n",
      " 360 / 360 : loss = 15.524022Eopch:    89 mean_loss = 20.425435\n",
      " 360 / 360 : loss = 15.706696Eopch:    90 mean_loss = 20.359980\n",
      " 360 / 360 : loss = 16.169765Eopch:    91 mean_loss = 20.459589\n",
      " 360 / 360 : loss = 15.009295Eopch:    92 mean_loss = 20.336181\n",
      " 360 / 360 : loss = 16.962402Eopch:    93 mean_loss = 20.105923\n",
      " 360 / 360 : loss = 16.668873Eopch:    94 mean_loss = 20.287381\n",
      " 360 / 360 : loss = 16.546406Eopch:    95 mean_loss = 20.457271\n",
      " 360 / 360 : loss = 16.327133Eopch:    96 mean_loss = 20.568152\n",
      " 360 / 360 : loss = 16.665634Eopch:    97 mean_loss = 20.348460\n",
      " 360 / 360 : loss = 17.268858Eopch:    98 mean_loss = 20.216445\n",
      " 360 / 360 : loss = 18.322527Eopch:    99 mean_loss = 20.115985\n",
      " 360 / 360 : loss = 16.651029Eopch:   100 mean_loss = 19.814383\n",
      " 79 / 80 0 mHR@5=0.419750, mAP@5=0.417958, mRR@5=0.989443\n",
      "mHR@10=0.419375, mAP@10=0.416621, mRR@10=0.982060\n",
      "mHR@15=0.418333, mAP@15=0.415275, mRR@15=0.976796\n",
      "mHR@20=0.418438, mAP@20=0.413850, mRR@20=0.963651\n"
     ]
    }
   ],
   "source": [
    "#define Attention-based Hashing network with pytorch\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_channels=in_planes, out_channels=in_planes // 2, kernel_size=1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_channels=in_planes // 2, out_channels=in_planes, kernel_size=1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class AHNet(nn.Module): #deep Hashint Network:DHNet\n",
    "    def __init__(self,inChannels=3,outHashcode=16):\n",
    "        super(AHNet, self).__init__()\n",
    "        #(channels, Height, Width)\n",
    "        #layer1: Convolution, (3,512,512)->(8,256,256)\n",
    "        self.conv1 = nn.Conv2d(in_channels=inChannels, out_channels=8, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        #layer2: max pooling,(8,256,256)->(8,128,128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        #layer3: Channel and Spatial Attention Layer, (8,256,256)->(8,256,256)\n",
    "        self.ca = ChannelAttention(8)\n",
    "        #self.sa = SpatialAttention()\n",
    "        #layer4: Convolution, (8,128,128)->(2,64,64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=2, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #layer5: mean pooling, (2,64,64)->(2,32,32)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(2)\n",
    "        #layer6: fully connected, 2*32*32->512\n",
    "        self.fcl1 = nn.Linear(2*32*32,512)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        #layer7: Hashing layer, 512->16\n",
    "        self.fcl2 = nn.Linear(512,outHashcode)#\n",
    "        self.tanh = nn.Tanh() #{-1,1}\n",
    "              \n",
    "    def forward(self,x):\n",
    "        #input: (batch_size, in_channels, Height, Width)\n",
    "        #output: (batch_size, out_channels, Height, Width)\n",
    "        #layer1: convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        #layer2: max pooling\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        #layer3: Attention\n",
    "        x = self.ca(x) * x  #channel\n",
    "        #x = self.sa(x) * x  #spatial\n",
    "        #layer4: Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu2(x)\n",
    "        #layer5: mean pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = self.bn4(x)\n",
    "        #layer6:fully connected\n",
    "        x = x.view(x.size(0),-1) #transfer three dims to one dim\n",
    "        x = self.fcl1(x)\n",
    "        x = self.relu3(x)\n",
    "        #layer7: Hashing layer\n",
    "        x = self.fcl2(x)\n",
    "        x = self.tanh(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "#https://pytorch-cn.readthedocs.io/zh/latest/    \n",
    "#https://github.com/filipradenovic/cnnimageretrieval-pytorch/blob/master/cirtorch/layers/functional.py\n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "    \n",
    "    def forward(self,h1,h2,y): \n",
    "        #h1=h2:NxD,y:N\n",
    "        dim = h1.shape[1]\n",
    "        euc_dist = F.pairwise_distance(h1, h2, p=2, eps=1e-06) # Calcualte Euclidean Distance\n",
    "        sim_term = 0.5*(1-y)*euc_dist #penalize the similar iamge pairs when y=0\n",
    "        unsim_term = 0.5*y*torch.clamp(self.margin*dim-euc_dist,0)#penalize the unsimlar image pairs when y =1\n",
    "        reg_term = self.alpha * ( torch.sum((torch.abs(h1)-1),dim=1) + torch.sum((torch.abs(h2)-1),dim=1) ) #regularization term\n",
    "        #loss = torch.mean(sim_term + unsim_term + reg_term) \n",
    "        loss = torch.sum(sim_term + unsim_term+ reg_term) \n",
    "        return loss\n",
    "    \n",
    "#define model\n",
    "model = AHNet(outHashcode=16).cuda()\n",
    "criterion  = HashLossFunc(margin=0.5).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "#best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "num_batches = len(trY_sf) // batchSize\n",
    "for epoch in range(100):#iteration\n",
    "    losses = []\n",
    "    for i in range(num_batches):   \n",
    "        optimizer.zero_grad() #grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    #if np.mean(losses) < best_loss:\n",
    "        #best_loss = np.mean(losses)\n",
    "        #best_net = copy.deepcopy(model)\n",
    "#print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "#model = model.cpu()\n",
    "#torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(model(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = torch.sign(model(I_batch.permute(0, 3, 1, 2)))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#train data with list: trData, trI, trF, trY\n",
    "#test data with list: teData, teI, teF, teY\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        map_item_score = {}\n",
    "        for j, trVal in enumerate(trF):\n",
    "            map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in ranklist:\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17563, 2)\n",
      "0    12939\n",
      "2     2590\n",
      "1     1231\n",
      "3      448\n",
      "4      355\n",
      "Name: level, dtype: int64\n",
      "7200\n",
      "800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate dataset\n",
    "dataset = pd.read_csv(\"/data/fjsdata/fundus/kaggle_DR/trainLabels.csv\" , sep=',')#load dataset\n",
    "trainset, testset = [], []\n",
    "#dataset = dataset[dataset['image'].contains('left')]\n",
    "tf = dataset['image'].str.contains('right')#right eye\n",
    "dataset = dataset[tf]\n",
    "print (dataset.shape)\n",
    "print (dataset['level'].value_counts())\n",
    "ds =dataset[dataset['level']==0].sample(3376).sample(frac=1) #0:3376-338,shuffle\n",
    "testset.extend(np.array(ds).tolist()[0:339])\n",
    "trainset.extend(np.array(ds).tolist()[339:])\n",
    "ds =dataset[dataset['level']==1].sample(frac=1)#1:1231-123,shuffle\n",
    "testset.extend(np.array(ds).tolist()[0:123])\n",
    "trainset.extend(np.array(ds).tolist()[123:])\n",
    "ds =dataset[dataset['level']==2].sample(frac=1)#2:2590-259,shuffle\n",
    "testset.extend(np.array(ds).tolist()[0:259])\n",
    "trainset.extend(np.array(ds).tolist()[259:])\n",
    "ds =dataset[dataset['level']==3].sample(frac=1)#3:448-44,shuffle\n",
    "testset.extend(np.array(ds).tolist()[0:44])\n",
    "trainset.extend(np.array(ds).tolist()[44:])\n",
    "ds =dataset[dataset['level']==4].sample(frac=1)#4:355-35,shuffle\n",
    "testset.extend(np.array(ds).tolist()[0:35])\n",
    "trainset.extend(np.array(ds).tolist()[35:])\n",
    "print (len(trainset))\n",
    "print (len(testset))\n",
    "pd.DataFrame(trainset).to_csv('/data/fjsdata/fundus/kaggle_DR/CBIR_train.csv',index=False)\n",
    "pd.DataFrame(testset).to_csv('/data/fjsdata/fundus/kaggle_DR/CBIR_test.csv',index=False)\n",
    "del ds,dataset\n",
    "gc.collect() #release cpu memory"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "outputs:\n",
    "1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
