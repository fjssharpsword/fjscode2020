{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Develop Env: linux+cuda9+python3+opencv+pytorch+faiss\n",
    "2.Dataset: Fundus-iSee with 10000 images(AMD-720, DR-270, glaucoma-450,myopia-790,norm-7770)\n",
    "        trainset(9000): AMD-648, DR-243, glaucoma-405, myopia-711, norm-6993, \n",
    "        testset(1000): AMD-72, DR-27, glaucoma-45, myopia-79, norm=777\n",
    "3.Performance Metric: \n",
    "  1)MHR(Mean Hit Ratio):  for evaluating the precison of relevance retrieval;\n",
    "  2)MAP(Mean Average Precision): for evaluation the rank of relevance retrieval;\n",
    "  3)MRR(Mean Reciprocal Rank): for evaluation the first hit rank of relevance retrieval;\n",
    "  4)Memory consumption and Retrieval Speed.\n",
    "4.Algorithm: \n",
    "  1)Baseline: CNN-FCL,CNN-HL\n",
    "  2)Attention: CNN-AH\n",
    "  3)effectiveness: CNN-FCL<CNN-HL<CNN-AH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "9.0.176\n",
      "TITAN Xp\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "#import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "print (torch.cuda.is_available())\n",
    "print (torch.version.cuda)\n",
    "print (torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "torch.cuda.set_device(6)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1. Read data with List storage [name,type,features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 / 9000 The length of train set is 9000\n",
      "1000 / 1000 The length of test set is 1000\n",
      "Completed buliding index in 1981 seconds\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/data/fjsdata/fundus/iSee/iSee_multi_dataset/' #the path of images\n",
    "trainset = pd.read_csv(\"/data/fjsdata/fundus/iSee/iSee_multi_dataset/CBIR_iSee_train.csv\" , sep=',')#load trainset\n",
    "testset = pd.read_csv(\"/data/fjsdata/fundus/iSee/iSee_multi_dataset/CBIR_iSee_test.csv\" , sep=',')#load testset\n",
    "tstart = time.time()\n",
    "#read train image with CV\n",
    "trData = []\n",
    "for iname, itype in np.array(trainset).tolist():\n",
    "    if iname.endswith(\".jpg\"):\n",
    "        try:\n",
    "            image_dir = root_dir+'img_data_%s'%itype\n",
    "            image_path = os.path.join(image_dir, iname)\n",
    "            img = cv2.resize(cv2.imread(image_path).astype(np.float32), (1024, 1024))#(1920,1920,3)->(1024,1024,3)\n",
    "            trData.append([iname,itype,img])\n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trData),trainset.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(trData))\n",
    "#read test image with CV\n",
    "teData = []\n",
    "for iname, itype in np.array(testset).tolist():\n",
    "    if iname.endswith(\".jpg\"):\n",
    "        try:\n",
    "            image_dir = root_dir+'img_data_%s'%itype\n",
    "            image_path = os.path.join(image_dir, iname)\n",
    "            img = cv2.resize(cv2.imread(image_path), (1024, 1024))#(1920,1920,3)->(1024,1024,3)\n",
    "            teData.append([iname,itype,img])\n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(teData),testset.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "print('The length of test set is %d'%len(teData))\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#2. define attention-based hashing network with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class AttHashNet(nn.Module): #Attention-based Hashint Network:AHNet\n",
    "    def __init__(self,inChannels=3):\n",
    "        super(AttHashNet, self).__init__()\n",
    "        #(channels, Height, Width)\n",
    "        #layer1: Convolution, (3,1024,1024)->(16,1024,1024)\n",
    "        self.conv1 = nn.Conv2d(in_channels=inChannels, out_channels=16, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        #layer2: max pooling,(16,1024,1024)->(16,512,512)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        #layer3: Spatial Attention Layer, (16,512,512)->(16,512,512)\n",
    "        self.sa = SpatialAttention()\n",
    "        #layer4: Convolution, (16,512,512)->(8,256,256)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(8)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #layer5: mean pooling, (8,256,256)->(8,128,128)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(8)\n",
    "        #layer6: fully connected, (8,128,128)->(4*64*64)\n",
    "        self.fcl = nn.Conv2d(in_channels = 8, out_channels=4, kernel_size=1, stride=2)\n",
    "        self.bn5 = nn.BatchNorm2d(4)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        #layer7: hash layer, binary-likeï¼Œ(4*64*64)->(1*32*32)\n",
    "        self.hl = nn.Conv2d(in_channels = 4, out_channels=1, kernel_size=1, stride=2)\n",
    "        self.bn6 = nn.BatchNorm2d(1)\n",
    "        self.tanh = nn.Tanh()#hyberpolic tangent activation\n",
    "              \n",
    "    def forward(self,x):\n",
    "        #input: (batch_size, in_channels, Height, Width)\n",
    "        #output: (batch_size, out_channels, Height, Width)\n",
    "        #layer1: convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        #layer2: max pooling\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        #layer3: Attention\n",
    "        x = self.sa(x)*x\n",
    "        #layer4: Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu2(x)\n",
    "        #layer5: mean pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = self.bn4(x)\n",
    "        #layer6: fully connected\n",
    "        x = self.fcl(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu3(x)\n",
    "        #layer7: hash layer\n",
    "        x = self.hl(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.tanh(x) \n",
    "        \n",
    "        return x\n",
    "    \n",
    "#https://pytorch-cn.readthedocs.io/zh/latest/    \n",
    "#https://github.com/filipradenovic/cnnimageretrieval-pytorch/blob/master/cirtorch/layers/functional.py\n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self,margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "    \n",
    "    def forward(self,h1,h2,y): \n",
    "        #h1=h2:NxD,y:N\n",
    "        dim = h1.shape[1]*h1.shape[2]*h1.shape[3]\n",
    "        h1 = h1.reshape(h1.shape[0],dim)\n",
    "        h2 = h2.reshape(h2.shape[0],dim)\n",
    "        euc_dist = F.pairwise_distance(h1, h2, p=2, eps=1e-06) # Calcualte Euclidean Distance\n",
    "        sim_term = 0.5*(1-y)*euc_dist #penalize the similar iamge pairs when y=0\n",
    "        unsim_term = 0.5*y*torch.clamp(self.margin*dim-euc_dist,0)#penalize the unsimlar image pairs when y =1\n",
    "        reg_term = self.alpha * ( torch.sum((torch.abs(h1)-1),dim=1) + torch.sum((torch.abs(h2)-1),dim=1) ) #regularization term\n",
    "        #loss = torch.mean(sim_term + unsim_term + reg_term) \n",
    "        loss = torch.sum(sim_term + unsim_term + reg_term) \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1162.296630859375\n"
     ]
    }
   ],
   "source": [
    "#test network\n",
    "x1 = torch.rand(10,3,1024,1024).cuda()\n",
    "x2 = torch.rand(10,3,1024,1024).cuda()\n",
    "y = torch.FloatTensor([0,1,1,0,1,0,0,0,1,1]).cuda()\n",
    "model = AttHashNet().cuda()\n",
    "out1 = model(x1)#out.grad_fn\n",
    "out2 = model(x2)\n",
    "criterion  = HashLossFunc().cuda() #define loss function\n",
    "loss = criterion(out1,out2,y)\n",
    "print (loss.item())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "x3 = torch.rand(10,3,1024,1024).cuda()\n",
    "out3 = model(x3)\n",
    "out3 = torch.sign(out3) #Binarization,[-1,1]->{-1,1}\n",
    "#print (out3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1799 / 1801 : loss = 1074.900391"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ba1b46a2e267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mY_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI1_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#adjust channel to the second\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI2_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(100,1024,1024,3)->(100,3,1024,1024)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "#train list to numpy\n",
    "trData = np.array(trData) \n",
    "#define model\n",
    "model = AttHashNet().cuda()\n",
    "criterion  = HashLossFunc().cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train parameters\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    #prepare train data\n",
    "    I1,I2,Y =[],[],[] #list of training data\n",
    "    for itype in ['AMD','DR','glaucoma','myopia','norm']:\n",
    "        trType = np.asarray([x for x in trData if x[1]==itype ])\n",
    "        trTypeList = trType.tolist() #numpy to list\n",
    "        trNType = np.asarray([x for x in trData if x[1]!=itype ])\n",
    "        trNTypeList = trNType.tolist()\n",
    "        for x in trType:\n",
    "            sim = random.sample(trTypeList, 1) \n",
    "            I1.append(x[2])\n",
    "            I2.append(sim[0][2])\n",
    "            Y.append(0) #sim\n",
    "            unsim = random.sample(trNTypeList,1)\n",
    "            I1.append(x[2])\n",
    "            I2.append(unsim[0][2])\n",
    "            Y.append(1) #unsim\n",
    "        \n",
    "    #train model\n",
    "    num_batches = len(Y) // batchSize \n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(Y), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(np.array(I1[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(np.array(I2[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(np.array(Y[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        #suitable for siamese network\n",
    "        out1 = model(I1_batch.permute(0, 3, 1, 2))#adjust channel to the second\n",
    "        out2 = model(I2_batch.permute(0, 3, 1, 2))#(100,1024,1024,3)->(100,3,1024,1024)\n",
    "        loss = criterion(out1,out2,Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#4. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.8182, grad_fn=<NegBackward>)\n",
      "tensor([1.0010], requires_grad=True) tensor([1.0010], requires_grad=True)\n",
      "tensor(-0.8183, grad_fn=<NegBackward>)\n",
      "tensor([1.0020], requires_grad=True) tensor([1.0020], requires_grad=True)\n",
      "tensor(-0.8184, grad_fn=<NegBackward>)\n",
      "tensor([1.0030], requires_grad=True) tensor([1.0030], requires_grad=True)\n",
      "tensor(-0.8186, grad_fn=<NegBackward>)\n",
      "tensor([1.0040], requires_grad=True) tensor([1.0040], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#define loss function with torch\n",
    "#https://www.zhihu.com/question/66988664\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.autograd import Function\n",
    "\n",
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, input, target):        \n",
    "        return -dice_coef(input, target) \n",
    " \n",
    " \n",
    "def dice_coef(input, target): \n",
    "    smooth = 1\n",
    "    input_flat = input.view(-1)  \n",
    "    target_flat = target.view(-1)\n",
    "    intersection = input_flat * target_flat\n",
    "    return (2 * intersection.sum() + smooth) / (input_flat.sum() + target_flat.sum() + smooth)\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "x= torch.tensor([1.,1.,1.,1.],requires_grad=False)\n",
    "w = torch.tensor([1.],requires_grad=True)\n",
    "b = torch.tensor([1.],requires_grad=True)\n",
    "target=torch.tensor([1.,0.,1.,0.],requires_grad=False)\n",
    "\n",
    "for i in range(4):\n",
    "    y=w*x+b\n",
    "    diceloss = DiceLoss().cuda()\n",
    "    loss=diceloss(y,target)\n",
    "\n",
    "    optimizer = torch.optim.Adam([w,b], lr = 0.001)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss)\n",
    "    print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9433, 14.6670,  8.8840,  3.7459, 10.4771],\n",
      "        [ 3.5248, 14.1914,  6.3522,  2.3557,  8.7670],\n",
      "        [11.5300,  3.4276,  9.5391,  6.0868,  2.8697],\n",
      "        [11.4902,  7.0307,  5.4385,  6.7889,  8.7239],\n",
      "        [ 5.6001,  8.6130,  9.8114,  2.7259,  9.8378],\n",
      "        [11.4902,  3.9720,  2.7170,  6.5256,  8.7239],\n",
      "        [11.5300, 13.3017,  2.2072,  8.9868,  2.8697],\n",
      "        [ 3.5248,  6.2588,  4.3148,  7.9802,  8.7670]],\n",
      "       grad_fn=<BadFFTFunctionBackward>)\n",
      "tensor([[ 0.1568, -1.7971,  0.6778,  0.2603, -0.0574, -0.8125, -0.4640, -1.5712],\n",
      "        [ 1.7153,  0.0527,  2.1980,  1.0389, -0.6894, -1.3743,  0.2009,  0.3321],\n",
      "        [ 1.0450,  2.0642,  0.1938, -0.0262, -0.9482, -1.5863,  0.5905, -0.0913],\n",
      "        [-0.5643, -0.6841,  1.3027, -1.3130,  0.3162, -2.1120, -1.1172, -0.8846],\n",
      "        [-0.1827,  0.3809, -0.2066,  0.8642,  1.1014,  2.0335, -0.3849, -0.2000],\n",
      "        [ 1.0178, -1.3539,  0.1065,  0.6148, -0.1528, -0.7632,  0.0947,  1.4215],\n",
      "        [-1.3529,  0.2224,  1.5475,  0.4462,  0.5234,  0.8935, -0.1939,  0.6455],\n",
      "        [-0.7765,  0.2512,  1.4656,  0.7138, -0.4268, -1.2988, -0.5261, -0.6340]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#define loss function with numpy\n",
    "#https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from numpy.fft import rfft2, irfft2\n",
    "\n",
    "\n",
    "class BadFFTFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        numpy_input = input.detach().numpy()\n",
    "        result = abs(rfft2(numpy_input))\n",
    "        return input.new(result)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        numpy_go = grad_output.numpy()\n",
    "        result = irfft2(numpy_go)\n",
    "        return grad_output.new(result)\n",
    "\n",
    "# since this layer does not have any parameters, we can\n",
    "# simply declare this as a function, rather than as an nn.Module class\n",
    "\n",
    "def incorrect_fft(input):\n",
    "    return BadFFTFunction.apply(input)\n",
    "\n",
    "input = torch.randn(8, 8, requires_grad=True)\n",
    "result = incorrect_fft(input)\n",
    "print(result)\n",
    "result.backward(torch.randn(result.size()))\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42857143]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "x=[-1,1,1,-1,1,-1,-1]\n",
    "y=[1,-1,1,-1,1,-1,1]\n",
    "X=np.vstack([x,y])\n",
    "d=pdist(X,'hamming')\n",
    "print (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 6, 1]\n",
      "[10  7  2]\n",
      "[1 4 9]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "trI=[1,2,3,4,5,6,7,8,9,10]\n",
    "trY=[10,9,8,7,6,5,4,3,2,1]\n",
    "idx_sf = random.sample(range(0, 10),3)\n",
    "print (idx_sf)\n",
    "trI_sf = np.array(trI)[idx_sf]\n",
    "print (trI_sf)\n",
    "trY_sf = np.array(trY)[idx_sf]\n",
    "print (trY_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 3, 0]\n",
      "[7, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "idx_sf = random.sample(range(0, 8),3)\n",
    "print (idx_sf)\n",
    "trI = [1,2,3,4,5,6,7,8]\n",
    "trI_sf = []\n",
    "for i in idx_sf:\n",
    "    trI_sf.append(trI[i])\n",
    "print (trI_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
