{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Develop Env: linux+cuda9+python3+opencv+pytorch\n",
    "2.Dataset: Fundus-iSee with 10000 images(AMD-720, DR-270, glaucoma-450,myopia-790,norm-7770)\n",
    "        trainset(9000): AMD-648, DR-243, glaucoma-405, myopia-711, norm-6993, \n",
    "        testset(1000): AMD-72, DR-27, glaucoma-45, myopia-79, norm=777\n",
    "3.Performance Metric for unbalanced sample(triplet loss): \n",
    "  1)Accuracy(Acc):  for evaluating the precison of top 1 in the returned list;\n",
    "  2)Specificity(Spe): for evaluating the misdiagnosis rate of normal\n",
    "  3)Sensitivity(Sen): for evaluating the missed diagnosis rate of abnorml(S,V,F)\n",
    "4.Performance Metric for retrieval (Spatial Attention Mechanism):\n",
    "  1)MHR(Mean Hit Ratio):  for evaluating the precison of relevance retrieval;\n",
    "  2)MAP(Mean Average Precision): for evaluation the rank of relevance retrieval;\n",
    "  3)MRR(Mean Reciprocal Rank): for evaluation the first hit rank of relevance retrieval;\n",
    "5.Algorithm: Attention-based Triplet Hashing Network(ATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import itertools  \n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "from scipy.spatial.distance import pdist\n",
    "import cv2\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc \n",
    "from functools import reduce\n",
    "import wfdb#https://github.com/MIT-LCP/wfdb-python\n",
    "from wfdb import processing\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.set_device(3)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 / 20000 The length of train set is 20000\n",
      "20000 / 20000 The length of test set is 20000\n"
     ]
    }
   ],
   "source": [
    "#read train image with CV\n",
    "train_dir = '/data/fjsdata/ECG/MIT-BIH/train' #the path of images\n",
    "trN, trI, trY = [],[],[]\n",
    "for iname in os.listdir(train_dir):\n",
    "    if iname.endswith(\".png\"):\n",
    "        try:\n",
    "            image_path = os.path.join(train_dir, iname)\n",
    "            itype = int(os.path.splitext(iname)[0].split(\"-\")[1])\n",
    "            img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(500,300,3)->(256,256,3)\n",
    "            trN.append(iname)\n",
    "            trI.append(img)\n",
    "            trY.append(itype)\n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trY),20000))\n",
    "        sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(trY))\n",
    "#read test image with CV\n",
    "test_dir = '/data/fjsdata/ECG/MIT-BIH/test' #the path of images\n",
    "teN, teI, teY = [],[],[]\n",
    "for iname in os.listdir(test_dir):\n",
    "    if iname.endswith(\".png\"):\n",
    "        try:\n",
    "            image_path = os.path.join(test_dir, iname)\n",
    "            itype = int(os.path.splitext(iname)[0].split(\"-\")[1])\n",
    "            img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(500,300,3)->(256,256,3)\n",
    "            teN.append(iname)\n",
    "            teI.append(img)\n",
    "            teY.append(itype)\n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(teY),20000))\n",
    "        sys.stdout.flush()\n",
    "print('The length of test set is %d'%len(teY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True) #resnet\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ATHNet(nn.Module):\n",
    "    def __init__(self, hash_size: int, type_size: int):\n",
    "        super().__init__()\n",
    "        #resnet and maxpool\n",
    "        self.net1 = nn.Sequential(#(3,256,256)->(16,128,128)\n",
    "            ResBlock(in_channels=3, out_channels=16, stride=2), \n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        )\n",
    "        \n",
    "        #Attention (16,128,128)->(16,128,128)\n",
    "        self.sa = SpatialAttention()\n",
    "        \n",
    "        #resnet and meanpool\n",
    "        self.net2 =nn.Sequential( #(16,128,128)->(8,64,64)\n",
    "            ResBlock(in_channels=16, out_channels=8, stride=2),\n",
    "            nn.AvgPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        ) \n",
    "         \n",
    "        #fully connected with conv (8,64,64)->(1,32,32)\n",
    "        self.dense=ResBlock(in_channels=8, out_channels=1, stride=2)\n",
    "        #fully connected (1,32,32)->class_size\n",
    "        self.hashlayer = nn.Linear(1*32*32, hash_size)\n",
    "        self.typelayer = nn.Linear(1*32*32, type_size)\n",
    "    \n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.sa(x)*x\n",
    "        x = self.net2(x)\n",
    "        x = self.dense(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x_hash = self.hashlayer(x)\n",
    "        x_type = self.typelayer(x)\n",
    "        return x_hash, x_type\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.l1_loss = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    def forward(self,h1,h2,y):    \n",
    "        margin_val = self.margin * h1.shape[1]\n",
    "        squared_loss = torch.mean(self.mse_loss(h1, h2), dim=1)\n",
    "        # T1: 0.5 * (1 - y) * dist(x1, x2)\n",
    "        positive_pair_loss = (0.5 * (1 - y) * squared_loss)\n",
    "        mean_positive_pair_loss = torch.mean(positive_pair_loss)\n",
    "        # T2: 0.5 * y * max(margin - dist(x1, x2), 0)\n",
    "        zeros = torch.zeros_like(squared_loss)\n",
    "        marginMat = margin_val * torch.ones_like(squared_loss)\n",
    "        negative_pair_loss = 0.5 * y * torch.max(zeros, marginMat - squared_loss)\n",
    "        mean_negative_pair_loss = torch.mean(negative_pair_loss)\n",
    "\n",
    "        # T3: alpha(dst_l1(abs(x1), 1)) + dist_l1(abs(x2), 1)))\n",
    "        mean_value_regularization = self.alpha * (\n",
    "                self.l1_loss(torch.abs(h1), torch.ones_like(h1)) +\n",
    "                self.l1_loss(torch.abs(h2), torch.ones_like(h2)))\n",
    "\n",
    "        loss = mean_positive_pair_loss + mean_negative_pair_loss + mean_value_regularization\n",
    "        return loss\n",
    "    \n",
    "#https://github.com/luyajie/triplet-deep-hash-pytorch#triplet-deep-hash-pytorch            \n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self,H_q,H_p,H_n):    \n",
    "        margin_val = self.margin * H_q.shape[1]\n",
    "        squared_loss_pos = torch.mean(self.mse_loss(H_q, H_p), dim=1)\n",
    "        squared_loss_neg = torch.mean(self.mse_loss(H_q, H_n), dim=1)\n",
    "        zeros = torch.zeros_like(squared_loss_neg)\n",
    "        loss  = torch.max(zeros, margin_val - squared_loss_neg + squared_loss_pos)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    #Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, out, y):\n",
    "        y = y.view(-1,1)\n",
    "        logpt = F.log_softmax(out,dim=1)#default ,dim=1\n",
    "        logpt = logpt.gather(1,y)# dim=1, index=y, max\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=out.data.type():\n",
    "                self.alpha = self.alpha.type_as(out.data)\n",
    "            at = self.alpha.gather(0,y.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()\n",
    "\n",
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, scale=32, margin=0.25, similarity='cos', **kwargs):\n",
    "        super(CircleLoss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.similarity = similarity\n",
    "\n",
    "    def forward(self, feats, labels):\n",
    "        assert feats.size(0) == labels.size(0), \\\n",
    "            f\"feats.size(0): {feats.size(0)} is not equal to labels.size(0): {labels.size(0)}\"\n",
    "        batch_size = feats.size(0)\n",
    "        if self.similarity == 'dot':\n",
    "            sim_mat = torch.matmul(feats, torch.t(feats))\n",
    "        elif self.similarity == 'cos':\n",
    "            feats = F.normalize(feats)\n",
    "            sim_mat = feats.mm(feats.t())\n",
    "        else:\n",
    "            raise ValueError('This similarity is not implemented.')\n",
    "        loss = list()\n",
    "        for i in range(batch_size):\n",
    "            pos_index = labels == labels[i]\n",
    "            pos_index[i] = 0\n",
    "            neg_index = labels != labels[i]\n",
    "            pos_pair_ = sim_mat[i][pos_index]\n",
    "            neg_pair_ = sim_mat[i][neg_index]\n",
    "\n",
    "            alpha_p = torch.relu(-pos_pair_ + 1 + self.margin)\n",
    "            alpha_n = torch.relu(neg_pair_ + self.margin)\n",
    "            margin_p = 1 - self.margin\n",
    "            margin_n = self.margin\n",
    "            loss_p = torch.sum(torch.exp(-self.scale * alpha_p * (pos_pair_ - margin_p)))\n",
    "            loss_n = torch.sum(torch.exp(self.scale * alpha_n * (neg_pair_ - margin_n)))\n",
    "            loss.append(torch.log(1 + loss_p * loss_n))\n",
    "\n",
    "        loss = sum(loss) / batch_size\n",
    "        return loss\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs():\n",
    "    if (len(trY) % 2) == 0: spls = len(trY)\n",
    "    else:  spls = len(trY)-1\n",
    "    idx_sf = random.sample(range(0, spls),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgTriplets( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])[0:4555]\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    idx_2 = np.where( np.array(trY) == 2 ) #class 2\n",
    "    idx_2 = list(idx_2[0])\n",
    "    idx_sf.extend(idx_2)\n",
    "    idx_3 = np.where( np.array(trY) == 3 ) #class 3\n",
    "    idx_3 = list(idx_3[0])\n",
    "    idx_sf.extend(idx_3)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    trQ_y, trP_y, trN_y = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        trQ_y.append(trY[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        elif trY[iQ] == 2:\n",
    "            idx_tmp = idx_2.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_2))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        elif trY[iQ] == 3:\n",
    "            idx_tmp = idx_3.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_3))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf), np.array(trQ_y), np.array(trP_y), np.array(trN_y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#--------------------------------------------------------\n",
    "#ATH-Triplet+CE\n",
    "#--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 / 1000 : loss = 5.250372Eopch:     1 mean_loss = 7.813445\n",
      " 1000 / 1000 : loss = 2.081184Eopch:     2 mean_loss = 4.135663\n",
      " 1000 / 1000 : loss = 2.827699Eopch:     3 mean_loss = 3.476321\n",
      " 1000 / 1000 : loss = 0.034373Eopch:     4 mean_loss = 2.956136\n",
      " 1000 / 1000 : loss = 0.888488Eopch:     5 mean_loss = 2.536000\n",
      " 1000 / 1000 : loss = 2.151465Eopch:     6 mean_loss = 2.159403\n",
      " 1000 / 1000 : loss = 6.509357Eopch:     7 mean_loss = 1.801342\n",
      " 1000 / 1000 : loss = 0.586431Eopch:     8 mean_loss = 1.498455\n",
      " 1000 / 1000 : loss = 2.998755Eopch:     9 mean_loss = 1.226462\n",
      " 1000 / 1000 : loss = 0.403038Eopch:    10 mean_loss = 1.072182\n",
      "best_loss = 1.072182\n",
      " 1999 / 2000 Completed buliding index in 1 seconds\n",
      "hash_size=48, margin=0.7, topk=10, mHR=0.776310, mAP=0.743370, mRR=0.911503\n"
     ]
    }
   ],
   "source": [
    "#trQ_sf, trP_sf, trN_sf, trQ_y, trP_y, trN_y = onlineGenImgTriplets() #sample  triplet labels\n",
    "assert (trQ_sf.shape==trP_sf.shape and trQ_sf.shape==trN_sf.shape)\n",
    "assert (trQ_y.shape==trP_y.shape and trQ_y.shape==trN_y.shape)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trP_y))!=0,1,0))==0.0)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trN_y))!=0,1,0))==1.0)\n",
    "\n",
    "#define model\n",
    "for k in [48]:#[12,24,36,48]:\n",
    "    for r in [0.7]:#[0.3,0.5,0.7]:\n",
    "        model = ATHNet(hash_size=k, type_size=4).cuda()#initialize model\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "        tl_loss  = TripletLoss(margin=r).cuda() #define TripletLoss \n",
    "        ce_loss  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes\n",
    "\n",
    "        #train model\n",
    "        best_net, best_loss = None, float('inf')\n",
    "        batchSize = 10\n",
    "        for epoch in range(10):#iteration\n",
    "            losses = []\n",
    "            shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "            train_q = trQ_sf[shuffled_idx]\n",
    "            train_q_y = trQ_y[shuffled_idx]\n",
    "            train_p = trP_sf[shuffled_idx]\n",
    "            train_p_y = trP_y[shuffled_idx]\n",
    "            train_n = trN_sf[shuffled_idx]\n",
    "            train_n_y = trN_y[shuffled_idx]\n",
    "            num_batches = len(trQ_sf) // batchSize\n",
    "            for i in range(num_batches):\n",
    "                optimizer.zero_grad()#grad vanish\n",
    "                min_idx = i * batchSize\n",
    "                max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "                Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "                Q_y_batch = torch.from_numpy(train_q_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "                P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "                P_y_batch = torch.from_numpy(train_p_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "                N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "                N_y_batch = torch.from_numpy(train_n_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "                #forword\n",
    "                Q_hash, Q_type = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "                P_hash, P_type = model(P_batch.permute(0, 3, 1, 2))\n",
    "                N_hash, N_type = model(N_batch.permute(0, 3, 1, 2))\n",
    "                #loss\n",
    "                hash_loss = tl_loss(Q_hash,P_hash,N_hash)\n",
    "                type_loss = ce_loss(Q_type,Q_y_batch) + ce_loss(P_type,P_y_batch) + ce_loss(N_type,N_y_batch) #F.log_softmax+F.nll_loss\n",
    "                loss = hash_loss+type_loss\n",
    "                #backward\n",
    "                loss.backward()\n",
    "                #update parameters\n",
    "                optimizer.step()\n",
    "                #show loss\n",
    "                sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "                sys.stdout.flush()     \n",
    "                losses.append(loss.item())\n",
    "            print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "            if np.mean(losses) < best_loss:\n",
    "                best_loss = np.mean(losses)\n",
    "                best_net = copy.deepcopy(model)\n",
    "        print(\"best_loss = %.6f\" % (best_loss))\n",
    "        #release gpu memory\n",
    "        model = model.cpu()\n",
    "        tl_loss=tl_loss.cpu()\n",
    "        ce_loss=ce_loss.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        #hash code of train data from model\n",
    "        batchSize = 10\n",
    "        num_batches = len(trI) // batchSize\n",
    "        trF = []\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * batchSize\n",
    "            max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "            I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "            X_batch,_ = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "            I_batch = I_batch.cpu()\n",
    "            X_batch = X_batch.cpu()\n",
    "            torch.cuda.empty_cache()#release gpu memory\n",
    "            trF.extend(X_batch.data.numpy().tolist())\n",
    "            sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        #hash code of test data from model\n",
    "        teY_pred = []\n",
    "        teF = [] \n",
    "        num_batches = len(teY) // batchSize \n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * batchSize\n",
    "            max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "            x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "            x_hash, x_type = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "            teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "            x_type = F.log_softmax(x_type,dim=1) \n",
    "            pred = x_type.max(1,keepdim=True)[1]\n",
    "            teY_pred.extend(pred.cpu().data.numpy().tolist())\n",
    "            sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        #performance of retrieval\n",
    "        # buliding index of trainset\n",
    "        tstart = time.time()\n",
    "        cpu_index = faiss.IndexFlatL2(k) #\n",
    "        gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "        gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "        elapsed = time.time() - tstart    \n",
    "        print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "        for topk in [10]: #[5,10,15,20]:\n",
    "            MHR = [] #mean Hit ratio \n",
    "            MAP = [] #mean average precision\n",
    "            MRR = [] #mean reciprocal rank\n",
    "            scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "            for i, teVal in enumerate(teF):\n",
    "                stype = teY[i]\n",
    "                #perfromance\n",
    "                pos_len = 0\n",
    "                rank_len = 0\n",
    "                mrr_flag = 0\n",
    "                #for j in ranklist:\n",
    "                for j in neighbors[i].tolist():\n",
    "                    dtype = trY[j]\n",
    "                    rank_len=rank_len+1\n",
    "                    if stype==dtype:  #hit\n",
    "                        MHR.append(1)\n",
    "                        pos_len = pos_len +1\n",
    "                        MAP.append(pos_len/rank_len) \n",
    "                        if mrr_flag==0: \n",
    "                            MRR.append(pos_len/rank_len)\n",
    "                            mrr_flag =1\n",
    "                    else: \n",
    "                        MHR.append(0)\n",
    "                        MAP.append(0)   \n",
    "            print(\"hash_size={}, margin={}, topk={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(k, r, topk, np.mean(MHR),np.mean(MAP),np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "tl_loss=tl_loss.cpu()\n",
    "ce_loss=ce_loss.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1999 / 2000 Completed buliding index in 28 seconds\n"
     ]
    }
   ],
   "source": [
    "#hash code of train data from model\n",
    "tstart = time.time()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch,_ = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "elapsed = time.time() - tstart   \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2575674057006836\n",
      "Completed buliding index in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "tstart = time.time()\n",
    "scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=100)\n",
    "elapsed = time.time() - tstart   \n",
    "print(elapsed)\n",
    "print('Completed buliding index in %d seconds' % int(elapsed))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 / 1000 : loss = 3.876657Eopch:     1 mean_loss = 8.415611\n",
      " 1000 / 1000 : loss = 0.240579Eopch:     2 mean_loss = 4.064745\n",
      " 1000 / 1000 : loss = 1.178148Eopch:     3 mean_loss = 3.322722\n",
      " 1000 / 1000 : loss = 0.453971Eopch:     4 mean_loss = 2.540905\n",
      " 1000 / 1000 : loss = 2.305858Eopch:     5 mean_loss = 2.019811\n",
      " 1000 / 1000 : loss = 5.319075Eopch:     6 mean_loss = 1.553994\n",
      " 1000 / 1000 : loss = 0.085987Eopch:     7 mean_loss = 1.314307\n",
      " 1000 / 1000 : loss = 4.656636Eopch:     8 mean_loss = 0.949640\n",
      " 1000 / 1000 : loss = 0.005121Eopch:     9 mean_loss = 0.927324\n",
      " 1000 / 1000 : loss = 0.004219Eopch:    10 mean_loss = 0.593696\n",
      " 1000 / 1000 : loss = 0.754326Eopch:    11 mean_loss = 0.574776\n",
      " 1000 / 1000 : loss = 0.334976Eopch:    12 mean_loss = 0.542534\n",
      " 1000 / 1000 : loss = 0.086265Eopch:    13 mean_loss = 0.428681\n",
      " 1000 / 1000 : loss = 0.056847Eopch:    14 mean_loss = 0.420563\n",
      " 1000 / 1000 : loss = 0.003638Eopch:    15 mean_loss = 0.298493\n",
      " 1000 / 1000 : loss = 0.069464Eopch:    16 mean_loss = 0.415184\n",
      " 1000 / 1000 : loss = 0.542333Eopch:    17 mean_loss = 0.357167\n",
      " 1000 / 1000 : loss = 0.216822Eopch:    18 mean_loss = 0.402634\n",
      " 1000 / 1000 : loss = 0.009288Eopch:    19 mean_loss = 0.259925\n",
      " 1000 / 1000 : loss = 0.554513Eopch:    20 mean_loss = 0.256964\n",
      " 1000 / 1000 : loss = 0.033765Eopch:    21 mean_loss = 0.309828\n",
      " 1000 / 1000 : loss = 0.387313Eopch:    22 mean_loss = 0.206765\n",
      " 1000 / 1000 : loss = 0.899404Eopch:    23 mean_loss = 0.210824\n",
      " 1000 / 1000 : loss = 0.052673Eopch:    24 mean_loss = 0.382196\n",
      " 1000 / 1000 : loss = 0.017271Eopch:    25 mean_loss = 0.293286\n",
      " 1000 / 1000 : loss = 0.003498Eopch:    26 mean_loss = 0.245069\n",
      " 1000 / 1000 : loss = 0.351326Eopch:    27 mean_loss = 0.218477\n",
      " 1000 / 1000 : loss = 0.249863Eopch:    28 mean_loss = 0.310734\n",
      " 1000 / 1000 : loss = 0.020695Eopch:    29 mean_loss = 0.149567\n",
      " 1000 / 1000 : loss = 0.059409Eopch:    30 mean_loss = 0.249050\n",
      " 1000 / 1000 : loss = 0.055473Eopch:    31 mean_loss = 0.190043\n",
      " 1000 / 1000 : loss = 0.000714Eopch:    32 mean_loss = 0.219080\n",
      " 1000 / 1000 : loss = 0.001592Eopch:    33 mean_loss = 0.206642\n",
      " 1000 / 1000 : loss = 0.071907Eopch:    34 mean_loss = 0.189055\n",
      " 1000 / 1000 : loss = 0.014967Eopch:    35 mean_loss = 0.177574\n",
      " 1000 / 1000 : loss = 0.041166Eopch:    36 mean_loss = 0.213311\n",
      " 1000 / 1000 : loss = 0.002995Eopch:    37 mean_loss = 0.148864\n",
      " 1000 / 1000 : loss = 0.095559Eopch:    38 mean_loss = 0.170867\n",
      " 1000 / 1000 : loss = 0.020811Eopch:    39 mean_loss = 0.055062\n",
      " 1000 / 1000 : loss = 0.013449Eopch:    40 mean_loss = 0.215528\n",
      " 1000 / 1000 : loss = 0.012901Eopch:    41 mean_loss = 0.229272\n",
      " 1000 / 1000 : loss = 0.116118Eopch:    42 mean_loss = 0.124452\n",
      " 1000 / 1000 : loss = 0.002692Eopch:    43 mean_loss = 0.153917\n",
      " 1000 / 1000 : loss = 0.012279Eopch:    44 mean_loss = 0.140828\n",
      " 1000 / 1000 : loss = 0.00257Eopch:    45 mean_loss = 0.086310\n",
      " 1000 / 1000 : loss = 0.000525Eopch:    46 mean_loss = 0.150326\n",
      " 1000 / 1000 : loss = 0.075748Eopch:    47 mean_loss = 0.174683\n",
      " 1000 / 1000 : loss = 0.040058Eopch:    48 mean_loss = 0.192414\n",
      " 1000 / 1000 : loss = 0.002202Eopch:    49 mean_loss = 0.174030\n",
      " 1000 / 1000 : loss = 0.000151Eopch:    50 mean_loss = 0.132514\n",
      "best_loss = 0.055062\n",
      "Completed buliding index in 2325 seconds\n"
     ]
    }
   ],
   "source": [
    "model = ATHNet(hash_size=k, type_size=4).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "tl_loss  = TripletLoss(margin=r).cuda() #define TripletLoss \n",
    "ce_loss  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes\n",
    "\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "tstart = time.time()\n",
    "for epoch in range(50):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_q_y = trQ_y[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_p_y = trP_y[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    train_n_y = trN_y[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Q_y_batch = torch.from_numpy(train_q_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_y_batch = torch.from_numpy(train_p_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_y_batch = torch.from_numpy(train_n_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash, Q_type = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash, P_type = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash, N_type = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #loss\n",
    "        hash_loss = tl_loss(Q_hash,P_hash,N_hash)\n",
    "        type_loss = ce_loss(Q_type,Q_y_batch) + ce_loss(P_type,P_y_batch) + ce_loss(N_type,N_y_batch) #F.log_softmax+F.nll_loss\n",
    "        loss = hash_loss+type_loss\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "elapsed = time.time() - tstart   \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))  \n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "tl_loss=tl_loss.cpu()\n",
    "ce_loss=ce_loss.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
