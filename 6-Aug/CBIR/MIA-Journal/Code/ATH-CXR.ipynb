{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Develop Env: linux+cuda9+python3+opencv+pytorch\n",
    "2.Dataset: MIMIC-CXR https://mimic-cxr.mit.edu/about/access/\n",
    "        1)Label:No Finding,Enlarged Cardiomediastinum,Cardiomegaly,Airspace Opacity,Lung Lesion,\n",
    "               Edema,Consolidation,Pneumonia,Atelectasis,Pneumothorax,Pleural Effusion,Pleural Other,Fracture,Support Devices\n",
    "               0.0 - negative, 1.0 - positve, -1.0 - uncertain\n",
    "        2)Dataset Statistics: \n",
    "        trainset：Normal = 14555， Edema = 1837, Pneumonia = 3220, Fracture = 388 \n",
    "        testset：Normal = 14854， Edema = 944,  Pneumonia = 3788, Fracture = 414 \n",
    "3.Performance Metric for unbalanced sample(triplet loss): \n",
    "  1)Accuracy(Acc):  for evaluating the precison of top 1 in the returned list;\n",
    "  2)Specificity(Spe): for evaluating the misdiagnosis rate of normal\n",
    "  3)Sensitivity(Sen): for evaluating the missed diagnosis rate of abnorml(S,V,F)\n",
    "4.Performance Metric for retrieval (Spatial Attention Mechanism):\n",
    "  1)MHR(Mean Hit Ratio):  for evaluating the precison of relevance retrieval;\n",
    "  2)MAP(Mean Average Precision): for evaluation the rank of relevance retrieval;\n",
    "  3)MRR(Mean Reciprocal Rank): for evaluation the first hit rank of relevance retrieval;\n",
    "5.Algorithm: Attention-based Triplet Hashing Network(ATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import itertools  \n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "from scipy.spatial.distance import pdist\n",
    "import cv2\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc \n",
    "from functools import reduce\n",
    "import wfdb#https://github.com/MIT-LCP/wfdb-python\n",
    "from wfdb import processing\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.set_device(1)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 / 20000 The length of train set is 20000\n",
      "20000 / 20000 The length of test set is 20000\n"
     ]
    }
   ],
   "source": [
    "#read train image with CV\n",
    "train_dir = '/data/fjsdata/ECG/MIT-BIH/train' #the path of images\n",
    "trN, trI, trY = [],[],[]\n",
    "for iname in os.listdir(train_dir):\n",
    "    if iname.endswith(\".png\"):\n",
    "        try:\n",
    "            image_path = os.path.join(train_dir, iname)\n",
    "            itype = int(os.path.splitext(iname)[0].split(\"-\")[1])\n",
    "            img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(500,300,3)->(256,256,3)\n",
    "            trN.append(iname)\n",
    "            trI.append(img)\n",
    "            trY.append(itype)\n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trY),20000))\n",
    "        sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(trY))\n",
    "#read test image with CV\n",
    "test_dir = '/data/fjsdata/ECG/MIT-BIH/test' #the path of images\n",
    "teN, teI, teY = [],[],[]\n",
    "for iname in os.listdir(test_dir):\n",
    "    if iname.endswith(\".png\"):\n",
    "        try:\n",
    "            image_path = os.path.join(test_dir, iname)\n",
    "            itype = int(os.path.splitext(iname)[0].split(\"-\")[1])\n",
    "            img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(500,300,3)->(256,256,3)\n",
    "            teN.append(iname)\n",
    "            teI.append(img)\n",
    "            teY.append(itype)\n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(teY),20000))\n",
    "        sys.stdout.flush()\n",
    "print('The length of test set is %d'%len(teY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True) #resnet\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ATHNet(nn.Module):\n",
    "    def __init__(self, hash_size: int, type_size: int):\n",
    "        super().__init__()\n",
    "        #resnet and maxpool\n",
    "        self.net1 = nn.Sequential(#(3,256,256)->(16,128,128)\n",
    "            ResBlock(in_channels=3, out_channels=16, stride=2), \n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        )\n",
    "        \n",
    "        #Attention (16,128,128)->(16,128,128)\n",
    "        self.sa = SpatialAttention()\n",
    "        \n",
    "        #resnet and meanpool\n",
    "        self.net2 =nn.Sequential( #(16,128,128)->(8,64,64)\n",
    "            ResBlock(in_channels=16, out_channels=8, stride=2),\n",
    "            nn.AvgPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        ) \n",
    "         \n",
    "        #fully connected with conv (8,64,64)->(1,32,32)\n",
    "        self.dense=ResBlock(in_channels=8, out_channels=1, stride=2)\n",
    "        #fully connected (1,32,32)->class_size\n",
    "        self.hashlayer = nn.Linear(1*32*32, hash_size)\n",
    "        self.typelayer = nn.Linear(1*32*32, type_size)\n",
    "    \n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.sa(x)*x\n",
    "        x = self.net2(x)\n",
    "        x = self.dense(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x_hash = self.hashlayer(x)\n",
    "        x_type = self.typelayer(x)\n",
    "        return x_hash, x_type\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.l1_loss = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    def forward(self,h1,h2,y):    \n",
    "        margin_val = self.margin * h1.shape[1]\n",
    "        squared_loss = torch.mean(self.mse_loss(h1, h2), dim=1)\n",
    "        # T1: 0.5 * (1 - y) * dist(x1, x2)\n",
    "        positive_pair_loss = (0.5 * (1 - y) * squared_loss)\n",
    "        mean_positive_pair_loss = torch.mean(positive_pair_loss)\n",
    "        # T2: 0.5 * y * max(margin - dist(x1, x2), 0)\n",
    "        zeros = torch.zeros_like(squared_loss)\n",
    "        marginMat = margin_val * torch.ones_like(squared_loss)\n",
    "        negative_pair_loss = 0.5 * y * torch.max(zeros, marginMat - squared_loss)\n",
    "        mean_negative_pair_loss = torch.mean(negative_pair_loss)\n",
    "\n",
    "        # T3: alpha(dst_l1(abs(x1), 1)) + dist_l1(abs(x2), 1)))\n",
    "        mean_value_regularization = self.alpha * (\n",
    "                self.l1_loss(torch.abs(h1), torch.ones_like(h1)) +\n",
    "                self.l1_loss(torch.abs(h2), torch.ones_like(h2)))\n",
    "\n",
    "        loss = mean_positive_pair_loss + mean_negative_pair_loss + mean_value_regularization\n",
    "        return loss\n",
    "    \n",
    "#https://github.com/luyajie/triplet-deep-hash-pytorch#triplet-deep-hash-pytorch            \n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self,H_q,H_p,H_n):    \n",
    "        margin_val = self.margin * H_q.shape[1]\n",
    "        squared_loss_pos = torch.mean(self.mse_loss(H_q, H_p), dim=1)\n",
    "        squared_loss_neg = torch.mean(self.mse_loss(H_q, H_n), dim=1)\n",
    "        zeros = torch.zeros_like(squared_loss_neg)\n",
    "        loss  = torch.max(zeros, margin_val - squared_loss_neg + squared_loss_pos)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    #Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, out, y):\n",
    "        y = y.view(-1,1)\n",
    "        logpt = F.log_softmax(out,dim=1)#default ,dim=1\n",
    "        logpt = logpt.gather(1,y)# dim=1, index=y, max\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=out.data.type():\n",
    "                self.alpha = self.alpha.type_as(out.data)\n",
    "            at = self.alpha.gather(0,y.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()\n",
    "\n",
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, scale=32, margin=0.25, similarity='cos', **kwargs):\n",
    "        super(CircleLoss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.similarity = similarity\n",
    "\n",
    "    def forward(self, feats, labels):\n",
    "        assert feats.size(0) == labels.size(0), \\\n",
    "            f\"feats.size(0): {feats.size(0)} is not equal to labels.size(0): {labels.size(0)}\"\n",
    "        batch_size = feats.size(0)\n",
    "        if self.similarity == 'dot':\n",
    "            sim_mat = torch.matmul(feats, torch.t(feats))\n",
    "        elif self.similarity == 'cos':\n",
    "            feats = F.normalize(feats)\n",
    "            sim_mat = feats.mm(feats.t())\n",
    "        else:\n",
    "            raise ValueError('This similarity is not implemented.')\n",
    "        loss = list()\n",
    "        for i in range(batch_size):\n",
    "            pos_index = labels == labels[i]\n",
    "            pos_index[i] = 0\n",
    "            neg_index = labels != labels[i]\n",
    "            pos_pair_ = sim_mat[i][pos_index]\n",
    "            neg_pair_ = sim_mat[i][neg_index]\n",
    "\n",
    "            alpha_p = torch.relu(-pos_pair_ + 1 + self.margin)\n",
    "            alpha_n = torch.relu(neg_pair_ + self.margin)\n",
    "            margin_p = 1 - self.margin\n",
    "            margin_n = self.margin\n",
    "            loss_p = torch.sum(torch.exp(-self.scale * alpha_p * (pos_pair_ - margin_p)))\n",
    "            loss_n = torch.sum(torch.exp(self.scale * alpha_n * (neg_pair_ - margin_n)))\n",
    "            loss.append(torch.log(1 + loss_p * loss_n))\n",
    "\n",
    "        loss = sum(loss) / batch_size\n",
    "        return loss\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs():\n",
    "    if (len(trY) % 2) == 0: spls = len(trY)\n",
    "    else:  spls = len(trY)-1\n",
    "    idx_sf = random.sample(range(0, spls),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgTriplets( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])[0:4555]\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    idx_2 = np.where( np.array(trY) == 2 ) #class 2\n",
    "    idx_2 = list(idx_2[0])\n",
    "    idx_sf.extend(idx_2)\n",
    "    idx_3 = np.where( np.array(trY) == 3 ) #class 3\n",
    "    idx_3 = list(idx_3[0])\n",
    "    idx_sf.extend(idx_3)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    trQ_y, trP_y, trN_y = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        trQ_y.append(trY[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        elif trY[iQ] == 2:\n",
    "            idx_tmp = idx_2.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_2))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        elif trY[iQ] == 3:\n",
    "            idx_tmp = idx_3.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_3))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf), np.array(trQ_y), np.array(trP_y), np.array(trN_y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#--------------------------------------------------------\n",
    "#ATH-Triplet+CE\n",
    "#--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 / 1000 : loss = 1.476945Eopch:     1 mean_loss = 4.266039\n",
      " 1000 / 1000 : loss = 2.122481Eopch:     2 mean_loss = 2.482491\n",
      " 1000 / 1000 : loss = 5.973835Eopch:     3 mean_loss = 2.081608\n",
      " 1000 / 1000 : loss = 0.398822Eopch:     4 mean_loss = 1.787284\n",
      " 1000 / 1000 : loss = 0.262652Eopch:     5 mean_loss = 1.572742\n",
      " 1000 / 1000 : loss = 2.074561Eopch:     6 mean_loss = 1.387705\n",
      " 1000 / 1000 : loss = 0.105523Eopch:     7 mean_loss = 1.196310\n",
      " 1000 / 1000 : loss = 0.244838Eopch:     8 mean_loss = 0.991722\n",
      " 1000 / 1000 : loss = 12.335144Eopch:     9 mean_loss = 0.873514\n",
      " 1000 / 1000 : loss = 0.271365Eopch:    10 mean_loss = 0.779182\n",
      "best_loss = 0.779182\n",
      " 1999 / 2000 Completed buliding index in 22 seconds\n",
      "topk=5, mHR=0.750120, mAP=0.722837, mRR=0.926407\n",
      "topk=10, mHR=0.753030, mAP=0.716481, mRR=0.896289\n",
      "topk=15, mHR=0.755083, mAP=0.714177, mRR=0.881909\n",
      "topk=20, mHR=0.755822, mAP=0.712810, mRR=0.873714\n",
      "Accuracy: 0.721600\n",
      "[[10410  1003  3100   341]\n",
      " [  227   433   267    17]\n",
      " [  109   117  3509    53]\n",
      " [  212     1   121    80]]\n",
      "Sensitivity of Normal: 0.700821\n",
      "Sensitivity of Edema: 0.458686\n",
      "Sensitivity of Pneumonia: 0.926346\n",
      "Sensitivity of Fracture: 0.193237\n"
     ]
    }
   ],
   "source": [
    "#trQ_sf, trP_sf, trN_sf, trQ_y, trP_y, trN_y = onlineGenImgTriplets() #sample  triplet labels\n",
    "assert (trQ_sf.shape==trP_sf.shape and trQ_sf.shape==trN_sf.shape)\n",
    "assert (trQ_y.shape==trP_y.shape and trQ_y.shape==trN_y.shape)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trP_y))!=0,1,0))==0.0)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trN_y))!=0,1,0))==1.0)\n",
    "\n",
    "#define model\n",
    "model = ATHNet(hash_size=36, type_size=4).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "tl_loss  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "ce_loss  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes\n",
    "\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_q_y = trQ_y[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_p_y = trP_y[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    train_n_y = trN_y[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Q_y_batch = torch.from_numpy(train_q_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_y_batch = torch.from_numpy(train_p_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_y_batch = torch.from_numpy(train_n_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash, Q_type = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash, P_type = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash, N_type = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #loss\n",
    "        hash_loss = tl_loss(Q_hash,P_hash,N_hash)\n",
    "        type_loss = ce_loss(Q_type,Q_y_batch) + ce_loss(P_type,P_y_batch) + ce_loss(N_type,N_y_batch) #F.log_softmax+F.nll_loss\n",
    "        loss = hash_loss+type_loss\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "tl_loss=tl_loss.cpu()\n",
    "ce_loss=ce_loss.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#hash code of train data from model\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch,_ = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teY_pred = []\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, x_type = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "    x_type = F.log_softmax(x_type,dim=1) \n",
    "    pred = x_type.max(1,keepdim=True)[1]\n",
    "    teY_pred.extend(pred.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "#performance of retrieval\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"topk={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(topk, np.mean(MHR),np.mean(MAP),np.mean(MRR)))\n",
    "#performance of classification\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, teY_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, teY_pred, labels=labels )\n",
    "print (cm)\n",
    "print ('Sensitivity of Normal: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of Edema: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of Pneumonia: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of Fracture: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=0, mHR=0.768332, mAP=0.734026, mRR=1.000000\n",
      "label=1, mHR=0.331674, mAP=0.256860, mRR=0.333333\n",
      "label=2, mHR=0.874340, mAP=0.836893, mRR=1.000000\n",
      "label=3, mHR=0.054831, mAP=0.033287, mRR=0.142857\n"
     ]
    }
   ],
   "source": [
    "for cls in labels:\n",
    "    teY_cls = np.array(teY)[np.where( np.array(teY) == cls)[0]]\n",
    "    teF_cls = np.array(teF)[np.where( np.array(teY) == cls)[0]]\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = 0.0 #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF_cls).astype('float32'), k=10)\n",
    "    for i, teVal in enumerate(teF_cls):#perfromance\n",
    "        stype = teY_cls[i]\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR= pos_len/rank_len\n",
    "                    mrr_flag = 1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"label={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(cls, np.mean(MHR),np.mean(MAP),np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ATH+Triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 / 1000 : loss = 3.393445Eopch:     1 mean_loss = 3.654553\n",
      " 1000 / 1000 : loss = 0.058874Eopch:     2 mean_loss = 2.197043\n",
      " 1000 / 1000 : loss = 0.086843Eopch:     3 mean_loss = 1.818961\n",
      " 1000 / 1000 : loss = 7.015194Eopch:     4 mean_loss = 1.498876\n",
      " 1000 / 1000 : loss = 1.539314Eopch:     5 mean_loss = 1.252285\n",
      " 1000 / 1000 : loss = 0.244521Eopch:     6 mean_loss = 1.055028\n",
      " 1000 / 1000 : loss = 0.026969Eopch:     7 mean_loss = 0.800839\n",
      " 1000 / 1000 : loss = 0.004753Eopch:     8 mean_loss = 0.645815\n",
      " 1000 / 1000 : loss = 0.080672Eopch:     9 mean_loss = 0.516862\n",
      " 1000 / 1000 : loss = 14.829358Eopch:    10 mean_loss = 0.488338\n",
      "best_loss = 0.488338\n",
      " 1999 / 2000 Completed buliding index in 1 seconds\n",
      "topk=5, mHR=0.747860, mAP=0.721664, mRR=0.933217\n",
      "topk=10, mHR=0.750270, mAP=0.716717, mRR=0.908965\n",
      "topk=15, mHR=0.751197, mAP=0.714871, mRR=0.895957\n",
      "topk=20, mHR=0.751415, mAP=0.713724, mRR=0.886979\n",
      "Accuracy: 0.124200\n",
      "[[ 346 1275 3835 9398]\n",
      " [  12  179  235  518]\n",
      " [  32  661 1659 1436]\n",
      " [   0   72   42  300]]\n",
      "Sensitivity of Normal: 0.023293\n",
      "Sensitivity of Edema: 0.189619\n",
      "Sensitivity of Pneumonia: 0.437962\n",
      "Sensitivity of Fracture: 0.724638\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = ATHNet(hash_size=36, type_size=4).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "tl_loss  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "pl_t_i = []\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash, _ = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash, _ = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash, _ = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #loss\n",
    "        loss = tl_loss(Q_hash,P_hash,N_hash)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    pl_t_i.append(np.mean(losses))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "tl_loss=tl_loss.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#hash code of train data from model\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize \n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch,_ = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "teY_pred = []\n",
    "teY_prob = []\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, x_type = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "    x_type = F.log_softmax(x_type,dim=1) \n",
    "    teY_prob.extend(x_type.cpu().data.numpy().tolist())\n",
    "    pred = x_type.max(1,keepdim=True)[1]\n",
    "    teY_pred.extend(pred.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#performance of retrieval\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"topk={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(topk, np.mean(MHR),np.mean(MAP),np.mean(MRR)))\n",
    "#performance of classification\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, teY_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, teY_pred, labels=labels )\n",
    "print (cm)\n",
    "print ('Sensitivity of Normal: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of Edema: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of Pneumonia: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of Fracture: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=0, mHR=0.782180, mAP=0.752045, mRR=1.000000\n",
      "label=1, mHR=0.343432, mAP=0.277156, mRR=1.000000\n",
      "label=2, mHR=0.807049, mAP=0.765643, mRR=0.200000\n",
      "label=3, mHR=0.013527, mAP=0.003807, mRR=0.250000\n"
     ]
    }
   ],
   "source": [
    "for cls in labels:\n",
    "    teY_cls = np.array(teY)[np.where( np.array(teY) == cls)[0]]\n",
    "    teF_cls = np.array(teF)[np.where( np.array(teY) == cls)[0]]\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = 0.0 #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF_cls).astype('float32'), k=10)\n",
    "    for i, teVal in enumerate(teF_cls):#perfromance\n",
    "        stype = teY_cls[i]\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR= pos_len/rank_len\n",
    "                    mrr_flag = 1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"label={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(cls, np.mean(MHR),np.mean(MAP),np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ATH+Pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 / 1000 : loss = 0.210501Eopch:     1 mean_loss = 1.306045\n",
      " 1000 / 1000 : loss = 0.477685Eopch:     2 mean_loss = 0.888655\n",
      " 1000 / 1000 : loss = 1.85638Eopch:     3 mean_loss = 0.858163\n",
      " 1000 / 1000 : loss = 0.171504Eopch:     4 mean_loss = 0.818934\n",
      " 1000 / 1000 : loss = 1.033973Eopch:     5 mean_loss = 0.790585\n",
      " 1000 / 1000 : loss = 1.021456Eopch:     6 mean_loss = 0.774501\n",
      " 1000 / 1000 : loss = 0.31713Eopch:     7 mean_loss = 0.737395\n",
      " 1000 / 1000 : loss = 0.129243Eopch:     8 mean_loss = 0.745025\n",
      " 1000 / 1000 : loss = 0.750228Eopch:     9 mean_loss = 0.700849\n",
      " 1000 / 1000 : loss = 0.570241Eopch:    10 mean_loss = 0.714397\n",
      "best_loss = 0.700849\n",
      " 1999 / 2000 Completed buliding index in 1 seconds\n",
      "topk=5, mHR=0.748680, mAP=0.717284, mRR=0.926671\n",
      "topk=10, mHR=0.747445, mAP=0.706423, mRR=0.902467\n",
      "topk=15, mHR=0.747130, mAP=0.701632, mRR=0.889925\n",
      "topk=20, mHR=0.746910, mAP=0.699054, mRR=0.883101\n",
      "Accuracy: 0.185900\n",
      "[[ 2872 11423    12   547]\n",
      " [  226   686     2    30]\n",
      " [ 1688  1372    28   700]\n",
      " [   27   255     0   132]]\n",
      "Sensitivity of Normal: 0.193349\n",
      "Sensitivity of Edema: 0.726695\n",
      "Sensitivity of Pneumonia: 0.007392\n",
      "Sensitivity of Fracture: 0.318841\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = ATHNet(hash_size=36, type_size=4).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "criterion  = HashLossFunc(margin=0.5).cuda() #define PairwiseLoss \n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs()\n",
    "    losses = []\n",
    "    num_batches = len(trY_sf) // batchSize \n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch,_ = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch,_ = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "tl_loss=tl_loss.cpu()\n",
    "ce_loss=ce_loss.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize \n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch,_ = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teY_pred = []\n",
    "teY_prob = []\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, x_type = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "    x_type = F.log_softmax(x_type,dim=1) \n",
    "    teY_prob.extend(x_type.cpu().data.numpy().tolist())\n",
    "    pred = x_type.max(1,keepdim=True)[1]\n",
    "    teY_pred.extend(pred.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#performance of retrieval\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"topk={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(topk, np.mean(MHR),np.mean(MAP),np.mean(MRR)))\n",
    "#performance of classification\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, teY_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, teY_pred, labels=labels )\n",
    "print (cm)\n",
    "print ('Sensitivity of Normal: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of Edema: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of Pneumonia: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of Fracture: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=0, mHR=0.777898, mAP=0.738831, mRR=1.000000\n",
      "label=1, mHR=0.123517, mAP=0.073226, mRR=0.500000\n",
      "label=2, mHR=0.859556, mAP=0.812353, mRR=1.000000\n",
      "label=3, mHR=0.051691, mAP=0.018219, mRR=0.200000\n"
     ]
    }
   ],
   "source": [
    "for cls in labels:\n",
    "    teY_cls = np.array(teY)[np.where( np.array(teY) == cls)[0]]\n",
    "    teF_cls = np.array(teF)[np.where( np.array(teY) == cls)[0]]\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = 0.0 #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF_cls).astype('float32'), k=10)\n",
    "    for i, teVal in enumerate(teF_cls):#perfromance\n",
    "        stype = teY_cls[i]\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR= pos_len/rank_len\n",
    "                    mrr_flag = 1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"label={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(cls, np.mean(MHR),np.mean(MAP),np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ATH+Circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2000 / 2000 : loss = 0.8732189Eopch:     1 mean_loss = 6.135985\n",
      " 2000 / 2000 : loss = 0.0211238Eopch:     2 mean_loss = 4.730092\n",
      " 2000 / 2000 : loss = 10.220989Eopch:     3 mean_loss = 4.424806\n",
      " 2000 / 2000 : loss = 3.8150846Eopch:     4 mean_loss = 3.960394\n",
      " 2000 / 2000 : loss = 0.5865719Eopch:     5 mean_loss = 3.854856\n",
      " 2000 / 2000 : loss = 1.0194685Eopch:     6 mean_loss = 3.628097\n",
      " 2000 / 2000 : loss = 0.9292346Eopch:     7 mean_loss = 3.508072\n",
      " 2000 / 2000 : loss = 3.5984125Eopch:     8 mean_loss = 3.285920\n",
      " 2000 / 2000 : loss = 8.3393984Eopch:     9 mean_loss = 3.180617\n",
      " 2000 / 2000 : loss = 7.2266784Eopch:    10 mean_loss = inf\n",
      "best_loss = 3.180617\n",
      " 1999 / 2000 Completed buliding index in 1 seconds\n",
      "topk=5, mHR=0.758270, mAP=0.745401, mRR=0.962952\n",
      "topk=10, mHR=0.758370, mAP=0.740716, mRR=0.943948\n",
      "topk=15, mHR=0.758323, mAP=0.738289, mRR=0.932298\n",
      "topk=20, mHR=0.758617, mAP=0.737108, mRR=0.924433\n",
      "Accuracy: 0.760200\n",
      "[[11679   407  2726    42]\n",
      " [  543   141   257     3]\n",
      " [  264   121  3370    33]\n",
      " [  242     3   155    14]]\n",
      "Sensitivity of Normal: 0.786253\n",
      "Sensitivity of Edema: 0.149364\n",
      "Sensitivity of Pneumonia: 0.889652\n",
      "Sensitivity of Fracture: 0.033816\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = ATHNet(hash_size=36, type_size=4).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "criterion  = CircleLoss().cuda() #define TripletLoss \n",
    "#train model\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "trI = np.array(trI)\n",
    "trY = np.array(trY)\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trY)))\n",
    "    train_x = trI[shuffled_idx]\n",
    "    train_y = trY[shuffled_idx]\n",
    "    num_batches = len(trY) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY), (i+1)*batchSize])\n",
    "        x_batch = torch.from_numpy(train_x[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        y_batch = torch.from_numpy(train_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        out_batch,_ = model(x_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        loss = criterion(out_batch,y_batch) #F.log_softmax+F.nll_loss\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "criterion=criterion.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#hash code of train data from model\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch,_ = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch,_ = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):#perfromance\n",
    "        stype = teY[i]\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"topk={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(topk, np.mean(MHR),np.mean(MAP),np.mean(MRR)))\n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels )\n",
    "print (cm)\n",
    "print ('Sensitivity of Normal: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of Edema: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of Pneumonia: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of Fracture: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=0, mHR=0.786906, mAP=0.771929, mRR=1.000000\n",
      "label=1, mHR=0.146398, mAP=0.103561, mRR=1.000000\n",
      "label=2, mHR=0.879857, mAP=0.857105, mRR=1.000000\n",
      "label=3, mHR=0.018357, mAP=0.008742, mRR=0.125000\n"
     ]
    }
   ],
   "source": [
    "for cls in labels:\n",
    "    teY_cls = np.array(teY)[np.where( np.array(teY) == cls)[0]]\n",
    "    teF_cls = np.array(teF)[np.where( np.array(teY) == cls)[0]]\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = 0.0 #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF_cls).astype('float32'), k=10)\n",
    "    for i, teVal in enumerate(teF_cls):#perfromance\n",
    "        stype = teY_cls[i]\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR= pos_len/rank_len\n",
    "                    mrr_flag = 1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"label={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(cls, np.mean(MHR),np.mean(MAP),np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ATH+focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2000 / 2000 : loss = 0.090194Eopch:     1 mean_loss = 0.018596\n",
      " 2000 / 2000 : loss = 0.000774Eopch:     2 mean_loss = 0.011180\n",
      " 2000 / 2000 : loss = 0.010479Eopch:     3 mean_loss = 0.009744\n",
      " 2000 / 2000 : loss = 0.001551Eopch:     4 mean_loss = 0.008319\n",
      " 2000 / 2000 : loss = 0.022994Eopch:     5 mean_loss = 0.007469\n",
      " 2000 / 2000 : loss = 0.001793Eopch:     6 mean_loss = 0.006831\n",
      " 2000 / 2000 : loss = 0.016303Eopch:     7 mean_loss = 0.006214\n",
      " 2000 / 2000 : loss = 0.010128Eopch:     8 mean_loss = 0.005845\n",
      " 2000 / 2000 : loss = 0.001841Eopch:     9 mean_loss = 0.005276\n",
      " 2000 / 2000 : loss = 0.011183Eopch:    10 mean_loss = 0.005058\n",
      "best_loss = 0.005058\n",
      " 1999 / 2000 Completed buliding index in 1 seconds\n",
      "topk=5, mHR=0.744360, mAP=0.709509, mRR=0.920276\n",
      "topk=10, mHR=0.742235, mAP=0.696114, mRR=0.894587\n",
      "topk=15, mHR=0.741500, mAP=0.690030, mRR=0.880695\n",
      "topk=20, mHR=0.740827, mAP=0.686240, mRR=0.871135\n",
      "Accuracy: 0.747400\n",
      "[[12602   833  1362    57]\n",
      " [  634   120   189     1]\n",
      " [ 1431    96  2217    44]\n",
      " [  332    10    63     9]]\n",
      "Sensitivity of Normal: 0.848391\n",
      "Sensitivity of Edema: 0.127119\n",
      "Sensitivity of Pneumonia: 0.585269\n",
      "Sensitivity of Fracture: 0.021739\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = ATHNet(hash_size=36, type_size=4).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "criterion  = FocalLoss(gamma=2,alpha=[0.2,0.4,0.2,0.15,0.05]).cuda() #define ce mutli-classes, \n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "trI = np.array(trI)\n",
    "trY = np.array(trY)\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trY)))\n",
    "    train_x = trI[shuffled_idx]\n",
    "    train_y = trY[shuffled_idx]\n",
    "    num_batches = len(trY) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY), (i+1)*batchSize])\n",
    "        x_batch = torch.from_numpy(train_x[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        y_batch = torch.from_numpy(train_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        _, out_batch = model(x_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        loss = criterion(out_batch,y_batch) #F.log_softmax+F.nll_loss\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "criterion = criterion.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    feature, _ = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    feature = feature.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(feature.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "teY_pred = []\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    feature, out_batch = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(feature.cpu().data.numpy().tolist()) #record feature\n",
    "    out_batch = F.log_softmax(out_batch,dim=1) \n",
    "    pred = out_batch.max(1,keepdim=True)[1]\n",
    "    teY_pred.extend(pred.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):#perfromance\n",
    "        stype = teY[i]\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"topk={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(topk, np.mean(MHR),np.mean(MAP),np.mean(MRR)))\n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels )\n",
    "print (cm)\n",
    "print ('Sensitivity of Normal: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of Edema: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of Pneumonia: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of Fracture: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=0, mHR=0.861573, mAP=0.822008, mRR=1.000000\n",
      "label=1, mHR=0.097669, mAP=0.054774, mRR=1.000000\n",
      "label=2, mHR=0.513516, mAP=0.437300, mRR=1.000000\n",
      "label=3, mHR=0.022947, mAP=0.009587, mRR=0.500000\n"
     ]
    }
   ],
   "source": [
    "for cls in labels:\n",
    "    teY_cls = np.array(teY)[np.where( np.array(teY) == cls)[0]]\n",
    "    teF_cls = np.array(teF)[np.where( np.array(teY) == cls)[0]]\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = 0.0 #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF_cls).astype('float32'), k=10)\n",
    "    for i, teVal in enumerate(teF_cls):#perfromance\n",
    "        stype = teY_cls[i]\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR= pos_len/rank_len\n",
    "                    mrr_flag = 1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"label={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(cls, np.mean(MHR),np.mean(MAP),np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ATH+CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2000 / 2000 : loss = 1.001225Eopch:     1 mean_loss = 0.168930\n",
      " 2000 / 2000 : loss = 0.004424Eopch:     2 mean_loss = 0.106988\n",
      " 2000 / 2000 : loss = 0.104603Eopch:     3 mean_loss = 0.094448\n",
      " 2000 / 2000 : loss = 0.009696Eopch:     4 mean_loss = 0.084006\n",
      " 2000 / 2000 : loss = 0.687054Eopch:     5 mean_loss = 0.078320\n",
      " 2000 / 2000 : loss = 0.185454Eopch:     6 mean_loss = 0.072163\n",
      " 2000 / 2000 : loss = 0.010744Eopch:     7 mean_loss = 0.067127\n",
      " 2000 / 2000 : loss = 0.024265Eopch:     8 mean_loss = 0.064203\n",
      " 2000 / 2000 : loss = 0.018953Eopch:     9 mean_loss = 0.060996\n",
      " 2000 / 2000 : loss = 0.001119Eopch:    10 mean_loss = 0.057120\n",
      "best_loss = 0.057120\n",
      " 1999 / 2000 Completed buliding index in 1 seconds\n",
      "topk=5, mHR=0.701590, mAP=0.663227, mRR=0.906416\n",
      "topk=10, mHR=0.701515, mAP=0.649574, mRR=0.870855\n",
      "topk=15, mHR=0.701643, mAP=0.643353, mRR=0.852944\n",
      "topk=20, mHR=0.702002, mAP=0.639781, mRR=0.841875\n",
      "Accuracy: 0.706950\n",
      "[[11445  1874  1487    48]\n",
      " [  652   158   132     2]\n",
      " [ 1077   148  2523    40]\n",
      " [  314    12    75    13]]\n",
      "Sensitivity of Normal: 0.770500\n",
      "Sensitivity of Edema: 0.167373\n",
      "Sensitivity of Pneumonia: 0.666051\n",
      "Sensitivity of Fracture: 0.031401\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = ATHNet(hash_size=36, type_size=4).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "criterion  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "trI = np.array(trI)\n",
    "trY = np.array(trY)\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trY)))\n",
    "    train_x = trI[shuffled_idx]\n",
    "    train_y = trY[shuffled_idx]\n",
    "    num_batches = len(trY) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY), (i+1)*batchSize])\n",
    "        x_batch = torch.from_numpy(train_x[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        y_batch = torch.from_numpy(train_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        _,out_batch = model(x_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        loss = criterion(out_batch,y_batch) #F.log_softmax+F.nll_loss\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "criterion = criterion.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    feature, _ = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    feature = feature.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(feature.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "teY_pred = []\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    feature, out_batch = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(feature.cpu().data.numpy().tolist()) #record feature\n",
    "    out_batch = F.log_softmax(out_batch,dim=1) \n",
    "    pred = out_batch.max(1,keepdim=True)[1]\n",
    "    teY_pred.extend(pred.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):#perfromance\n",
    "        stype = teY[i]\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"topk={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(topk, np.mean(MHR),np.mean(MAP),np.mean(MRR)))\n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels )\n",
    "print (cm)\n",
    "print ('Sensitivity of Normal: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of Edema: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of Pneumonia: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of Fracture: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=0, mHR=0.782813, mAP=0.734045, mRR=1.000000\n",
      "label=1, mHR=0.154237, mAP=0.102608, mRR=0.200000\n",
      "label=2, mHR=0.592793, mAP=0.524021, mRR=1.000000\n",
      "label=3, mHR=0.027295, mAP=0.014792, mRR=0.333333\n"
     ]
    }
   ],
   "source": [
    "for cls in labels:\n",
    "    teY_cls = np.array(teY)[np.where( np.array(teY) == cls)[0]]\n",
    "    teF_cls = np.array(teF)[np.where( np.array(teY) == cls)[0]]\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = 0.0 #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF_cls).astype('float32'), k=10)\n",
    "    for i, teVal in enumerate(teF_cls):#perfromance\n",
    "        stype = teY_cls[i]\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR= pos_len/rank_len\n",
    "                    mrr_flag = 1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"label={}, mHR={:.6f}, mAP={:.6f}, mRR={:.6f}\".format(cls, np.mean(MHR),np.mean(MAP),np.mean(MRR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "tl_loss=tl_loss.cpu()\n",
    "ce_loss=ce_loss.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
