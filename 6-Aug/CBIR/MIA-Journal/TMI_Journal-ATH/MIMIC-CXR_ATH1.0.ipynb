{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Develop Env: linux+cuda9+python3+opencv+pytorch\n",
    "2.Performance Metric for unbalanced sample(triplet loss): \n",
    "  1)AUC (Area Under Curve);\n",
    "  2)Specificity(Spe): for evaluating the misdiagnosis rate of normal\n",
    "  3)Sensitivity(Sen): for evaluating the missed diagnosis rate of abnorml(S,V,F)\n",
    "3.Performance Metric for retrieval (Spatial Attention Mechanism):\n",
    "  1)MHR(Mean Hit Ratio):  for evaluating the precison of relevance retrieval;\n",
    "  2)MAP(Mean Average Precision): for evaluation the rank of relevance retrieval;\n",
    "  3)MRR(Mean Reciprocal Rank): for evaluation the first hit rank of relevance retrieval;\n",
    "4.Algorithm: Attention-based Triplet Hashing Network(ATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import itertools  \n",
    "import gc\n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc,roc_auc_score  \n",
    "from functools import reduce\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.set_device(1)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True) #resnet\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ATHNet(nn.Module):\n",
    "    def __init__(self, hash_size: int, type_size: int):\n",
    "        super().__init__()\n",
    "        #resnet and maxpool\n",
    "        self.net1 = nn.Sequential(#(3,256,256)->(16,128,128)\n",
    "            ResBlock(in_channels=3, out_channels=16, stride=2), \n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        )\n",
    "        \n",
    "        #Attention (16,128,128)->(16,128,128)\n",
    "        self.sa = SpatialAttention()\n",
    "        \n",
    "        #resnet and meanpool\n",
    "        self.net2 =nn.Sequential( #(16,128,128)->(8,64,64)\n",
    "            ResBlock(in_channels=16, out_channels=8, stride=2),\n",
    "            nn.AvgPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        ) \n",
    "         \n",
    "        #fully connected with conv (8,64,64)->(1,32,32)\n",
    "        self.dense=ResBlock(in_channels=8, out_channels=1, stride=2)\n",
    "        #fully connected (1,32,32)->class_size\n",
    "        self.hashlayer = nn.Linear(1*32*32, hash_size)\n",
    "        self.typelayer = nn.Linear(1*32*32, type_size)\n",
    "    \n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.sa(x)*x\n",
    "        x = self.net2(x)\n",
    "        x = self.dense(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x_hash = self.hashlayer(x)\n",
    "        x_type = self.typelayer(x)\n",
    "        return x_hash, x_type\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "#https://github.com/luyajie/triplet-deep-hash-pytorch#triplet-deep-hash-pytorch            \n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self,H_q,H_p,H_n):    \n",
    "        margin_val = self.margin * H_q.shape[1]\n",
    "        squared_loss_pos = torch.mean(self.mse_loss(H_q, H_p), dim=1)\n",
    "        squared_loss_neg = torch.mean(self.mse_loss(H_q, H_n), dim=1)\n",
    "        zeros = torch.zeros_like(squared_loss_neg)\n",
    "        loss  = torch.max(zeros, margin_val - squared_loss_neg + squared_loss_pos)\n",
    "        return torch.mean(loss)\n",
    "    \n",
    "#https://github.com/marvis/pytorch-yolo2/blob/master/FocalLoss.py\n",
    "#https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py  \n",
    "class FocalLoss(nn.Module):\n",
    "    #Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, out, y):\n",
    "        y = y.view(-1,1)\n",
    "        logpt = F.log_softmax(out,dim=1)#default ,dim=1\n",
    "        logpt = logpt.gather(1,y)# dim=1, index=y, max\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=out.data.type():\n",
    "                self.alpha = self.alpha.type_as(out.data)\n",
    "            at = self.alpha.gather(0,y.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()\n",
    "#https://github.com/qianjinhao/circle-loss\n",
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, scale=32, margin=0.25, similarity='cos', **kwargs):\n",
    "        super(CircleLoss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.similarity = similarity\n",
    "\n",
    "    def forward(self, feats, labels):\n",
    "        assert feats.size(0) == labels.size(0), \\\n",
    "            f\"feats.size(0): {feats.size(0)} is not equal to labels.size(0): {labels.size(0)}\"\n",
    "        batch_size = feats.size(0)\n",
    "        if self.similarity == 'dot':\n",
    "            sim_mat = torch.matmul(feats, torch.t(feats))\n",
    "        elif self.similarity == 'cos':\n",
    "            feats = F.normalize(feats)\n",
    "            sim_mat = feats.mm(feats.t())\n",
    "        else:\n",
    "            raise ValueError('This similarity is not implemented.')\n",
    "        loss = list()\n",
    "        for i in range(batch_size):\n",
    "            pos_index = labels == labels[i]\n",
    "            pos_index[i] = 0\n",
    "            neg_index = labels != labels[i]\n",
    "            pos_pair_ = sim_mat[i][pos_index]\n",
    "            neg_pair_ = sim_mat[i][neg_index]\n",
    "\n",
    "            alpha_p = torch.relu(-pos_pair_ + 1 + self.margin)\n",
    "            alpha_n = torch.relu(neg_pair_ + self.margin)\n",
    "            margin_p = 1 - self.margin\n",
    "            margin_n = self.margin\n",
    "            loss_p = torch.sum(torch.exp(-self.scale * alpha_p * (pos_pair_ - margin_p)))\n",
    "            loss_n = torch.sum(torch.exp(self.scale * alpha_n * (neg_pair_ - margin_n)))\n",
    "            loss.append(torch.log(1 + loss_p * loss_n))\n",
    "\n",
    "        loss = sum(loss) / batch_size\n",
    "        return loss    \n",
    "\n",
    "#define loss function:pairwise loss            \n",
    "class PairwiseLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(PairwiseLoss, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.l1_loss = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    def forward(self,h1,h2,y):    \n",
    "        margin_val = self.margin * h1.shape[1]\n",
    "        squared_loss = torch.mean(self.mse_loss(h1, h2), dim=1)\n",
    "        # T1: 0.5 * (1 - y) * dist(x1, x2)\n",
    "        positive_pair_loss = (0.5 * (1 - y) * squared_loss)\n",
    "        mean_positive_pair_loss = torch.mean(positive_pair_loss)\n",
    "        # T2: 0.5 * y * max(margin - dist(x1, x2), 0)\n",
    "        zeros = torch.zeros_like(squared_loss)\n",
    "        marginMat = margin_val * torch.ones_like(squared_loss)\n",
    "        negative_pair_loss = 0.5 * y * torch.max(zeros, marginMat - squared_loss)\n",
    "        mean_negative_pair_loss = torch.mean(negative_pair_loss)\n",
    "\n",
    "        # T3: alpha(dst_l1(abs(x1), 1)) + dist_l1(abs(x2), 1)))\n",
    "        mean_value_regularization = self.alpha * (\n",
    "                self.l1_loss(torch.abs(h1), torch.ones_like(h1)) +\n",
    "                self.l1_loss(torch.abs(h2), torch.ones_like(h2)))\n",
    "\n",
    "        loss = mean_positive_pair_loss + mean_negative_pair_loss + mean_value_regularization\n",
    "        return loss\n",
    "    \n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])#[0:4555]\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    idx_2 = np.where( np.array(trY) == 2 ) #class 2\n",
    "    idx_2 = list(idx_2[0])\n",
    "    idx_sf.extend(idx_2)\n",
    "    idx_3 = np.where( np.array(trY) == 3 ) #class 3\n",
    "    idx_3 = list(idx_3[0])\n",
    "    idx_sf.extend(idx_3)\n",
    "    idx_4 = np.where( np.array(trY) == 4 ) #class 4\n",
    "    idx_4 = list(idx_4[0])#[0:993]\n",
    "    idx_sf.extend(idx_4)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    trQ_y, trP_y, trN_y = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        trQ_y.append(trY[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        elif trY[iQ] == 2:\n",
    "            idx_tmp = idx_2.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_2))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        elif trY[iQ] == 3:\n",
    "            idx_tmp = idx_3.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_3))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        elif trY[iQ] == 4:\n",
    "            idx_tmp = idx_4.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_4))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf), np.array(trQ_y), np.array(trP_y), np.array(trN_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 / 20000 The length of train set is 20000\n",
      "2000 / 2000 The length of test set is 2000\n",
      "Completed buliding index in 3448 seconds\n"
     ]
    }
   ],
   "source": [
    "#1. Read data with List storage Name:[name],I:[img],Y[type]\n",
    "root_dir = '/data/fjsdata/physionet/MIMIC-CXR' #the path of images\n",
    "trainset = pd.read_csv(\"/data/fjsdata/physionet/MIMIC-CXR/CBIR_train.csv\" , sep=',')#load dataset\n",
    "testset = pd.read_csv(\"/data/fjsdata/physionet/MIMIC-CXR/CBIR_test.csv\" , sep=',')#load dataset\n",
    "tstart = time.time()\n",
    "#read train image with CV\n",
    "trN, trI, trY = [],[],[]\n",
    "for iname, itype in np.array(trainset).tolist():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, iname)\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))\n",
    "        trN.append(iname)\n",
    "        trI.append(img)\n",
    "        trY.append(itype)\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(trN),trainset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(trN))\n",
    "#read test image with CV\n",
    "teN, teI, teY = [],[],[]\n",
    "for iname, itype in np.array(testset).tolist():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, iname)\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))\n",
    "        teN.append(iname)\n",
    "        teI.append(img)\n",
    "        teY.append(itype)  \n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(teN),testset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of test set is %d'%len(teN))\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2000 / 2000 : loss = 14.672886Eopch:     1 mean_loss = 18.992300\n",
      " 2000 / 2000 : loss = 19.174988Eopch:     2 mean_loss = 18.120743\n",
      " 2000 / 2000 : loss = 15.079523Eopch:     3 mean_loss = 17.773632\n",
      " 2000 / 2000 : loss = 20.981371Eopch:     4 mean_loss = 17.433834\n",
      " 2000 / 2000 : loss = 19.018032Eopch:     5 mean_loss = 17.035319\n",
      " 2000 / 2000 : loss = 15.990587Eopch:     6 mean_loss = 16.647696\n",
      " 2000 / 2000 : loss = 22.902557Eopch:     7 mean_loss = 16.187693\n",
      " 2000 / 2000 : loss = 13.909217Eopch:     8 mean_loss = 15.653589\n",
      " 2000 / 2000 : loss = 13.953316Eopch:     9 mean_loss = 15.125361\n",
      " 2000 / 2000 : loss = 10.396567Eopch:    10 mean_loss = 14.481414\n",
      " 2000 / 2000 : loss = 10.272303Eopch:    11 mean_loss = 13.869727\n",
      " 2000 / 2000 : loss = 9.8454238Eopch:    12 mean_loss = 13.164536\n",
      " 2000 / 2000 : loss = 13.520651Eopch:    13 mean_loss = 12.554515\n",
      " 2000 / 2000 : loss = 18.417803Eopch:    14 mean_loss = 11.835068\n",
      " 2000 / 2000 : loss = 13.725584Eopch:    15 mean_loss = 11.147409\n",
      " 2000 / 2000 : loss = 12.972666Eopch:    16 mean_loss = 10.514611\n",
      " 2000 / 2000 : loss = 10.503733Eopch:    17 mean_loss = 9.959741\n",
      " 2000 / 2000 : loss = 8.1571878Eopch:    18 mean_loss = 9.353742\n",
      " 2000 / 2000 : loss = 12.456951Eopch:    19 mean_loss = 8.678941\n",
      " 2000 / 2000 : loss = 5.3942568Eopch:    20 mean_loss = 8.222612\n",
      " 2000 / 2000 : loss = 14.347718Eopch:    21 mean_loss = 7.710883\n",
      " 2000 / 2000 : loss = 6.0134699Eopch:    22 mean_loss = 7.259632\n",
      " 2000 / 2000 : loss = 8.8670376Eopch:    23 mean_loss = 6.820505\n",
      " 2000 / 2000 : loss = 11.817217Eopch:    24 mean_loss = 6.448830\n",
      " 2000 / 2000 : loss = 2.7204761Eopch:    25 mean_loss = 6.101332\n",
      " 2000 / 2000 : loss = 5.1095224Eopch:    26 mean_loss = 5.832196\n",
      " 2000 / 2000 : loss = 6.4795954Eopch:    27 mean_loss = 5.624932\n",
      " 2000 / 2000 : loss = 6.3631323Eopch:    28 mean_loss = 5.362428\n",
      " 2000 / 2000 : loss = 3.6090572Eopch:    29 mean_loss = 5.103683\n",
      " 2000 / 2000 : loss = 3.5891685Eopch:    30 mean_loss = 5.055139\n",
      " 2000 / 2000 : loss = 6.1123846Eopch:    31 mean_loss = 4.915538\n",
      " 2000 / 2000 : loss = 4.7930779Eopch:    32 mean_loss = 4.719033\n",
      " 2000 / 2000 : loss = 3.9810342Eopch:    33 mean_loss = 4.655895\n",
      " 2000 / 2000 : loss = 5.8782895Eopch:    34 mean_loss = 4.550985\n",
      " 2000 / 2000 : loss = 3.4503621Eopch:    35 mean_loss = 4.535462\n",
      " 2000 / 2000 : loss = 3.1848871Eopch:    36 mean_loss = 4.428725\n",
      " 2000 / 2000 : loss = 2.5747927Eopch:    37 mean_loss = 4.368983\n",
      " 2000 / 2000 : loss = 3.9969826Eopch:    38 mean_loss = 4.320443\n",
      " 2000 / 2000 : loss = 7.6602617Eopch:    39 mean_loss = 4.214999\n",
      " 2000 / 2000 : loss = 5.0250358Eopch:    40 mean_loss = 4.227203\n",
      " 2000 / 2000 : loss = 3.2269042Eopch:    41 mean_loss = 4.250119\n",
      " 2000 / 2000 : loss = 6.5486253Eopch:    42 mean_loss = 4.055396\n",
      " 2000 / 2000 : loss = 4.0079991Eopch:    43 mean_loss = 4.074586\n",
      " 2000 / 2000 : loss = 3.0558422Eopch:    44 mean_loss = 4.017971\n",
      " 2000 / 2000 : loss = 3.7593291Eopch:    45 mean_loss = 4.043517\n",
      " 2000 / 2000 : loss = 3.4419361Eopch:    46 mean_loss = 4.044961\n",
      " 2000 / 2000 : loss = 3.6845979Eopch:    47 mean_loss = 3.938926\n",
      " 2000 / 2000 : loss = 4.0696157Eopch:    48 mean_loss = 3.960851\n",
      " 2000 / 2000 : loss = 2.8933298Eopch:    49 mean_loss = 3.932363\n",
      " 2000 / 2000 : loss = 2.4447069Eopch:    50 mean_loss = 3.904994\n",
      "best_loss = 3.904994\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "#ATH-Triplet+CE\n",
    "#--------------------------------------------------------\n",
    "#sample  triplet labels\n",
    "#trQ_sf, trP_sf, trN_sf, trQ_y, trP_y, trN_y = onlineGenImgPairs() \n",
    "assert (trQ_sf.shape==trP_sf.shape and trQ_sf.shape==trN_sf.shape)\n",
    "assert (trQ_y.shape==trP_y.shape and trQ_y.shape==trN_y.shape)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trP_y))!=0,1,0))==0.0)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trN_y))!=0,1,0))==1.0)\n",
    "\n",
    "#define model\n",
    "model = ATHNet(hash_size=36, type_size=5).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "tl_loss  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "ce_loss  = nn.CrossEntropyLoss().cuda() #define cross-entropy loss\n",
    "\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "plot_tripletloss = []\n",
    "for epoch in range(50):#iteration\n",
    "    losses, hash_losses, class_loss = [], [], []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_q_y = trQ_y[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_p_y = trP_y[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    train_n_y = trN_y[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Q_y_batch = torch.from_numpy(train_q_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_y_batch = torch.from_numpy(train_p_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_y_batch = torch.from_numpy(train_n_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash, Q_type = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash, P_type = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash, N_type = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #loss,#F.log_softmax+F.nll_loss\n",
    "        hash_loss = tl_loss(Q_hash,P_hash,N_hash)\n",
    "        type_loss = ce_loss(Q_type,Q_y_batch) + ce_loss(P_type,P_y_batch) + ce_loss(N_type,N_y_batch) \n",
    "        loss = hash_loss+type_loss\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "        hash_losses.append(hash_loss.item())\n",
    "        class_loss.append(type_loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    plot_tripletloss.append(np.mean(hash_losses))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "tl_loss=tl_loss.cpu()\n",
    "ce_loss=ce_loss.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 199 / 200 0 Completed buliding index in 22 seconds\n",
      "mHR@10=0.488300, mAP@10=0.381058, mRR@10=0.711896\n",
      "[[754  94 101  27  24]\n",
      " [ 85 308  89   8  10]\n",
      " [131  99  54   8   8]\n",
      " [ 53  21  15   5   6]\n",
      " [ 50  25  14   5   6]]\n",
      "Sensitivity(TPR) of Normal: 0.754000\n",
      "Sensitivity(TPR) of Edema: 0.616000\n",
      "Sensitivity(TPR) of Pneumonia: 0.180000\n",
      "Sensitivity(TPR) of Lung Lesion: 0.050000\n",
      "Sensitivity(TPR) of Fracture: 0.060000\n",
      "AUC (Area Under Curve) of Micro: 0.813364\n"
     ]
    }
   ],
   "source": [
    "#hash code of train data from model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize \n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch,_ = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teY_pred = []\n",
    "teY_prob = []\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, x_type = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "    x_type = F.log_softmax(x_type,dim=1) \n",
    "    teY_prob.extend(x_type.cpu().data.numpy().tolist())\n",
    "    pred = x_type.max(1,keepdim=True)[1]\n",
    "    teY_pred.extend(pred.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "#performance of retrieval\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))\n",
    "#performance of classification\n",
    "#https://blog.csdn.net/hlang8160/article/details/78040311\n",
    "#https://www.jianshu.com/p/7919ef304b19\n",
    "#https://www.jianshu.com/p/c61ae11cc5f6\n",
    "#https://www.cnblogs.com/yanshw/p/12691329.html\n",
    "#TNR= TN / (FP + TN) ->low misdiagnosis rate->Specificity\n",
    "#TPR=TP / (TP+ FN) -> low missed diagnosis rate->Sensitivity\n",
    "#ROC: x axis = 1-TNR, y asis=TPR\n",
    "#print ( 'Accuracy: %.6f'%accuracy_score(teY, teY_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, teY_pred, labels=labels) \n",
    "print (cm)\n",
    "print ('Sensitivity(TPR) of Normal: %.6f'%float(cm[0][0]/np.sum(cm[0]))) \n",
    "print ('Sensitivity(TPR) of Edema: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity(TPR) of Pneumonia: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity(TPR) of Lung Lesion: %.6f'%float(cm[3][3]/np.sum(cm[3])))\n",
    "print ('Sensitivity(TPR) of Fracture: %.6f'%float(cm[4][4]/np.sum(cm[4])))\n",
    "#auc and roc\n",
    "teY_one_hot = label_binarize(np.array(teY), np.arange(len(labels)))\n",
    "auc_score = roc_auc_score(teY_one_hot, np.array(teY_prob), average='micro')#macro\n",
    "print ('AUC (Area Under Curve) of Micro: %.6f'% auc_score)\n",
    "fpr_tce, tpr_tce, thresholds = roc_curve(teY_one_hot.ravel(),np.array(teY_prob).ravel()) #for roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/p14648341/s01/view1_frontal.jpg:2-train/p17762094/s15/view1_frontal.jpg:2-Distance:0.170725\n",
      "train/p14648341/s01/view1_frontal.jpg:2-train/p19654137/s02/view1_frontal.jpg:1-Distance:0.199190\n",
      "train/p14648341/s01/view1_frontal.jpg:2-train/p18148694/s15/view1_frontal.jpg:1-Distance:0.202320\n",
      "train/p14648341/s01/view1_frontal.jpg:2-train/p19338803/s12/view1_frontal.jpg:0-Distance:0.206553\n",
      "train/p14648341/s01/view1_frontal.jpg:2-train/p14860546/s01/view1_frontal.jpg:0-Distance:0.208226\n",
      "train/p14670441/s05/view1_frontal.jpg:2-train/p16724859/s05/view1_frontal.jpg:2-Distance:0.212337\n",
      "train/p14670441/s05/view1_frontal.jpg:2-train/p15149227/s27/view1_frontal.jpg:1-Distance:0.229466\n",
      "train/p14670441/s05/view1_frontal.jpg:2-train/p15944907/s31/view1_frontal.jpg:1-Distance:0.234595\n",
      "train/p14670441/s05/view1_frontal.jpg:2-train/p15784687/s05/view1_frontal.jpg:1-Distance:0.236564\n",
      "train/p14670441/s05/view1_frontal.jpg:2-train/p13270755/s08/view1_frontal.jpg:3-Distance:0.250231\n",
      "train/p10490202/s03/view1_frontal.jpg:2-train/p15971063/s24/view1_frontal.jpg:2-Distance:0.177897\n",
      "train/p10490202/s03/view1_frontal.jpg:2-train/p17836926/s02/view1_frontal.jpg:3-Distance:0.179582\n",
      "train/p10490202/s03/view1_frontal.jpg:2-train/p16622129/s03/view1_frontal.jpg:0-Distance:0.202028\n",
      "train/p10490202/s03/view1_frontal.jpg:2-train/p11936095/s11/view1_frontal.jpg:0-Distance:0.207545\n",
      "train/p10490202/s03/view1_frontal.jpg:2-train/p13953303/s01/view1_frontal.jpg:0-Distance:0.215138\n",
      "train/p13628670/s05/view1_frontal.jpg:2-train/p10402135/s01/view1_frontal.jpg:2-Distance:0.232876\n",
      "train/p13628670/s05/view1_frontal.jpg:2-train/p17610678/s10/view1_frontal.jpg:2-Distance:0.237969\n",
      "train/p13628670/s05/view1_frontal.jpg:2-train/p18753518/s04/view1_frontal.jpg:2-Distance:0.241106\n",
      "train/p13628670/s05/view1_frontal.jpg:2-train/p10441044/s12/view1_frontal.jpg:4-Distance:0.247967\n",
      "train/p13628670/s05/view1_frontal.jpg:2-train/p16656904/s01/view1_frontal.jpg:1-Distance:0.250868\n",
      "train/p19081213/s04/view1_frontal.jpg:2-train/p18041094/s01/view1_frontal.jpg:2-Distance:0.175764\n",
      "train/p19081213/s04/view1_frontal.jpg:2-train/p11069015/s01/view1_frontal.jpg:1-Distance:0.187047\n",
      "train/p19081213/s04/view1_frontal.jpg:2-train/p14873817/s11/view1_frontal.jpg:1-Distance:0.190051\n",
      "train/p19081213/s04/view1_frontal.jpg:2-train/p14300144/s02/view1_frontal.jpg:1-Distance:0.201656\n",
      "train/p19081213/s04/view1_frontal.jpg:2-train/p10148417/s12/view1_frontal.jpg:2-Distance:0.203664\n",
      "train/p13724605/s10/view1_frontal.jpg:2-train/p13724605/s04/view1_frontal.jpg:2-Distance:0.144088\n",
      "train/p13724605/s10/view1_frontal.jpg:2-train/p14061701/s03/view1_frontal.jpg:0-Distance:0.187610\n",
      "train/p13724605/s10/view1_frontal.jpg:2-train/p10319873/s03/view1_frontal.jpg:0-Distance:0.189887\n",
      "train/p13724605/s10/view1_frontal.jpg:2-train/p10520955/s01/view1_frontal.jpg:0-Distance:0.190302\n",
      "train/p13724605/s10/view1_frontal.jpg:2-train/p19054786/s05/view2_frontal.jpg:2-Distance:0.194455\n",
      "train/p14398566/s08/view1_frontal.jpg:2-train/p11160857/s03/view1_frontal.jpg:2-Distance:0.182440\n",
      "train/p14398566/s08/view1_frontal.jpg:2-train/p11494099/s02/view1_frontal.jpg:0-Distance:0.198803\n",
      "train/p14398566/s08/view1_frontal.jpg:2-train/p10083678/s01/view1_frontal.jpg:0-Distance:0.215877\n",
      "train/p14398566/s08/view1_frontal.jpg:2-train/p12017780/s03/view1_frontal.jpg:0-Distance:0.217021\n",
      "train/p14398566/s08/view1_frontal.jpg:2-train/p15885814/s01/view1_frontal.jpg:0-Distance:0.221664\n",
      "train/p14936398/s12/view1_frontal.jpg:2-train/p13934278/s06/view1_frontal.jpg:2-Distance:0.216687\n",
      "train/p14936398/s12/view1_frontal.jpg:2-train/p15690056/s03/view2_frontal.jpg:2-Distance:0.225310\n",
      "train/p14936398/s12/view1_frontal.jpg:2-train/p13526309/s35/view2_frontal.jpg:1-Distance:0.243198\n",
      "train/p14936398/s12/view1_frontal.jpg:2-train/p12329981/s03/view1_frontal.jpg:1-Distance:0.244823\n",
      "train/p14936398/s12/view1_frontal.jpg:2-train/p18513809/s20/view1_frontal.jpg:2-Distance:0.245434\n",
      "train/p14979348/s01/view1_frontal.jpg:2-train/p19557539/s02/view1_frontal.jpg:2-Distance:0.187251\n",
      "train/p14979348/s01/view1_frontal.jpg:2-train/p15554865/s12/view1_frontal.jpg:0-Distance:0.196404\n",
      "train/p14979348/s01/view1_frontal.jpg:2-train/p15094687/s02/view1_frontal.jpg:1-Distance:0.205651\n",
      "train/p14979348/s01/view1_frontal.jpg:2-train/p15743778/s08/view1_frontal.jpg:1-Distance:0.222475\n",
      "train/p14979348/s01/view1_frontal.jpg:2-train/p12029365/s13/view1_frontal.jpg:0-Distance:0.225293\n",
      "train/p18551091/s56/view1_frontal.jpg:2-train/p18556017/s11/view1_frontal.jpg:2-Distance:0.196958\n",
      "train/p18551091/s56/view1_frontal.jpg:2-train/p19674244/s85/view1_frontal.jpg:0-Distance:0.221880\n",
      "train/p18551091/s56/view1_frontal.jpg:2-train/p17630048/s02/view1_frontal.jpg:0-Distance:0.230534\n",
      "train/p18551091/s56/view1_frontal.jpg:2-train/p12557602/s07/view1_frontal.jpg:0-Distance:0.230649\n",
      "train/p18551091/s56/view1_frontal.jpg:2-train/p13306384/s04/view2_frontal.jpg:1-Distance:0.231633\n",
      "train/p17173587/s01/view1_frontal.jpg:2-train/p18874543/s12/view1_frontal.jpg:2-Distance:0.285599\n",
      "train/p17173587/s01/view1_frontal.jpg:2-train/p19348515/s02/view1_frontal.jpg:1-Distance:0.285807\n",
      "train/p17173587/s01/view1_frontal.jpg:2-train/p16842320/s01/view1_frontal.jpg:1-Distance:0.289196\n",
      "train/p17173587/s01/view1_frontal.jpg:2-train/p10802399/s01/view2_frontal.jpg:1-Distance:0.316004\n",
      "train/p17173587/s01/view1_frontal.jpg:2-train/p13931230/s01/view1_frontal.jpg:1-Distance:0.318188\n",
      "train/p19722404/s02/view1_frontal.jpg:2-train/p11132868/s03/view1_frontal.jpg:2-Distance:0.147810\n",
      "train/p19722404/s02/view1_frontal.jpg:2-train/p15786017/s03/view1_frontal.jpg:0-Distance:0.155836\n",
      "train/p19722404/s02/view1_frontal.jpg:2-train/p19486351/s01/view1_frontal.jpg:0-Distance:0.171364\n",
      "train/p19722404/s02/view1_frontal.jpg:2-train/p17507655/s04/view1_frontal.jpg:0-Distance:0.174567\n",
      "train/p19722404/s02/view1_frontal.jpg:2-train/p14134981/s01/view1_frontal.jpg:0-Distance:0.174776\n",
      "train/p15503721/s05/view1_frontal.jpg:2-train/p10151556/s10/view1_frontal.jpg:2-Distance:0.150312\n",
      "train/p15503721/s05/view1_frontal.jpg:2-train/p15195362/s08/view1_frontal.jpg:3-Distance:0.169620\n",
      "train/p15503721/s05/view1_frontal.jpg:2-train/p15110016/s07/view1_frontal.jpg:1-Distance:0.170683\n",
      "train/p15503721/s05/view1_frontal.jpg:2-train/p14037995/s09/view1_frontal.jpg:4-Distance:0.178053\n",
      "train/p15503721/s05/view1_frontal.jpg:2-train/p15722937/s12/view1_frontal.jpg:2-Distance:0.179789\n",
      "train/p15610009/s01/view1_frontal.jpg:2-train/p15107848/s11/view1_frontal.jpg:2-Distance:0.248020\n",
      "train/p15610009/s01/view1_frontal.jpg:2-train/p15959458/s01/view1_frontal.jpg:1-Distance:0.248875\n",
      "train/p15610009/s01/view1_frontal.jpg:2-train/p19397036/s08/view2_frontal.jpg:0-Distance:0.283346\n",
      "train/p15610009/s01/view1_frontal.jpg:2-train/p16923182/s02/view1_frontal.jpg:1-Distance:0.283978\n",
      "train/p15610009/s01/view1_frontal.jpg:2-train/p12704088/s13/view1_frontal.jpg:2-Distance:0.284747\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-df76b9d540c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mteVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmap_item_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrVal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mmap_item_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mteVal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrVal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cosine'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#hamming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#figure 1 for problem introduction\n",
    "for i in range(len(teY)):\n",
    "    if teY[i]==2 :#Pneumonia\n",
    "    #if teN[i] in ['28901_right','19414_right']: \n",
    "        itype = teY[i]\n",
    "        teVal = teF[i]\n",
    "        image_path = os.path.join(root_dir, teN[i])\n",
    "        map_item_score = {}\n",
    "        for j, trVal in enumerate(trF):\n",
    "            map_item_score[j] = pdist(np.vstack([teVal,trVal]),'cosine')#hamming\n",
    "        ranklist = heapq.nsmallest(5, map_item_score, key=map_item_score.get)\n",
    "        if trY[ranklist[0]]==2:\n",
    "            for j in ranklist:\n",
    "                print ('%s:%d-%s:%d-Distance:%.6f'%(teN[i],teY[i],trN[j],trY[j],map_item_score[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.523923 -> Pneumonia\n",
      "0.350294 -> Normal\n",
      "0.071372 -> Edema\n",
      "0.042187 -> Lung Lesion\n",
      "0.012224 -> Fracture\n",
      "(1, 1, 32, 32)\n",
      "train/p14972430/s02/view1_frontal.jpg ->top1 prediction: Pneumonia\n",
      "0.402462 -> Pneumonia\n",
      "0.203858 -> Edema\n",
      "0.202981 -> Normal\n",
      "0.149424 -> Fracture\n",
      "0.041275 -> Lung Lesion\n",
      "(1, 1, 32, 32)\n",
      "train/p14683950/s02/view1_frontal.jpg ->top1 prediction: Pneumonia\n",
      "0.657104 -> Pneumonia\n",
      "0.169655 -> Edema\n",
      "0.102156 -> Normal\n",
      "0.060829 -> Fracture\n",
      "0.010256 -> Lung Lesion\n",
      "(1, 1, 32, 32)\n",
      "train/p13269046/s03/view1_frontal.jpg ->top1 prediction: Pneumonia\n",
      "0.386936 -> Pneumonia\n",
      "0.271496 -> Lung Lesion\n",
      "0.167934 -> Fracture\n",
      "0.097991 -> Edema\n",
      "0.075643 -> Normal\n",
      "(1, 1, 32, 32)\n",
      "train/p17686783/s01/view1_frontal.jpg ->top1 prediction: Pneumonia\n",
      "0.622423 -> Pneumonia\n",
      "0.225515 -> Normal\n",
      "0.114624 -> Edema\n",
      "0.026934 -> Lung Lesion\n",
      "0.010504 -> Fracture\n",
      "(1, 1, 32, 32)\n",
      "train/p10516481/s05/view1_frontal.jpg ->top1 prediction: Pneumonia\n",
      "0.697572 -> Pneumonia\n",
      "0.242299 -> Edema\n",
      "0.044893 -> Normal\n",
      "0.010453 -> Fracture\n",
      "0.004783 -> Lung Lesion\n",
      "(1, 1, 32, 32)\n",
      "train/p12213423/s30/view1_frontal.jpg ->top1 prediction: Pneumonia\n",
      "0.515579 -> Pneumonia\n",
      "0.352395 -> Edema\n",
      "0.061711 -> Lung Lesion\n",
      "0.040499 -> Normal\n",
      "0.029815 -> Fracture\n",
      "(1, 1, 32, 32)\n",
      "train/p16812975/s02/view1_frontal.jpg ->top1 prediction: Pneumonia\n",
      "0.508455 -> Pneumonia\n",
      "0.234298 -> Normal\n",
      "0.125250 -> Edema\n",
      "0.119209 -> Fracture\n",
      "0.012789 -> Lung Lesion\n",
      "(1, 1, 32, 32)\n",
      "train/p11633382/s01/view1_frontal.jpg ->top1 prediction: Pneumonia\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5ee5b65f2e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(256, 256) is the model input size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#forword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# generate class activation mapping for the top1 prediction\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = 1,1,32,32#feature_conv.shape\n",
    "    \n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        #cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc,h*w)))\n",
    "        cam = weight_softmax[class_idx]*(feature_conv.reshape((nc,h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "#last conv layer followed with one channel by last fully connected layer\n",
    "final_conv = 'dense' \n",
    "best_net._modules.get(final_conv).register_forward_hook(hook_feature)\n",
    "#get weights parameters\n",
    "params = list(best_net.parameters())\n",
    "#get the last and second last weights, like [classes, hiden nodes]\n",
    "weight_softmax = np.squeeze(params[-2].data.cpu().numpy()) \n",
    "# define class type\n",
    "classes = {0:'Normal', 1: 'Edema', 2: 'Pneumonia', 3:'Lung Lesion', 4:'Fracture', }\n",
    "#read image\n",
    "filename= 1\n",
    "for iname, itype in np.array(testset).tolist():\n",
    "    if itype==2:\n",
    "        root = os.path.join(root_dir, iname)\n",
    "        img = []\n",
    "        img.append( cv2.resize(cv2.imread(root).astype(np.float32), (256, 256)))#(256, 256) is the model input size\n",
    "        data = torch.from_numpy(np.array(img)).type(torch.FloatTensor).cuda()\n",
    "        _,logit = best_net(data.permute(0, 3, 1, 2))#forword\n",
    "        h_x = F.softmax(logit, dim=1).data.squeeze()#softmax\n",
    "        probs, idx = h_x.sort(0, True) #probabilities of classes\n",
    "\n",
    "        if classes[idx[0].item()] == 'Pneumonia':\n",
    "            # output: the prediction\n",
    "            for i in range(0, len(classes)):\n",
    "                line = '{:.6f} -> {}'.format(probs[i], classes[idx[i].item()])\n",
    "                print(line)\n",
    "            #get the class activation maps\n",
    "            print (features_blobs[-1].shape)\n",
    "            feature_layer = features_blobs[-1].squeeze()[0]\n",
    "            feature_layer = cv2.resize(feature_layer, (32, 32))\n",
    "            CAMs = returnCAM(feature_layer, weight_softmax, [idx[0].item()])\n",
    "\n",
    "            # render the CAM and show\n",
    "            print('%s ->top1 prediction: %s' %(iname, classes[idx[0].item()]))\n",
    "            img = cv2.imread(root)\n",
    "            height, width, _ = img.shape\n",
    "            CAM = cv2.resize(CAMs[0], (width, height))\n",
    "            heatmap = cv2.applyColorMap(CAM, cv2.COLORMAP_JET)\n",
    "            #result = heatmap * 0.3 + img * 0.5\n",
    "            #result = heatmap * 0.7 + img * 0.3\n",
    "            result = heatmap \n",
    "            cv2.imwrite('/data/tmpexec/tmi/'+str(filename)+'_cam.jpg', result)\n",
    "            filename = filename +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.657 -> Pneumonia\n",
      "0.170 -> Edema\n",
      "0.102 -> Normal\n",
      "0.061 -> Fracture\n",
      "0.010 -> Lung Lesion\n",
      "(1, 1, 32, 32)\n",
      "output CAM.jpg for the top1 prediction: Pneumonia\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate class activation mapping for the top1 prediction\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = 1,1,32,32#feature_conv.shape\n",
    "    \n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        #cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc,h*w)))\n",
    "        cam = weight_softmax[class_idx]*(feature_conv.reshape((nc,h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "#last conv layer followed with one channel by last fully connected layer\n",
    "final_conv = 'net1' \n",
    "best_net._modules.get(final_conv).register_forward_hook(hook_feature)\n",
    "#get weights parameters\n",
    "params = list(best_net.parameters())\n",
    "#get the last and second last weights, like [classes, hiden nodes]\n",
    "weight_softmax = np.squeeze(params[-2].data.cpu().numpy()) \n",
    "# define class type\n",
    "classes = {0:'Normal', 1: 'Edema', 2: 'Pneumonia', 3:'Lung Lesion', 4:'Fracture', }\n",
    "#read image\n",
    "imagepath=os.path.join(root_dir, 'train/p13269046/s03/view1_frontal.jpg')\n",
    "img = []\n",
    "img.append( cv2.resize(cv2.imread(imagepath).astype(np.float32), (256, 256)))#(256, 256) is the model input size\n",
    "data = torch.from_numpy(np.array(img)).type(torch.FloatTensor).cuda()\n",
    "_,logit = best_net(data.permute(0, 3, 1, 2))#forword\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()#softmax\n",
    "probs, idx = h_x.sort(0, True) #probabilities of classes\n",
    "\n",
    "# output: the prediction\n",
    "for i in range(0, len(classes)):\n",
    "    line = '{:.3f} -> {}'.format(probs[i], classes[idx[i].item()])\n",
    "    print(line)\n",
    "#get the class activation maps\n",
    "print (features_blobs[-1].shape)\n",
    "feature_layer = features_blobs[-1].squeeze()[0]\n",
    "feature_layer = cv2.resize(feature_layer, (32, 32))\n",
    "CAMs = returnCAM(feature_layer, weight_softmax, [idx[0].item()])\n",
    "\n",
    "# render the CAM and show\n",
    "print('output CAM.jpg for the top1 prediction: %s' % classes[idx[0].item()])\n",
    "img = cv2.imread(root)\n",
    "height, width, _ = img.shape\n",
    "CAM = cv2.resize(CAMs[0], (width, height))\n",
    "heatmap = cv2.applyColorMap(CAM, cv2.COLORMAP_JET)\n",
    "#result = heatmap * 0.3 + img * 0.5\n",
    "result = heatmap \n",
    "cv2.imwrite('/data/tmpexec/tmi/cxr_cam.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 128, 128)\n",
      "(16, 128, 128)\n",
      "(128, 128)\n",
      "output CAM.jpg for the top1 prediction: Pneumonia\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (features_blobs[0].shape)\n",
    "feature_layer = features_blobs[0].squeeze()\n",
    "print (feature_layer.shape)\n",
    "feature_layer = np.mean(feature_layer, axis=0)\n",
    "#feature_layer = np.max(feature_layer, axis=0)\n",
    "print (feature_layer.shape)\n",
    "feature_layer = cv2.resize(feature_layer, (32, 32))\n",
    "CAMs = returnCAM(feature_layer, weight_softmax, [idx[0].item()])\n",
    "\n",
    "# render the CAM and show\n",
    "print('output CAM.jpg for the top1 prediction: %s' % classes[idx[0].item()])\n",
    "img = cv2.imread(root)\n",
    "height, width, _ = img.shape\n",
    "CAM = cv2.resize(CAMs[0], (width, height))\n",
    "heatmap = cv2.applyColorMap(CAM, cv2.COLORMAP_JET)\n",
    "#result = heatmap * 0.3 + img * 0.5\n",
    "result = heatmap \n",
    "cv2.imwrite('/data/tmpexec/tmi/cxr_cam.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
