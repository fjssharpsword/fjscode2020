{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Develop Env: linux+cuda9+python3+opencv+pytorch\n",
    "2.Dataset: Fundus-iSee with 10000 images(AMD-720, DR-270, glaucoma-450,myopia-790,norm-7770)\n",
    "        trainset(9000): AMD-648, DR-243, glaucoma-405, myopia-711, norm-6993, \n",
    "        testset(1000): AMD-72, DR-27, glaucoma-45, myopia-79, norm=777\n",
    "3.Performance Metric for unbalanced sample(triplet loss): \n",
    "  1)Accuracy(Acc):  for evaluating the precison of top 1 in the returned list;\n",
    "  2)Specificity(Spe): for evaluating the misdiagnosis rate of normal\n",
    "  3)Sensitivity(Sen): for evaluating the missed diagnosis rate of abnorml(S,V,F)\n",
    "4.Performance Metric for retrieval (Spatial Attention Mechanism):\n",
    "  1)MHR(Mean Hit Ratio):  for evaluating the precison of relevance retrieval;\n",
    "  2)MAP(Mean Average Precision): for evaluation the rank of relevance retrieval;\n",
    "  3)MRR(Mean Reciprocal Rank): for evaluation the first hit rank of relevance retrieval;\n",
    "5.Algorithm: Attention-based Triplet Hashing Network(ATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import itertools  \n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "from scipy.spatial.distance import pdist\n",
    "import cv2\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc \n",
    "from functools import reduce\n",
    "import wfdb#https://github.com/MIT-LCP/wfdb-python\n",
    "from wfdb import processing\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.set_device(0)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2779 / 9000 "
     ]
    }
   ],
   "source": [
    "#Read data with List storage Name:[name],I:[img],Y[type]\n",
    "def TypetoNum(itype): #map the type into number.\n",
    "    if itype =='AMD': return 0\n",
    "    elif itype =='DR': return 1\n",
    "    elif itype =='glaucoma': return 2\n",
    "    elif itype =='myopia': return 3\n",
    "    else: return 4 #norm\n",
    "    \n",
    "root_dir = '/data/fjsdata/fundus/iSee/iSee_multi_dataset/' #the path of images\n",
    "trainset = pd.read_csv(\"/data/fjsdata/fundus/iSee/iSee_multi_dataset/CBIR_iSee_train.csv\" , sep=',')#load trainset\n",
    "testset = pd.read_csv(\"/data/fjsdata/fundus/iSee/iSee_multi_dataset/CBIR_iSee_test.csv\" , sep=',')#load testset\n",
    "tstart = time.time()\n",
    "#read train image with CV\n",
    "trN, trI, trY = [],[],[]\n",
    "norm = 6993\n",
    "for iname, itype in np.array(trainset).tolist():\n",
    "    if iname.endswith(\".jpg\"):\n",
    "        try:\n",
    "            image_dir = root_dir+'img_data_%s'%itype\n",
    "            image_path = os.path.join(image_dir, iname)\n",
    "            if itype == 'norm':\n",
    "                if norm>0:\n",
    "                    img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(1920,1920,3)->(256,256,3)\n",
    "                    trN.append(iname)\n",
    "                    trI.append(img)\n",
    "                    trY.append(TypetoNum(itype))\n",
    "                    norm = norm - 1\n",
    "            else:\n",
    "                img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(1920,1920,3)->(256,256,3)\n",
    "                trN.append(iname)\n",
    "                trI.append(img)\n",
    "                trY.append(TypetoNum(itype))    \n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trN),trainset.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "print('The length of train set is %d'%len(trN))\n",
    "#read test image with CV\n",
    "teN, teI, teY = [],[],[]\n",
    "norm = 777\n",
    "for iname, itype in np.array(testset).tolist():\n",
    "    if iname.endswith(\".jpg\"):\n",
    "        try:\n",
    "            image_dir = root_dir+'img_data_%s'%itype\n",
    "            image_path = os.path.join(image_dir, iname)\n",
    "            if itype == 'norm':\n",
    "                if norm>0:\n",
    "                    img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(1920,1920,3)->(256,256,3)\n",
    "                    teN.append(iname)\n",
    "                    teI.append(img)\n",
    "                    teY.append(TypetoNum(itype))\n",
    "                    norm = norm - 1\n",
    "            else:\n",
    "                img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(1920,1920,3)->(256,256,3)\n",
    "                teN.append(iname)\n",
    "                teI.append(img)\n",
    "                teY.append(TypetoNum(itype)) \n",
    "        except:\n",
    "            print(iname+\":\"+str(image_path))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(teN),testset.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "print('The length of test set is %d'%len(teN))\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed data handle in %d seconds' % int(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 900 / 900 : loss = 18.221262Eopch:     1 mean_loss = 18.010405\n",
      " 900 / 900 : loss = 15.415217Eopch:     2 mean_loss = 17.953204\n",
      " 900 / 900 : loss = 17.485758Eopch:     3 mean_loss = 17.768559\n",
      " 900 / 900 : loss = 16.892576Eopch:     4 mean_loss = 16.718294\n",
      " 900 / 900 : loss = 17.360962Eopch:     5 mean_loss = 15.982806\n",
      " 900 / 900 : loss = 20.503071Eopch:     6 mean_loss = 15.823859\n",
      " 900 / 900 : loss = 21.589684Eopch:     7 mean_loss = 14.751288\n",
      " 900 / 900 : loss = 17.918093Eopch:     8 mean_loss = 14.179495\n",
      " 900 / 900 : loss = 14.9987186Eopch:     9 mean_loss = 14.151638\n",
      " 900 / 900 : loss = 12.933948Eopch:    10 mean_loss = 13.224145\n",
      "best_loss = 13.224145\n",
      " 99 / 100  Completed buliding index in 21 seconds\n",
      "mHR@10=0.713200, mAP@10=0.631033, mRR@10=0.876704\n",
      "Accuracy: 0.736000\n",
      "[[  7   3   4   5  53]\n",
      " [  1   0   1   1  24]\n",
      " [  4   1   2   0  38]\n",
      " [  7   0   9  46  17]\n",
      " [ 38  15  28  15 681]]\n",
      "Specificity of normal: 0.876448\n",
      "Sensitivity of AMD: 0.097222\n",
      "Sensitivity of DR: 0.000000\n",
      "Sensitivity of glaucoma: 0.044444\n",
      "Sensitivity of myopia: 0.582278\n"
     ]
    }
   ],
   "source": [
    "#ATH model with Tripet loss\n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True) #resnet\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ATHNet(nn.Module):\n",
    "    def __init__(self, code_size: int):\n",
    "        super().__init__()\n",
    "        #resnet and maxpool\n",
    "        self.net1 = nn.Sequential(#(3,256,256)->(16,128,128)\n",
    "            ResBlock(in_channels=3, out_channels=16, stride=2), \n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        )\n",
    "        \n",
    "        #Attention (16,128,128)->(16,128,128)\n",
    "        self.sa = SpatialAttention()\n",
    "        \n",
    "        #resnet and meanpool\n",
    "        self.net2 =nn.Sequential( #(16,128,128)->(8,64,64)\n",
    "            ResBlock(in_channels=16, out_channels=8, stride=2),\n",
    "            nn.AvgPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        ) \n",
    "         \n",
    "        #fully connected with conv (8,64,64)->(1,32,32)\n",
    "        self.dense=ResBlock(in_channels=8, out_channels=1, stride=2)\n",
    "        #fully connected (1,32,32)->class_size\n",
    "        self.linear = nn.Linear(1*32*32, code_size)\n",
    "    \n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.sa(x)*x\n",
    "        x = self.net2(x)\n",
    "        x = self.dense(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "#https://github.com/luyajie/triplet-deep-hash-pytorch#triplet-deep-hash-pytorch            \n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self,H_q,H_p,H_n):    \n",
    "        margin_val = self.margin * H_q.shape[1]\n",
    "        squared_loss_pos = torch.mean(self.mse_loss(H_q, H_p), dim=1)\n",
    "        squared_loss_neg = torch.mean(self.mse_loss(H_q, H_n), dim=1)\n",
    "        zeros = torch.zeros_like(squared_loss_neg)\n",
    "        loss  = torch.max(zeros, margin_val - squared_loss_neg + squared_loss_pos)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    idx_2 = np.where( np.array(trY) == 2 ) #class 2\n",
    "    idx_2 = list(idx_2[0])\n",
    "    idx_sf.extend(idx_2)\n",
    "    idx_3 = np.where( np.array(trY) == 3 ) #class 3\n",
    "    idx_3 = list(idx_3[0])\n",
    "    idx_sf.extend(idx_3)\n",
    "    idx_4 = np.where( np.array(trY) == 4 ) #class 4\n",
    "    idx_4 = list(idx_4[0])#[0:993]\n",
    "    idx_sf.extend(idx_4)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        elif trY[iQ] == 2:\n",
    "            idx_tmp = idx_2.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_2))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        elif trY[iQ] == 3:\n",
    "            idx_tmp = idx_3.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_3))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        elif trY[iQ] == 4:\n",
    "            idx_tmp = idx_4.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_4))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf)\n",
    "#trQ_sf, trP_sf, trN_sf = onlineGenImgPairs() #sample \n",
    "assert (trQ_sf.shape==trP_sf.shape)\n",
    "assert (trQ_sf.shape==trN_sf.shape)\n",
    "\n",
    "#define model\n",
    "model = ATHNet(code_size=36).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "criterion  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    #trQ_sf, trP_sf, trN_sf = onlineGenImgPairs()\n",
    "    num_batches = len(trQ_sf) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(Q_hash,P_hash,N_hash)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "criterion=criterion.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:#[5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #map_item_score = {}\n",
    "        #for j, trVal in enumerate(trF):\n",
    "        #    map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        #ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))\n",
    "    \n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels ) #labels=['AMD','DR','glaucoma','myopia','norm']\n",
    "print (cm)\n",
    "print ('Specificity of normal: %.6f'%float(cm[4][4]/np.sum(cm[4])))\n",
    "print ('Sensitivity of AMD: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of DR: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of glaucoma: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of myopia: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 900 / 900 : loss = 13.698832Eopch:     1 mean_loss = 15.440414\n",
      " 900 / 900 : loss = 16.823997Eopch:     2 mean_loss = 15.112002\n",
      " 900 / 900 : loss = 16.599051Eopch:     3 mean_loss = 14.504598\n",
      " 900 / 900 : loss = 12.710625Eopch:     4 mean_loss = 14.381597\n",
      " 900 / 900 : loss = 13.626973Eopch:     5 mean_loss = 14.130198\n",
      " 900 / 900 : loss = 17.140287Eopch:     6 mean_loss = 13.988438\n",
      " 900 / 900 : loss = 14.015347Eopch:     7 mean_loss = 13.986028\n",
      " 900 / 900 : loss = 14.932935Eopch:     8 mean_loss = 13.616890\n",
      " 900 / 900 : loss = 0.0886385Eopch:     9 mean_loss = 13.725591\n",
      " 587 / 900 : loss = 13.174774"
     ]
    }
   ],
   "source": [
    "#ATH model with Circle loss\n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True) #resnet\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ATHNet(nn.Module):\n",
    "    def __init__(self, code_size: int):\n",
    "        super().__init__()\n",
    "        #resnet and maxpool\n",
    "        self.net1 = nn.Sequential(#(3,256,256)->(16,128,128)\n",
    "            ResBlock(in_channels=3, out_channels=16, stride=2), \n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        )\n",
    "        \n",
    "        #Attention (16,128,128)->(16,128,128)\n",
    "        self.sa = SpatialAttention()\n",
    "        \n",
    "        #resnet and meanpool\n",
    "        self.net2 =nn.Sequential( #(16,128,128)->(8,64,64)\n",
    "            ResBlock(in_channels=16, out_channels=8, stride=2),\n",
    "            nn.AvgPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        ) \n",
    "         \n",
    "        #fully connected with conv (8,64,64)->(1,32,32)\n",
    "        self.dense=ResBlock(in_channels=8, out_channels=1, stride=2)\n",
    "        #fully connected (1,32,32)->class_size\n",
    "        self.linear = nn.Linear(1*32*32, code_size)\n",
    "    \n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.sa(x)*x\n",
    "        x = self.net2(x)\n",
    "        x = self.dense(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "#https://github.com/qianjinhao/circle-loss        \n",
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, scale=32, margin=0.25, similarity='cos', **kwargs):\n",
    "        super(CircleLoss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.similarity = similarity\n",
    "\n",
    "    def forward(self, feats, labels):\n",
    "        assert feats.size(0) == labels.size(0), \\\n",
    "            f\"feats.size(0): {feats.size(0)} is not equal to labels.size(0): {labels.size(0)}\"\n",
    "        batch_size = feats.size(0)\n",
    "        if self.similarity == 'dot':\n",
    "            sim_mat = torch.matmul(feats, torch.t(feats))\n",
    "        elif self.similarity == 'cos':\n",
    "            feats = F.normalize(feats)\n",
    "            sim_mat = feats.mm(feats.t())\n",
    "        else:\n",
    "            raise ValueError('This similarity is not implemented.')\n",
    "        loss = list()\n",
    "        for i in range(batch_size):\n",
    "            pos_index = labels == labels[i]\n",
    "            pos_index[i] = 0\n",
    "            neg_index = labels != labels[i]\n",
    "            pos_pair_ = sim_mat[i][pos_index]\n",
    "            neg_pair_ = sim_mat[i][neg_index]\n",
    "\n",
    "            alpha_p = torch.relu(-pos_pair_ + 1 + self.margin)\n",
    "            alpha_n = torch.relu(neg_pair_ + self.margin)\n",
    "            margin_p = 1 - self.margin\n",
    "            margin_n = self.margin\n",
    "            loss_p = torch.sum(torch.exp(-self.scale * alpha_p * (pos_pair_ - margin_p)))\n",
    "            loss_n = torch.sum(torch.exp(self.scale * alpha_n * (neg_pair_ - margin_n)))\n",
    "            loss.append(torch.log(1 + loss_p * loss_n))\n",
    "\n",
    "        loss = sum(loss) / batch_size\n",
    "        return loss\n",
    "        \n",
    "#define model\n",
    "model = ATHNet(code_size=36).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "criterion  = CircleLoss().cuda() #define TripletLoss \n",
    "#train model\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "trI = np.array(trI)\n",
    "trY = np.array(trY)\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trY)))\n",
    "    train_x = trI[shuffled_idx]\n",
    "    train_y = trY[shuffled_idx]\n",
    "    num_batches = len(trY) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY), (i+1)*batchSize])\n",
    "        x_batch = torch.from_numpy(train_x[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        y_batch = torch.from_numpy(train_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        out_batch = model(x_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        loss = criterion(out_batch,y_batch) #F.log_softmax+F.nll_loss\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "criterion=criterion.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:#[5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #map_item_score = {}\n",
    "        #for j, trVal in enumerate(trF):\n",
    "        #    map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        #ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))\n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels ) #labels=['AMD','DR','glaucoma','myopia','norm']\n",
    "print (cm)\n",
    "print ('Specificity of normal: %.6f'%float(cm[4][4]/np.sum(cm[4])))\n",
    "print ('Sensitivity of AMD: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of DR: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of glaucoma: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of myopia: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 450 / 450 : loss = 1.801376Eopch:     1 mean_loss = 3.452860\n",
      " 450 / 450 : loss = 5.400209Eopch:     2 mean_loss = 3.384453\n",
      " 450 / 450 : loss = 3.600199Eopch:     3 mean_loss = 3.404326\n",
      " 450 / 450 : loss = 0.900284Eopch:     4 mean_loss = 3.480284\n",
      " 450 / 450 : loss = 3.600359Eopch:     5 mean_loss = 3.446244\n",
      " 450 / 450 : loss = 4.500153Eopch:     6 mean_loss = 3.446244\n",
      " 450 / 450 : loss = 3.600244Eopch:     7 mean_loss = 3.416210\n",
      " 450 / 450 : loss = 2.700194Eopch:     8 mean_loss = 3.398425\n",
      " 450 / 450 : loss = 0.000248Eopch:     9 mean_loss = 3.432221\n",
      " 450 / 450 : loss = 0.900221Eopch:    10 mean_loss = 3.424178\n",
      "best_loss = 3.384453\n",
      " 99 / 100  Completed buliding index in 1 seconds\n",
      "mHR@10=0.639600, mAP@10=0.540474, mRR@10=0.821242\n",
      "Accuracy: 0.651000\n",
      "[[  8   2   2   7  53]\n",
      " [  4   1   5   3  14]\n",
      " [  2   3   6   4  30]\n",
      " [  7   3   2  12  55]\n",
      " [ 59  23  33  38 624]]\n",
      "Specificity of normal: 0.803089\n",
      "Sensitivity of AMD: 0.111111\n",
      "Sensitivity of DR: 0.037037\n",
      "Sensitivity of glaucoma: 0.133333\n",
      "Sensitivity of myopia: 0.151899\n"
     ]
    }
   ],
   "source": [
    "#ATH model with pairwise loss\n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True) #resnet\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ATHNet(nn.Module):\n",
    "    def __init__(self, code_size: int):\n",
    "        super().__init__()\n",
    "        #resnet and maxpool\n",
    "        self.net1 = nn.Sequential(#(3,256,256)->(16,128,128)\n",
    "            ResBlock(in_channels=3, out_channels=16, stride=2), \n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        )\n",
    "        \n",
    "        #Attention (16,128,128)->(16,128,128)\n",
    "        self.sa = SpatialAttention()\n",
    "        \n",
    "        #resnet and meanpool\n",
    "        self.net2 =nn.Sequential( #(16,128,128)->(8,64,64)\n",
    "            ResBlock(in_channels=16, out_channels=8, stride=2),\n",
    "            nn.AvgPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        ) \n",
    "         \n",
    "        #fully connected with conv (8,64,64)->(1,32,32)\n",
    "        self.dense=ResBlock(in_channels=8, out_channels=1, stride=2)\n",
    "        #fully connected (1,32,32)->class_size\n",
    "        self.linear = nn.Linear(1*32*32, code_size)\n",
    "    \n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.sa(x)*x\n",
    "        x = self.net2(x)\n",
    "        x = self.dense(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "#define loss function:pairwise loss            \n",
    "class PairwiseLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(PairwiseLoss, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.l1_loss = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    def forward(self,h1,h2,y):    \n",
    "        margin_val = self.margin * h1.shape[1]\n",
    "        squared_loss = torch.mean(self.mse_loss(h1, h2), dim=1)\n",
    "        # T1: 0.5 * (1 - y) * dist(x1, x2)\n",
    "        positive_pair_loss = (0.5 * (1 - y) * squared_loss)\n",
    "        mean_positive_pair_loss = torch.mean(positive_pair_loss)\n",
    "        # T2: 0.5 * y * max(margin - dist(x1, x2), 0)\n",
    "        zeros = torch.zeros_like(squared_loss)\n",
    "        marginMat = margin_val * torch.ones_like(squared_loss)\n",
    "        negative_pair_loss = 0.5 * y * torch.max(zeros, marginMat - squared_loss)\n",
    "        mean_negative_pair_loss = torch.mean(negative_pair_loss)\n",
    "\n",
    "        # T3: alpha(dst_l1(abs(x1), 1)) + dist_l1(abs(x2), 1)))\n",
    "        mean_value_regularization = self.alpha * (\n",
    "                self.l1_loss(torch.abs(h1), torch.ones_like(h1)) +\n",
    "                self.l1_loss(torch.abs(h2), torch.ones_like(h2)))\n",
    "\n",
    "        loss = mean_positive_pair_loss + mean_negative_pair_loss + mean_value_regularization\n",
    "        return loss\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs():\n",
    "    if (len(trY) % 2) == 0: spls = len(trY)\n",
    "    else:  spls = len(trY)-1\n",
    "    idx_sf = random.sample(range(0, spls),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "\n",
    "#define model\n",
    "model = ATHNet(code_size=36).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "criterion  = PairwiseLoss(margin=0.5).cuda() #define PairwiseLoss \n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs()\n",
    "    losses = []\n",
    "    num_batches = len(trY_sf) // batchSize \n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "criterion=criterion.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:#[5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #map_item_score = {}\n",
    "        #for j, trVal in enumerate(trF):\n",
    "        #    map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        #ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))\n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels ) #labels=['AMD','DR','glaucoma','myopia','norm']\n",
    "print (cm)\n",
    "print ('Specificity of normal: %.6f'%float(cm[4][4]/np.sum(cm[4])))\n",
    "print ('Sensitivity of AMD: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of DR: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of glaucoma: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of myopia: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 900 / 900 : loss = 0.046426Eopch:     1 mean_loss = 0.076662\n",
      " 900 / 900 : loss = 0.068917Eopch:     2 mean_loss = 0.072288\n",
      " 900 / 900 : loss = 0.045904Eopch:     3 mean_loss = 0.068340\n",
      " 900 / 900 : loss = 0.087834Eopch:     4 mean_loss = 0.065638\n",
      " 900 / 900 : loss = 0.054034Eopch:     5 mean_loss = 0.063121\n",
      " 900 / 900 : loss = 0.033454Eopch:     6 mean_loss = 0.061164\n",
      " 900 / 900 : loss = 0.036283Eopch:     7 mean_loss = 0.058640\n",
      " 900 / 900 : loss = 0.047109Eopch:     8 mean_loss = 0.057238\n",
      " 900 / 900 : loss = 0.050482Eopch:     9 mean_loss = 0.056136\n",
      " 900 / 900 : loss = 0.120574Eopch:    10 mean_loss = 0.054896\n",
      " 900 / 900 : loss = 0.039096Eopch:    11 mean_loss = 0.052798\n",
      " 900 / 900 : loss = 0.091118Eopch:    12 mean_loss = 0.051601\n",
      " 900 / 900 : loss = 0.047247Eopch:    13 mean_loss = 0.050370\n",
      " 900 / 900 : loss = 0.052529Eopch:    14 mean_loss = 0.049675\n",
      " 900 / 900 : loss = 0.041666Eopch:    15 mean_loss = 0.049421\n",
      " 900 / 900 : loss = 0.070098Eopch:    16 mean_loss = 0.048403\n",
      " 900 / 900 : loss = 0.054841Eopch:    17 mean_loss = 0.048030\n",
      " 900 / 900 : loss = 0.030843Eopch:    18 mean_loss = 0.047563\n",
      " 900 / 900 : loss = 0.021637Eopch:    19 mean_loss = 0.046599\n",
      " 900 / 900 : loss = 0.018433Eopch:    20 mean_loss = 0.045455\n",
      "best_loss = 0.045455\n",
      " 99 / 100  Completed buliding index in 2 seconds\n",
      "mHR@10=0.652500, mAP@10=0.560583, mRR@10=0.838591\n",
      "Accuracy: 0.661000\n",
      "[[  1   3   0  13  55]\n",
      " [  1   4   1   4  17]\n",
      " [  0   5   9   3  28]\n",
      " [  5   2   3  27  42]\n",
      " [ 32  21  34  70 620]]\n",
      "Specificity of normal: 0.797941\n",
      "Sensitivity of AMD: 0.013889\n",
      "Sensitivity of DR: 0.148148\n",
      "Sensitivity of glaucoma: 0.200000\n",
      "Sensitivity of myopia: 0.341772\n"
     ]
    }
   ],
   "source": [
    "#ATH model with focal loss\n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True) #resnet\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ATHNet(nn.Module):\n",
    "    def __init__(self, class_size: int):\n",
    "        super().__init__()\n",
    "        #resnet and maxpool\n",
    "        self.net1 = nn.Sequential(#(3,256,256)->(16,128,128)\n",
    "            ResBlock(in_channels=3, out_channels=16, stride=2), \n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        )\n",
    "        \n",
    "        #Attention (16,128,128)->(16,128,128)\n",
    "        self.sa = SpatialAttention()\n",
    "        \n",
    "        #resnet and meanpool\n",
    "        self.net2 =nn.Sequential( #(16,128,128)->(8,64,64)\n",
    "            ResBlock(in_channels=16, out_channels=8, stride=2),\n",
    "            nn.AvgPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        ) \n",
    "         \n",
    "        #fully connected with conv (8,64,64)->(1,32,32)\n",
    "        self.dense=ResBlock(in_channels=8, out_channels=1, stride=2)\n",
    "        #fully connected (1,32,32)->class_size\n",
    "        self.linear = nn.Linear(1*32*32, class_size)\n",
    "    \n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.sa(x)*x\n",
    "        x = self.net2(x)\n",
    "        x = self.dense(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        out = self.linear(x)\n",
    "        return x,out\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "#https://github.com/marvis/pytorch-yolo2/blob/master/FocalLoss.py\n",
    "#https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py\n",
    "class FocalLoss(nn.Module):\n",
    "    #Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, out, y):\n",
    "        y = y.view(-1,1)\n",
    "        logpt = F.log_softmax(out,dim=1)#default ,dim=1\n",
    "        logpt = logpt.gather(1,y)# dim=1, index=y, max\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=out.data.type():\n",
    "                self.alpha = self.alpha.type_as(out.data)\n",
    "            at = self.alpha.gather(0,y.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()\n",
    "        \n",
    "#define model\n",
    "model = ATHNet(class_size=5).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "criterion  = FocalLoss(gamma=2,alpha=[0.2,0.4,0.2,0.15,0.05]).cuda() #define ce mutli-classes, \n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "trI = np.array(trI)\n",
    "trY = np.array(trY)\n",
    "for epoch in range(20):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trY)))\n",
    "    train_x = trI[shuffled_idx]\n",
    "    train_y = trY[shuffled_idx]\n",
    "    num_batches = len(trY) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY), (i+1)*batchSize])\n",
    "        x_batch = torch.from_numpy(train_x[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        y_batch = torch.from_numpy(train_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        _, out_batch = model(x_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        loss = criterion(out_batch,y_batch) #F.log_softmax+F.nll_loss\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "criterion = criterion.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    feature, _ = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    feature = feature.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(feature.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "teY_pred = []\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    feature, out_batch = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(feature.cpu().data.numpy().tolist()) #record feature\n",
    "    out_batch = F.log_softmax(out_batch,dim=1) \n",
    "    pred = out_batch.max(1,keepdim=True)[1]\n",
    "    teY_pred.extend(pred.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(32*32) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:#[5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #map_item_score = {}\n",
    "        #for j, trVal in enumerate(trF):\n",
    "        #    map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        #ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))\n",
    "\n",
    "#confusion matrix\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, teY_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, teY_pred, labels=labels ) #labels=['AMD','DR','glaucoma','myopia','norm']\n",
    "print (cm)\n",
    "print ('Specificity of normal: %.6f'%float(cm[4][4]/np.sum(cm[4])))\n",
    "print ('Sensitivity of AMD: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of DR: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of glaucoma: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of myopia: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 900 / 900 : loss = 0.761225Eopch:     1 mean_loss = 0.818043\n",
      " 900 / 900 : loss = 0.720572Eopch:     2 mean_loss = 0.768105\n",
      " 900 / 900 : loss = 0.683275Eopch:     3 mean_loss = 0.755878\n",
      " 900 / 900 : loss = 1.594184Eopch:     4 mean_loss = 0.743103\n",
      " 900 / 900 : loss = 0.719763Eopch:     5 mean_loss = 0.736230\n",
      " 900 / 900 : loss = 0.814738Eopch:     6 mean_loss = 0.728397\n",
      " 900 / 900 : loss = 0.418521Eopch:     7 mean_loss = 0.711555\n",
      " 900 / 900 : loss = 0.606084Eopch:     8 mean_loss = 0.700541\n",
      " 900 / 900 : loss = 0.503086Eopch:     9 mean_loss = 0.696415\n",
      " 900 / 900 : loss = 0.504301Eopch:    10 mean_loss = 0.680204\n",
      "best_loss = 0.680204\n",
      " 99 / 100  Completed buliding index in 2 seconds\n",
      "mHR@10=0.653500, mAP@10=0.558163, mRR@10=0.832206\n",
      "Accuracy: 0.778000\n",
      "[[  0   0   0   5  67]\n",
      " [  0   0   0   3  24]\n",
      " [  0   0   0   4  41]\n",
      " [  0   0   0  25  54]\n",
      " [  0   0   0  24 753]]\n",
      "Specificity of normal: 0.969112\n",
      "Sensitivity of AMD: 0.000000\n",
      "Sensitivity of DR: 0.000000\n",
      "Sensitivity of glaucoma: 0.000000\n",
      "Sensitivity of myopia: 0.316456\n"
     ]
    }
   ],
   "source": [
    "#ATH model with cross entropy loss\n",
    "class SpatialAttention(nn.Module):#spatial attention layer\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True) #resnet\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ATHNet(nn.Module):\n",
    "    def __init__(self, class_size: int):\n",
    "        super().__init__()\n",
    "        #resnet and maxpool\n",
    "        self.net1 = nn.Sequential(#(3,256,256)->(16,128,128)\n",
    "            ResBlock(in_channels=3, out_channels=16, stride=2), \n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        )\n",
    "        \n",
    "        #Attention (16,128,128)->(16,128,128)\n",
    "        self.sa = SpatialAttention()\n",
    "        \n",
    "        #resnet and meanpool\n",
    "        self.net2 =nn.Sequential( #(16,128,128)->(8,64,64)\n",
    "            ResBlock(in_channels=16, out_channels=8, stride=2),\n",
    "            nn.AvgPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        ) \n",
    "         \n",
    "        #fully connected with conv (8,64,64)->(1,32,32)\n",
    "        self.dense=ResBlock(in_channels=8, out_channels=1, stride=2)\n",
    "        #fully connected (1,32,32)->class_size\n",
    "        self.linear = nn.Linear(1*32*32, class_size)\n",
    "    \n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.sa(x)*x\n",
    "        x = self.net2(x)\n",
    "        x = self.dense(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        out = self.linear(x)\n",
    "        return x,out\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "#define model\n",
    "model = ATHNet(class_size=5).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "criterion  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "trI = np.array(trI)\n",
    "trY = np.array(trY)\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trY)))\n",
    "    train_x = trI[shuffled_idx]\n",
    "    train_y = trY[shuffled_idx]\n",
    "    num_batches = len(trY) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY), (i+1)*batchSize])\n",
    "        x_batch = torch.from_numpy(train_x[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        y_batch = torch.from_numpy(train_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        _,out_batch = model(x_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        loss = criterion(out_batch,y_batch) #F.log_softmax+F.nll_loss\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "criterion = criterion.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    feature, _ = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    feature = feature.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(feature.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "teY_pred = []\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    feature, out_batch = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(feature.cpu().data.numpy().tolist()) #record feature\n",
    "    out_batch = F.log_softmax(out_batch,dim=1) \n",
    "    pred = out_batch.max(1,keepdim=True)[1]\n",
    "    teY_pred.extend(pred.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(32*32) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:#[5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #map_item_score = {}\n",
    "        #for j, trVal in enumerate(trF):\n",
    "        #    map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        #ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))\n",
    "#confusion matrix\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, teY_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, teY_pred, labels=labels ) #labels=['AMD','DR','glaucoma','myopia','norm']\n",
    "print (cm)\n",
    "print ('Specificity of normal: %.6f'%float(cm[4][4]/np.sum(cm[4])))\n",
    "print ('Sensitivity of AMD: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of DR: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of glaucoma: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of myopia: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Org data dimension is 1024.Embedded data dimension is 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize : t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import random\n",
    "def scatter(X, y):\n",
    "    #X,y:numpy-array\n",
    "    classes = len(list(set(y.tolist())))#get number of classes\n",
    "    #palette = np.array(sns.color_palette(\"hls\", classes))# choose a color palette with seaborn.\n",
    "    color = ['c','y','m','b','g','r']\n",
    "    marker = ['o','x','s','*','+']\n",
    "    label = ['AMD','DR','glaucoma','myopia','normal']\n",
    "    plt.figure(figsize=(8,8))#create a plot\n",
    "    for i in range(classes):\n",
    "        plt.scatter(X[y == i,0], X[y == i,1], c=color[i], marker=marker[i], label=label[i])\n",
    "    plt.axis('off')\n",
    "    plt.legend(loc='lower left')\n",
    "    #plt.savefig('digits_tsne-generated.png', dpi=100)\n",
    "    plt.show()\n",
    "\n",
    "#prepare dataclasses=5\n",
    "#idx= random.sample(np.where(np.array(teY)==0)[0].tolist(),100)\n",
    "idx= np.where(np.array(teY)==0)[0].tolist()\n",
    "X0= np.array(teF)[idx]\n",
    "y0= np.array(teY)[idx]\n",
    "\n",
    "idx= np.where(np.array(teY)==1)[0].tolist()\n",
    "X1= np.array(teF)[idx]\n",
    "y1= np.array(teY)[idx]\n",
    "\n",
    "idx= np.where(np.array(teY)==2)[0].tolist()\n",
    "X2= np.array(teF)[idx]\n",
    "y2= np.array(teY)[idx]\n",
    "\n",
    "idx= np.where(np.array(teY)==3)[0].tolist()\n",
    "X3= np.array(teF)[idx]\n",
    "y3= np.array(teY)[idx]\n",
    "\n",
    "idx= np.where(np.array(teY)==4)[0].tolist()\n",
    "X4= np.array(teF)[idx]\n",
    "y4= np.array(teY)[idx]\n",
    "\n",
    "y = np.append(y0,y1)\n",
    "y = np.append(y,y2)\n",
    "y = np.append(y,y3)\n",
    "y = np.append(y,y4)\n",
    "X = np.vstack((X0,X1))\n",
    "X = np.vstack((X,X2))\n",
    "X = np.vstack((X,X3))\n",
    "X = np.vstack((X,X4))\n",
    "#training t-sne \n",
    "tsne = TSNE(n_components=2, init='pca', random_state=501)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "print(\"Org data dimension is {}.Embedded data dimension is {}\".format(X.shape[-1], X_tsne.shape[-1]))\n",
    "\n",
    "#visualize\n",
    "scatter(X_tsne, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.374 -> glaucoma\n",
      "0.223 -> norm\n",
      "0.191 -> AMD\n",
      "0.113 -> DR\n",
      "0.099 -> myopia\n",
      "output CAM.jpg for the top1 prediction: glaucoma\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate class activation mapping for the top1 prediction\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    \n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        #cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc,h*w)))\n",
    "        cam = weight_softmax[class_idx]*(feature_conv.reshape((nc,h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "#last conv layer followed with one channel by last fully connected layer\n",
    "final_conv = 'dense' \n",
    "best_net._modules.get(final_conv).register_forward_hook(hook_feature)\n",
    "#get weights parameters\n",
    "params = list(best_net.parameters())\n",
    "#get the last and second last weights, like [classes, hiden nodes]\n",
    "weight_softmax = np.squeeze(params[-2].data.cpu().numpy()) \n",
    "# define class type\n",
    "classes = {0: 'AMD', 1: 'DR', 2:'glaucoma', 3:'myopia', 4:'norm'}\n",
    "#read image\n",
    "root='/data/fjsdata/fundus/iSee/iSee_multi_dataset/img_data_DR/913024.jpg'\n",
    "img = []\n",
    "img.append( cv2.resize(cv2.imread(root).astype(np.float32), (256, 256)))#(256, 256) is the model input size\n",
    "data = torch.from_numpy(np.array(img)).type(torch.FloatTensor).cuda()\n",
    "_,logit = best_net(data.permute(0, 3, 1, 2))#forword\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()#softmax\n",
    "probs, idx = h_x.sort(0, True) #probabilities of classes\n",
    "\n",
    "# output: the prediction\n",
    "for i in range(0, len(classes)):\n",
    "    line = '{:.3f} -> {}'.format(probs[i], classes[idx[i].item()])\n",
    "    print(line)\n",
    "#get the class activation maps\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0].item()])\n",
    "\n",
    "# render the CAM and show\n",
    "print('output CAM.jpg for the top1 prediction: %s' % classes[idx[0].item()])\n",
    "img = cv2.imread(root)\n",
    "height, width, _ = img.shape\n",
    "CAM = cv2.resize(CAMs[0], (width, height))\n",
    "heatmap = cv2.applyColorMap(CAM, cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "#result = heatmap \n",
    "cv2.imwrite('iSee_cam.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 450 / 450 : loss = 6.300294Eopch:     1 mean_loss = 4.443754\n",
      " 450 / 450 : loss = 1.800274Eopch:     2 mean_loss = 3.434417\n",
      " 450 / 450 : loss = 6.300045Eopch:     3 mean_loss = 3.418108\n",
      " 450 / 450 : loss = 5.400019Eopch:     4 mean_loss = 3.406024\n",
      " 450 / 450 : loss = 4.500027Eopch:     5 mean_loss = 3.422017\n",
      " 450 / 450 : loss = 0.900013Eopch:     6 mean_loss = 3.412016\n",
      " 450 / 450 : loss = 2.700021Eopch:     7 mean_loss = 3.434016\n",
      " 450 / 450 : loss = 3.600017Eopch:     8 mean_loss = 3.468018\n",
      " 450 / 450 : loss = 2.700022Eopch:     9 mean_loss = 3.458020\n",
      " 450 / 450 : loss = 1.800023Eopch:    10 mean_loss = 3.470020\n",
      "best_loss = 3.406024\n",
      " 99 / 100  Completed buliding index in 1 seconds\n",
      "mHR@10=0.606300, mAP@10=0.488250, mRR@10=0.777767\n",
      "Accuracy: 0.777000\n",
      "[[  0   0   0   3  69]\n",
      " [  0   0   0   1  26]\n",
      " [  0   0   0   1  44]\n",
      " [  0   0   0  12  67]\n",
      " [  0   0   0  12 765]]\n",
      "Specificity of normal: 0.984556\n",
      "Sensitivity of AMD: 0.000000\n",
      "Sensitivity of DR: 0.000000\n",
      "Sensitivity of glaucoma: 0.000000\n",
      "Sensitivity of myopia: 0.151899\n"
     ]
    }
   ],
   "source": [
    "#Baseline: DPSH-CVPR2015Feature Learning based Deep Supervised Hashing with Pairwise Labels\n",
    "#https://github.com/TreezzZ/DPSH_PyTorch/blob/master/models/alexnet.py\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, code_length):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            #nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1024)\n",
    "        )\n",
    "        #self.classifier = self.classifier[:-1]\n",
    "        self.hash_layer = nn.Linear(1024, code_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.hash_layer(x)\n",
    "        return x\n",
    "    \n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.l1_loss = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    def forward(self,h1,h2,y):    \n",
    "        margin_val = self.margin * h1.shape[1]\n",
    "        squared_loss = torch.mean(self.mse_loss(h1, h2), dim=1)\n",
    "        # T1: 0.5 * (1 - y) * dist(x1, x2)\n",
    "        positive_pair_loss = (0.5 * (1 - y) * squared_loss)\n",
    "        mean_positive_pair_loss = torch.mean(positive_pair_loss)\n",
    "        # T2: 0.5 * y * max(margin - dist(x1, x2), 0)\n",
    "        zeros = torch.zeros_like(squared_loss)\n",
    "        marginMat = margin_val * torch.ones_like(squared_loss)\n",
    "        negative_pair_loss = 0.5 * y * torch.max(zeros, marginMat - squared_loss)\n",
    "        mean_negative_pair_loss = torch.mean(negative_pair_loss)\n",
    "\n",
    "        # T3: alpha(dst_l1(abs(x1), 1)) + dist_l1(abs(x2), 1)))\n",
    "        mean_value_regularization = self.alpha * (\n",
    "                self.l1_loss(torch.abs(h1), torch.ones_like(h1)) +\n",
    "                self.l1_loss(torch.abs(h2), torch.ones_like(h2)))\n",
    "\n",
    "        loss = mean_positive_pair_loss + mean_negative_pair_loss + mean_value_regularization\n",
    "        return loss\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs():\n",
    "    if (len(trY) % 2) == 0: spls = len(trY)\n",
    "    else:  spls = len(trY)-1\n",
    "    idx_sf = random.sample(range(0, spls),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "#define model\n",
    "model = AlexNet(code_length=36).cuda()\n",
    "criterion  = HashLossFunc(margin=0.5).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "num_batches = len(trY_sf) // batchSize \n",
    "for epoch in range(10):#iteration\n",
    "    trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs()\n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "loss = loss.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:#[5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #map_item_score = {}\n",
    "        #for j, trVal in enumerate(trF):\n",
    "        #    map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        #ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))\n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels ) #labels=['AMD','DR','glaucoma','myopia','norm']\n",
    "print (cm)\n",
    "print ('Specificity of normal: %.6f'%float(cm[4][4]/np.sum(cm[4])))\n",
    "print ('Sensitivity of AMD: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of DR: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of glaucoma: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of myopia: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 450 / 450 : loss = 3.669319Eopch:     1 mean_loss = 4.754357\n",
      " 450 / 450 : loss = 2.8310886Eopch:     2 mean_loss = 3.578732\n",
      " 450 / 450 : loss = 3.642664Eopch:     3 mean_loss = 3.476371\n",
      " 450 / 450 : loss = 5.386829Eopch:     4 mean_loss = 3.390807\n",
      " 450 / 450 : loss = 3.381272Eopch:     5 mean_loss = 3.366870\n",
      " 450 / 450 : loss = 3.931137Eopch:     6 mean_loss = 3.286973\n",
      " 450 / 450 : loss = 1.379062Eopch:     7 mean_loss = 3.160406\n",
      " 450 / 450 : loss = 1.161796Eopch:     8 mean_loss = 3.197507\n",
      " 450 / 450 : loss = 2.342057Eopch:     9 mean_loss = 3.067730\n",
      " 450 / 450 : loss = 0.9871089Eopch:    10 mean_loss = 3.089459\n",
      "best_loss = 3.067730\n",
      " 99 / 100  Completed buliding index in 1 seconds\n",
      "mHR@10=0.676100, mAP@10=0.588996, mRR@10=0.855513\n",
      "Accuracy: 0.777000\n",
      "[[  0   0   0   3  69]\n",
      " [  0   0   0   1  26]\n",
      " [  0   0   0   1  44]\n",
      " [  0   0   0  12  67]\n",
      " [  0   0   0  12 765]]\n",
      "Specificity of normal: 0.984556\n",
      "Sensitivity of AMD: 0.000000\n",
      "Sensitivity of DR: 0.000000\n",
      "Sensitivity of glaucoma: 0.000000\n",
      "Sensitivity of myopia: 0.151899\n"
     ]
    }
   ],
   "source": [
    "#Baseline: DRH-MICCAI2017Hashing with Residual Networks for Image Retrieval\n",
    "#https://github.com/dansuh17/deep-supervised-hashing\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            ResBlock(in_channels=3, out_channels=16),\n",
    "            ResBlock(in_channels=16, out_channels=16),\n",
    "            ResBlock(in_channels=16, out_channels=16, stride=2),\n",
    "        )\n",
    "        self.linear_input_size = 16*128*128\n",
    "        self.linear = nn.Linear(self.linear_input_size, num_classes)\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(-1, self.linear_input_size)\n",
    "        return self.linear(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class DRH(nn.Module):\n",
    "    def __init__(self, code_size: int):\n",
    "        super().__init__()\n",
    "        resnet = ResNet(num_classes=10)\n",
    "        resnet.linear = nn.Linear(in_features=resnet.linear_input_size, out_features=code_size)\n",
    "        self.net = resnet\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "    \n",
    "class HashLossFunc(nn.Module):\n",
    "    def __init__(self, margin=0.5, alpha=0.01):\n",
    "        super(HashLossFunc, self).__init__()\n",
    "        self.alpha = alpha #regularization\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.l1_loss = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    def forward(self,h1,h2,y):    \n",
    "        margin_val = self.margin * h1.shape[1]\n",
    "        squared_loss = torch.mean(self.mse_loss(h1, h2), dim=1)\n",
    "        # T1: 0.5 * (1 - y) * dist(x1, x2)\n",
    "        positive_pair_loss = (0.5 * (1 - y) * squared_loss)\n",
    "        mean_positive_pair_loss = torch.mean(positive_pair_loss)\n",
    "        # T2: 0.5 * y * max(margin - dist(x1, x2), 0)\n",
    "        zeros = torch.zeros_like(squared_loss)\n",
    "        marginMat = margin_val * torch.ones_like(squared_loss)\n",
    "        negative_pair_loss = 0.5 * y * torch.max(zeros, marginMat - squared_loss)\n",
    "        mean_negative_pair_loss = torch.mean(negative_pair_loss)\n",
    "\n",
    "        # T3: alpha(dst_l1(abs(x1), 1)) + dist_l1(abs(x2), 1)))\n",
    "        mean_value_regularization = self.alpha * (\n",
    "                self.l1_loss(torch.abs(h1), torch.ones_like(h1)) +\n",
    "                self.l1_loss(torch.abs(h2), torch.ones_like(h2)))\n",
    "\n",
    "        loss = mean_positive_pair_loss + mean_negative_pair_loss + mean_value_regularization\n",
    "        return loss\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs():\n",
    "    if (len(trY) % 2) == 0: spls = len(trY)\n",
    "    else:  spls = len(trY)-1\n",
    "    idx_sf = random.sample(range(0, spls),spls)\n",
    "    trI1_sf, trI2_sf, trY1_sf, trY2_sf = [],[],[],[]\n",
    "    flag = 0\n",
    "    for i in idx_sf:\n",
    "        if flag==0:\n",
    "            trI1_sf.append(trI[i])\n",
    "            trY1_sf.append(trY[i])\n",
    "            flag =1\n",
    "        else:\n",
    "            trI2_sf.append(trI[i])\n",
    "            trY2_sf.append(trY[i])\n",
    "            flag =0\n",
    "    trY_sf = np.where((np.array(trY1_sf)-np.array(trY2_sf))!=0,1,0)\n",
    "    return np.array(trI1_sf),np.array(trI2_sf),trY_sf\n",
    "#define model\n",
    "model = DRH(code_size=36).cuda()\n",
    "criterion  = HashLossFunc(margin=0.5).cuda() #define loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "num_batches = len(trY_sf) // batchSize \n",
    "for epoch in range(10):#iteration\n",
    "    trI1_sf, trI2_sf, trY_sf = onlineGenImgPairs()\n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY_sf), (i+1)*batchSize])\n",
    "        I1_batch = torch.from_numpy(trI1_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        I2_batch = torch.from_numpy(trI2_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Y_batch = torch.from_numpy(trY_sf[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        X1_batch = model(I1_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        X2_batch = model(I2_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(X1_batch,X2_batch,Y_batch)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "loss = loss.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:#[5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #map_item_score = {}\n",
    "        #for j, trVal in enumerate(trF):\n",
    "        #    map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        #ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))\n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels ) #labels=['AMD','DR','glaucoma','myopia','norm']\n",
    "print (cm)\n",
    "print ('Specificity of normal: %.6f'%float(cm[4][4]/np.sum(cm[4])))\n",
    "print ('Sensitivity of AMD: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of DR: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of glaucoma: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of myopia: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 900 / 900 : loss = 18.00000284Eopch:     1 mean_loss = 21.662424\n",
      " 900 / 900 : loss = 18.000002Eopch:     2 mean_loss = 18.000044\n",
      " 900 / 900 : loss = 18.000004Eopch:     3 mean_loss = 18.000000\n",
      " 900 / 900 : loss = 17.999998Eopch:     4 mean_loss = 18.000000\n",
      " 900 / 900 : loss = 18.000002Eopch:     5 mean_loss = 18.000000\n",
      " 900 / 900 : loss = 17.999994Eopch:     6 mean_loss = 18.000000\n",
      " 900 / 900 : loss = 17.999947Eopch:     7 mean_loss = 17.999998\n",
      " 900 / 900 : loss = 18.002079641Eopch:     8 mean_loss = 34.327060\n",
      " 900 / 900 : loss = 18.099998Eopch:     9 mean_loss = 18.000002\n",
      " 900 / 900 : loss = 18.000002Eopch:    10 mean_loss = 18.000000\n",
      "best_loss = 17.999998\n",
      " 99 / 100  Completed buliding index in 1 seconds\n",
      "mHR@10=0.614600, mAP@10=0.509173, mRR@10=0.808769\n",
      "Accuracy: 0.545000\n",
      "[[  8   1   5   6  52]\n",
      " [  2   0   2   5  18]\n",
      " [  8   3   3   1  30]\n",
      " [  7   2   7  10  53]\n",
      " [ 93  31  48  81 524]]\n",
      "Specificity of normal: 0.674389\n",
      "Sensitivity of AMD: 0.111111\n",
      "Sensitivity of DR: 0.000000\n",
      "Sensitivity of glaucoma: 0.066667\n",
      "Sensitivity of myopia: 0.126582\n"
     ]
    }
   ],
   "source": [
    "#Baseline: DSH:ACCV2016deep supervised hashing with triplet labels\n",
    "#https://github.com/weixu000/DSH-pytorch/blob/master/model.py\n",
    "class DSH(nn.Module):\n",
    "    def __init__(self, num_binary):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, padding=2),  # same padding\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*31*31, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, num_binary)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if m.__class__ == nn.Conv2d or m.__class__ == nn.Linear:\n",
    "                nn.init.xavier_normal(m.weight.data)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#https://github.com/luyajie/triplet-deep-hash-pytorch#triplet-deep-hash-pytorch            \n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self,H_q,H_p,H_n):    \n",
    "        margin_val = self.margin * H_q.shape[1]\n",
    "        squared_loss_pos = torch.mean(self.mse_loss(H_q, H_p), dim=1)\n",
    "        squared_loss_neg = torch.mean(self.mse_loss(H_q, H_n), dim=1)\n",
    "        zeros = torch.zeros_like(squared_loss_neg)\n",
    "        loss  = torch.max(zeros, margin_val - squared_loss_neg + squared_loss_pos)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    idx_2 = np.where( np.array(trY) == 2 ) #class 2\n",
    "    idx_2 = list(idx_2[0])\n",
    "    idx_sf.extend(idx_2)\n",
    "    idx_3 = np.where( np.array(trY) == 3 ) #class 3\n",
    "    idx_3 = list(idx_3[0])\n",
    "    idx_sf.extend(idx_3)\n",
    "    idx_4 = np.where( np.array(trY) == 4 ) #class 4\n",
    "    idx_4 = list(idx_4[0])#[0:993]\n",
    "    idx_sf.extend(idx_4)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        elif trY[iQ] == 2:\n",
    "            idx_tmp = idx_2.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_2))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        elif trY[iQ] == 3:\n",
    "            idx_tmp = idx_3.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_3))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        elif trY[iQ] == 4:\n",
    "            idx_tmp = idx_4.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_4))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf)\n",
    "#trQ_sf, trP_sf, trN_sf = onlineGenImgPairs() #sample \n",
    "assert (trQ_sf.shape==trP_sf.shape)\n",
    "assert (trQ_sf.shape==trN_sf.shape)\n",
    "\n",
    "#define model\n",
    "model = DSH(num_binary=36).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "criterion  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    #trQ_sf, trP_sf, trN_sf = onlineGenImgPairs()\n",
    "    num_batches = len(trQ_sf) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss = criterion(Q_hash,P_hash,N_hash)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "criterion=criterion.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch = best_net(I_batch.permute(0, 3, 1, 2))#forword\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:#[5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #map_item_score = {}\n",
    "        #for j, trVal in enumerate(trF):\n",
    "        #    map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        #ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))\n",
    "    \n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels ) #labels=['AMD','DR','glaucoma','myopia','norm']\n",
    "print (cm)\n",
    "print ('Specificity of normal: %.6f'%float(cm[4][4]/np.sum(cm[4])))\n",
    "print ('Sensitivity of AMD: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of DR: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of glaucoma: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of myopia: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 900 / 900 : loss = 12.206721Eopch:     1 mean_loss = 18.582041\n",
      " 900 / 900 : loss = 24.562536Eopch:     2 mean_loss = 15.986259\n",
      " 900 / 900 : loss = 0.58120165Eopch:     3 mean_loss = 16.924295\n",
      " 900 / 900 : loss = 14.643861Eopch:     4 mean_loss = 13.197438\n",
      " 900 / 900 : loss = 13.094934Eopch:     5 mean_loss = 11.593795\n",
      " 900 / 900 : loss = 2.9740979Eopch:     6 mean_loss = 9.200633\n",
      " 900 / 900 : loss = 29.101393Eopch:     7 mean_loss = 6.434906\n",
      " 900 / 900 : loss = 5.4416022Eopch:     8 mean_loss = 4.138225\n",
      " 900 / 900 : loss = 1.3032885Eopch:     9 mean_loss = 2.608728\n",
      " 900 / 900 : loss = 0.0539192Eopch:    10 mean_loss = 1.542839\n",
      "best_loss = 1.542839\n",
      " 99 / 100  Completed buliding index in 25 seconds\n",
      "mHR@10=0.748200, mAP@10=0.712039, mRR@10=0.925648\n",
      "Accuracy: 0.765000\n",
      "[[  3   1   4   4  60]\n",
      " [  3   2   0   1  21]\n",
      " [  2   2   2   2  37]\n",
      " [  7   1   3  17  51]\n",
      " [ 20   4   7   5 741]]\n",
      "Specificity of normal: 0.953668\n",
      "Sensitivity of AMD: 0.041667\n",
      "Sensitivity of DR: 0.074074\n",
      "Sensitivity of glaucoma: 0.044444\n",
      "Sensitivity of myopia: 0.215190\n"
     ]
    }
   ],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.downsample_layer = None\n",
    "        self.do_downsample = False\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.do_downsample = True\n",
    "            self.downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.net(x)\n",
    "\n",
    "        if self.do_downsample:\n",
    "            identity = self.downsample_layer(x)\n",
    "\n",
    "        return F.relu(out + identity, inplace=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            ResBlock(in_channels=3, out_channels=16),\n",
    "            ResBlock(in_channels=16, out_channels=16),\n",
    "            ResBlock(in_channels=16, out_channels=16, stride=2),\n",
    "        )\n",
    "        self.linear_input_size = 16*128*128\n",
    "        self.linear = nn.Linear(self.linear_input_size, num_classes)\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x1,x2,y):\n",
    "        x1 = self.net(x1)\n",
    "        x1 = x1.view(-1, self.linear_input_size)\n",
    "        x1 = self.linear(x1)\n",
    "        x2 = self.net(x2)\n",
    "        x2 = x2.view(-1, self.linear_input_size)\n",
    "        x2 = self.linear(x2)\n",
    "        y = self.net(y)\n",
    "        y = y.view(-1, self.linear_input_size)\n",
    "        y = self.linear(y)\n",
    "        return x1, x2, y\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "        \n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    idx_2 = np.where( np.array(trY) == 2 ) #class 2\n",
    "    idx_2 = list(idx_2[0])\n",
    "    idx_sf.extend(idx_2)\n",
    "    idx_3 = np.where( np.array(trY) == 3 ) #class 3\n",
    "    idx_3 = list(idx_3[0])\n",
    "    idx_sf.extend(idx_3)\n",
    "    idx_4 = np.where( np.array(trY) == 4 ) #class 4\n",
    "    idx_4 = list(idx_4[0])#[0:993]\n",
    "    idx_sf.extend(idx_4)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        elif trY[iQ] == 2:\n",
    "            idx_tmp = idx_2.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_2))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        elif trY[iQ] == 3:\n",
    "            idx_tmp = idx_3.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_3))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        elif trY[iQ] == 4:\n",
    "            idx_tmp = idx_4.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_4))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf)\n",
    "trQ_sf, trP_sf, trN_sf = onlineGenImgPairs() #sample \n",
    "assert (trQ_sf.shape==trP_sf.shape)\n",
    "assert (trQ_sf.shape==trN_sf.shape)\n",
    "\n",
    "#define model\n",
    "model = ResNet(num_classes=36).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "pdist = nn.PairwiseDistance(2)\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    #trQ_sf, trP_sf, trN_sf = onlineGenImgPairs()\n",
    "    num_batches = len(trQ_sf) // batchSize\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash, P_hash, N_hash = model(Q_batch.permute(0, 3, 1, 2), P_batch.permute(0, 3, 1, 2), N_batch.permute(0, 3, 1, 2))\n",
    "        #binary-like loss\n",
    "        loss1 = pdist(Q_hash, P_hash)\n",
    "        loss2 = pdist(Q_hash, N_hash)\n",
    "        l = 18 - loss2 + loss1\n",
    "        loss = torch.mean(F.relu(l))\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "#criterion=criterion.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#hash code of train data from model\n",
    "#torch.cuda.synchronize()\n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch,_,_ = best_net(I_batch.permute(0, 3, 1, 2), I_batch.permute(0, 3, 1, 2), I_batch.permute(0, 3, 1, 2))\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    trF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data from model\n",
    "#torch.cuda.synchronize()\n",
    "teF = []\n",
    "num_batches = len(teI) // batchSize \n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teI), (i+1)*batchSize])\n",
    "    I_batch = torch.from_numpy(np.array(teI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    X_batch,_,_ = best_net(I_batch.permute(0, 3, 1, 2), I_batch.permute(0, 3, 1, 2), I_batch.permute(0, 3, 1, 2))\n",
    "    I_batch = I_batch.cpu()\n",
    "    X_batch = X_batch.cpu()\n",
    "    torch.cuda.empty_cache()#release gpu memory\n",
    "    teF.extend(X_batch.data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(36) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:#[5,10,15,20]:\n",
    "    MHR = [] #mean Hit ratio \n",
    "    MAP = [] #mean average precision\n",
    "    MRR = [] #mean reciprocal rank\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #map_item_score = {}\n",
    "        #for j, trVal in enumerate(trF):\n",
    "        #    map_item_score[j] = pdist(np.vstack([teVal,trVal]),'hamming')\n",
    "        #ranklist = heapq.nsmallest(topk, map_item_score, key=map_item_score.get)\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        mrr_flag = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                MHR.append(1)\n",
    "                pos_len = pos_len +1\n",
    "                MAP.append(pos_len/rank_len) \n",
    "                if mrr_flag==0: \n",
    "                    MRR.append(pos_len/rank_len)\n",
    "                    mrr_flag =1\n",
    "            else: \n",
    "                MHR.append(0)\n",
    "                MAP.append(0)   \n",
    "    print(\"mHR@{}={:.6f}, mAP@{}={:.6f}, mRR@{}={:.6f}\".format(topk,np.mean(MHR),topk,np.mean(MAP), topk, np.mean(MRR)))\n",
    "    \n",
    "#performance\n",
    "scores, neighbors = gpu_index.search(np.ascontiguousarray(teF, dtype=np.float32), k=1) #return top1\n",
    "y_pred = []\n",
    "for i in neighbors.flatten():\n",
    "    y_pred.append(np.array(trY)[i]) #label of top1\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, y_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, y_pred, labels=labels ) #labels=['AMD','DR','glaucoma','myopia','norm']\n",
    "print (cm)\n",
    "print ('Specificity of normal: %.6f'%float(cm[4][4]/np.sum(cm[4])))\n",
    "print ('Sensitivity of AMD: %.6f'%float(cm[0][0]/np.sum(cm[0])))\n",
    "print ('Sensitivity of DR: %.6f'%float(cm[1][1]/np.sum(cm[1])))\n",
    "print ('Sensitivity of glaucoma: %.6f'%float(cm[2][2]/np.sum(cm[2])))\n",
    "print ('Sensitivity of myopia: %.6f'%float(cm[3][3]/np.sum(cm[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
