{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import itertools  \n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "from scipy.spatial.distance import pdist\n",
    "import cv2\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc \n",
    "from functools import reduce\n",
    "import wfdb#https://github.com/MIT-LCP/wfdb-python\n",
    "from wfdb import processing\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.set_device(6)\n",
    "print (torch.cuda.current_device())\n",
    "#https://github.com/acheketa/pytorch-CAM\n",
    "#https://github.com/yizt/Grad-CAM.pytorch/blob/master/main.py\n",
    "#https://github.com/jacobgil/pytorch-grad-cam/blob/master/grad-cam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868 / 868 The length of train set is 781\n",
      "The length of test set is 87\n"
     ]
    }
   ],
   "source": [
    "#Generate Dataset\n",
    "root_dir = '/data/fjsdata/qtsys/img/' #the path of images\n",
    "data = pd.read_csv('/data/fjsdata/qtsys/label.csv') \n",
    "data = data.sample(frac=1).reset_index(drop=True) #shuffle\n",
    "#Dataset\n",
    "X, Y = [],[]\n",
    "for _, row in data.iterrows():\n",
    "    try:\n",
    "        image_path = os.path.join(root_dir, row['name'])\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(1600,800,3)->(256,256,3)\n",
    "        X.append(img)\n",
    "        if row['label']=='B':\n",
    "            Y.append(0) #buy\n",
    "        else:# row['label']=='S':\n",
    "            Y.append(1) #sell\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(Y),data.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "#split trainset and testset \n",
    "trI, teI, trY, teY = train_test_split(X, Y, test_size=0.1, random_state=42) #list after return\n",
    "print('The length of train set is %d'%len(trI))\n",
    "print('The length of test set is %d'%len(teI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 79 / 79 : loss = 0.693147Eopch:     1 mean_loss = 0.685663\n",
      " 79 / 79 : loss = 0.693147Eopch:     2 mean_loss = 0.526735\n",
      " 79 / 79 : loss = 0.693147Eopch:     3 mean_loss = 0.489372\n",
      " 79 / 79 : loss = 0.709821Eopch:     4 mean_loss = 0.477252\n",
      " 79 / 79 : loss = 0.693147Eopch:     5 mean_loss = 0.476874\n",
      " 79 / 79 : loss = 0.693147Eopch:     6 mean_loss = 0.468016\n",
      " 79 / 79 : loss = 0.693147Eopch:     7 mean_loss = 0.463995\n",
      " 79 / 79 : loss = 0.693147Eopch:     8 mean_loss = 0.462438\n",
      " 79 / 79 : loss = 0.693147Eopch:     9 mean_loss = 0.463249\n",
      " 79 / 79 : loss = 0.693147Eopch:    10 mean_loss = 0.462810\n",
      "best_loss = 0.462438\n",
      " 8 / 9 Accuracy: 0.965517\n",
      "[[50  1]\n",
      " [ 2 34]]\n"
     ]
    }
   ],
   "source": [
    "#2. define CNN network with pytorch\n",
    "class CNNNet(nn.Module): \n",
    "    def __init__(self,inChannels=3,classes=2):\n",
    "        super(CNNNet, self).__init__()\n",
    "        #(channels, Height, Width)\n",
    "        #layer1: Convolution, (3,256,256)->(16,128,128)\n",
    "        self.conv1 = nn.Conv2d(in_channels=inChannels, out_channels=16, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        #layer2: max pooling,(16,128,128)->(16,128,128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        #layer3: Convolution, (16,128,128)->(16,64,64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(16)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #layer4: mean pooling, 16,64,64)->(16,64,64)\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        self.bn4 = nn.BatchNorm2d(16)\n",
    "        #layer5: Convolution, (16,64,64)->(1,32,32)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn5 = nn.BatchNorm2d(1)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        #layer6: fully connected, (1,32,32)->classes\n",
    "        self.fcl1 = nn.Linear(1*32*32,classes)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "              \n",
    "    def forward(self,x):\n",
    "        #input: (batch_size, in_channels, Height, Width)\n",
    "        #output: (batch_size, out_channels, Height, Width)\n",
    "        #layer1: convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        #layer2: max pooling\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        #layer3: Convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu2(x)\n",
    "        #layer4: mean pooling\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.bn4(x)\n",
    "        #layer5: Convolution\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu3(x)\n",
    "        #layer6: fully connected\n",
    "        x = x.view(x.size(0),-1) #transfer three dims to one dim\n",
    "        x = self.fcl1(x)\n",
    "        x = self.relu4(x)\n",
    "        return x\n",
    "\n",
    "model = CNNNet(inChannels=3,classes=2).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  #define optimizer\n",
    "criterion  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    num_batches = len(trY) // batchSize +1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trY), (i+1)*batchSize])\n",
    "        X_batch = torch.from_numpy(np.array(trI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "        y_batch = torch.from_numpy(np.array(trY[min_idx:max_idx])).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        out_batch = model(X_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        #binary-like loss\n",
    "        loss = criterion(out_batch,y_batch) #F.log_softmax+F.nll_loss\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "criterion = criterion.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "#torch.cuda.synchronize()\n",
    "teY_pred = []\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize +1\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    X_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    out_batch = best_net(X_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(out_batch.cpu().data.numpy().tolist()) #record feature\n",
    "    out_batch = F.log_softmax(out_batch,dim=1) \n",
    "    pred = out_batch.max(1,keepdim=True)[1]\n",
    "    teY_pred.extend(pred.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "#confusion matrix\n",
    "print ( 'Accuracy: %.6f'%accuracy_score(teY, teY_pred))\n",
    "labels = list(set(teY))\n",
    "cm = confusion_matrix(teY, teY_pred, labels=labels ) \n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.500 -> Neg\n",
      "0.500 -> Pos\n",
      "output CAM.jpg for the top1 prediction: Neg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate class activation mapping for the top1 prediction\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    \n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        #cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc,h*w)))\n",
    "        cam = weight_softmax[class_idx]*(feature_conv.reshape((nc,h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "#last conv layer followed with one channel by last fully connected layer\n",
    "final_conv = 'conv3' \n",
    "best_net._modules.get(final_conv).register_forward_hook(hook_feature)\n",
    "#get weights parameters\n",
    "params = list(best_net.parameters())\n",
    "#get the last and second last weights, like [classes, hiden nodes]\n",
    "weight_softmax = np.squeeze(params[-2].data.cpu().numpy()) \n",
    "# define class type\n",
    "classes = {0: 'Pos', 1: 'Neg'}\n",
    "#read image\n",
    "root='/data/fjsdata/qtsys/img/sz.002509-20200325.png'\n",
    "img = []\n",
    "img.append( cv2.resize(cv2.imread(root).astype(np.float32), (256, 256)))#(256, 256) is the model input size\n",
    "data = torch.from_numpy(np.array(img)).type(torch.FloatTensor).cuda()\n",
    "logit = best_net(data.permute(0, 3, 1, 2))#forword\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()#softmax\n",
    "probs, idx = h_x.sort(0, True) #probabilities of classes\n",
    "\n",
    "# output: the prediction\n",
    "for i in range(0, 2):\n",
    "    line = '{:.3f} -> {}'.format(probs[i], classes[idx[i].item()])\n",
    "    print(line)\n",
    "#get the class activation maps\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0].item()])\n",
    "\n",
    "# render the CAM and show\n",
    "print('output CAM.jpg for the top1 prediction: %s' % classes[idx[0].item()])\n",
    "img = cv2.imread(root)\n",
    "height, width, _ = img.shape\n",
    "CAM = cv2.resize(CAMs[0], (width, height))\n",
    "heatmap = cv2.applyColorMap(CAM, cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('cam.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_items([('conv1', Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))), ('bn1', BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('relu1', ReLU(inplace)), ('maxpool', MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)), ('bn2', BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('conv2', Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))), ('bn3', BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('relu2', ReLU(inplace)), ('avgpool1', AvgPool2d(kernel_size=3, stride=1, padding=1)), ('bn4', BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('conv3', Conv2d(16, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))), ('bn5', BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('relu3', ReLU(inplace)), ('fcl1', Linear(in_features=1024, out_features=2, bias=True)), ('relu4', ReLU(inplace))])\n"
     ]
    }
   ],
   "source": [
    "print (best_net._modules.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM(object):\n",
    "    \"\"\"\n",
    "    1: gradients update when input\n",
    "    2: backpropatation by the high scores of class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net, layer_name):\n",
    "        self.net = net\n",
    "        self.layer_name = layer_name\n",
    "        self.feature = None\n",
    "        self.gradient = None\n",
    "        self.net.eval()\n",
    "        self.handlers = []\n",
    "        self._register_hook()\n",
    "\n",
    "    def _get_features_hook(self, module, input, output):\n",
    "        self.feature = output\n",
    "        #print(\"feature shape:{}\".format(output.size()))\n",
    "\n",
    "    def _get_grads_hook(self, module, input_grad, output_grad):\n",
    "        \"\"\"\n",
    "        :param input_grad: tuple, input_grad[0]: None\n",
    "                                   input_grad[1]: weight\n",
    "                                   input_grad[2]: bias\n",
    "        :param output_grad:tuple,length = 1\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.gradient = output_grad[0]\n",
    "\n",
    "    def _register_hook(self):\n",
    "        for (name, module) in self.net.named_modules():\n",
    "            if name == self.layer_name:\n",
    "                self.handlers.append(module.register_forward_hook(self._get_features_hook))\n",
    "                self.handlers.append(module.register_backward_hook(self._get_grads_hook))\n",
    "\n",
    "    def remove_handlers(self):\n",
    "        for handle in self.handlers:\n",
    "            handle.remove()\n",
    "\n",
    "    def __call__(self, inputs, index=None):\n",
    "        \"\"\"\n",
    "        :param inputs: [1,3,H,W]\n",
    "        :param index: class id\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.net.zero_grad()\n",
    "        output = self.net(inputs)  # [1,num_classes]\n",
    "        if index is None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "        target = output[0][index]\n",
    "        target.backward()\n",
    "\n",
    "        gradient = self.gradient[0].cpu().data.numpy()  # [C,H,W]\n",
    "        weight = np.mean(gradient, axis=(1, 2))  # [C]\n",
    "\n",
    "        feature = self.feature[0].cpu().data.numpy()  # [C,H,W]\n",
    "\n",
    "        cam = feature * weight[:, np.newaxis, np.newaxis]  # [C,H,W]\n",
    "        cam = np.sum(cam, axis=0)  # [H,W]\n",
    "        cam = np.maximum(cam, 0)  # ReLU\n",
    "\n",
    "        # nomalization\n",
    "        cam -= np.min(cam)\n",
    "        cam /= np.max(cam)\n",
    "        # resize to 256*256\n",
    "        cam = cv2.resize(cam, (256, 256))\n",
    "        return cam\n",
    "\n",
    "\n",
    "class GradCamPlusPlus(GradCAM):\n",
    "    def __init__(self, net, layer_name):\n",
    "        super(GradCamPlusPlus, self).__init__(net, layer_name)\n",
    "\n",
    "    def __call__(self, inputs, index=None):\n",
    "        \"\"\"\n",
    "        :param inputs: [1,3,H,W]\n",
    "        :param index: class id\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.net.zero_grad()\n",
    "        output = self.net(inputs)  # [1,num_classes]\n",
    "        if index is None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "        target = output[0][index]\n",
    "        target.backward()\n",
    "\n",
    "        gradient = self.gradient[0].cpu().data.numpy()  # [C,H,W]\n",
    "        gradient = np.maximum(gradient, 0.)  # ReLU\n",
    "        indicate = np.where(gradient > 0, 1., 0.)  # 示性函数\n",
    "        norm_factor = np.sum(gradient, axis=(1, 2))  # [C]归一化\n",
    "        for i in range(len(norm_factor)):\n",
    "            norm_factor[i] = 1. / norm_factor[i] if norm_factor[i] > 0. else 0.  # 避免除零\n",
    "        alpha = indicate * norm_factor[:, np.newaxis, np.newaxis]  # [C,H,W]\n",
    "\n",
    "        weight = np.sum(gradient * alpha, axis=(1, 2))  # [C]  alpha*ReLU(gradient)\n",
    "\n",
    "        feature = self.feature[0].cpu().data.numpy()  # [C,H,W]\n",
    "\n",
    "        cam = feature * weight[:, np.newaxis, np.newaxis]  # [C,H,W]\n",
    "        cam = np.sum(cam, axis=0)  # [H,W]\n",
    "        # cam = np.maximum(cam, 0)  # ReLU\n",
    "\n",
    "        # nomalization\n",
    "        cam -= np.min(cam)\n",
    "        cam /= np.max(cam)\n",
    "        # resize \n",
    "        cam = cv2.resize(cam, (256, 256))\n",
    "        return cam\n",
    "    \n",
    "class GuidedBackPropagation(object):\n",
    "\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "        for (name, module) in self.net.named_modules():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                module.register_backward_hook(self.backward_hook)\n",
    "                \n",
    "        self.net.eval()\n",
    "\n",
    "    @classmethod\n",
    "    def backward_hook(cls, module, grad_in, grad_out):\n",
    "        \"\"\"\n",
    "        :param module:\n",
    "        :param grad_in: tuple,length=1\n",
    "        :param grad_out: tuple,length=1\n",
    "        :return: tuple(new_grad_in,)\n",
    "        \"\"\"\n",
    "        return torch.clamp(grad_in[0], min=0.0),\n",
    "\n",
    "    def __call__(self, inputs, index=None):\n",
    "        \"\"\"\n",
    "        :param inputs: [1,3,H,W]\n",
    "        :param index: class_id\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.net.zero_grad()\n",
    "        output = self.net(inputs)  # [1,num_classes]\n",
    "        if index is None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "        target = output[0][index]\n",
    "\n",
    "        target.backward()\n",
    "\n",
    "        return inputs.grad[0]  # [3,H,W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'named_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2f7cf0fd01d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# GuidedBackPropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mgbp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGuidedBackPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-5f31cef97a86>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, net)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix)\u001b[0m\n\u001b[1;32m    980\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'named_modules'"
     ]
    }
   ],
   "source": [
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    cv2.imwrite(\"cam.jpg\", np.uint8(255 * cam))\n",
    "\n",
    "root='/data/fjsdata/qtsys/img/sz.002509-20200325.png'\n",
    "img_list = []\n",
    "img_list.append( cv2.resize(cv2.imread(root).astype(np.float32), (256, 256)))#(256, 256) is the model input size\n",
    "inputs = torch.from_numpy(np.array(img_list)).type(torch.FloatTensor).cuda()\n",
    "# Grad-CAM    \n",
    "#grad_cam = GradCAM(net=best_net, layer_name='conv3')\n",
    "#mask = grad_cam(inputs.permute(0, 3, 1, 2))  # cam mask\n",
    "#show_cam_on_image(img_list[0], mask)\n",
    "#grad_cam.remove_handlers()\n",
    "\n",
    "# Grad-CAM++\n",
    "#grad_cam_plus_plus = GradCamPlusPlus(net=best_net, layer_name='conv3')\n",
    "#mask_plus_plus = grad_cam_plus_plus(inputs.permute(0, 3, 1, 2))  # cam mask\n",
    "#show_cam_on_image(img_list[0], mask)\n",
    "#grad_cam_plus_plus.remove_handlers()\n",
    "\n",
    "# GuidedBackPropagation\n",
    "gbp = GuidedBackPropagation(best_net)\n",
    "inputs = inputs.requires_grad_(True)\n",
    "inputs.grad.zero_() \n",
    "grad = gbp(inputs.permute(0, 3, 1, 2))\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
