{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIFT+KMean+VLAD+PCA\n",
    "#https://github.com/Lithogenous/VLAD-SIFT-python/blob/master/vlad_raw.py\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import psycopg2 as ps\n",
    "from io import StringIO,BytesIO \n",
    "import pywt\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = ps.connect(host=\"127.0.0.1\", user=\"postgres\", password=\"postgres\", database=\"asoct\")#connect postgresql \n",
    "if conn is not None:\n",
    "    cur = conn.cursor()\n",
    "    #id SERIAL primary key\n",
    "    command = \"create table cataract_sift (name text NOT NULL, des float8[] NOT NULL);\"\n",
    "    cur.execute(command)\n",
    "    conn.commit()# commit the changes\n",
    "    cur.close()# close communication with the PostgreSQL database server\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40066 / 40066 "
     ]
    }
   ],
   "source": [
    "# SIFT\n",
    "def sift_extractor(file_path):\n",
    "    '''\n",
    "    Description: extract \\emph{sift} feature from given image\n",
    "    Input: file_path - image path\n",
    "    Output: des - a list of descriptors of all the keypoint from the image\n",
    "    '''\n",
    "    img = cv2.imread(file_path)\n",
    "    gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    _,des = sift.detectAndCompute(gray,None) \n",
    "\n",
    "    return des\n",
    "\n",
    "conn = ps.connect(host=\"127.0.0.1\", user=\"postgres\", password=\"postgres\", database=\"asoct\")#connect postgresql \n",
    "if conn is not None:\n",
    "    cur = conn.cursor()\n",
    "    #Extract features with SIFT\n",
    "    image_dir = '/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New' #the path of images\n",
    "    num_img = 0 #count\n",
    "    for fname in sorted(os.listdir(image_dir)):\n",
    "        num_img = num_img+1\n",
    "        sys.stdout.write('\\r{} / {} '.format(num_img,40066))\n",
    "        sys.stdout.flush()\n",
    "        if fname.endswith(\".jpg\"):\n",
    "            try:\n",
    "                des = sift_extractor(os.path.join(image_dir, fname))\n",
    "                command = \"insert into cataract_sift (name, des) values(%s, %s);\"\n",
    "                params = (fname, des.tolist())\n",
    "                cur.execute(command, params)\n",
    "                conn.commit()# commit the changes \n",
    "            except:\n",
    "                print(fname)\n",
    "                print(\"extract feature error\")              \n",
    "                continue\n",
    "    cur.close()# close communication with the PostgreSQL database server\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans\n",
    "def get_cluster_center(des_set, K):\n",
    "    '''\n",
    "    Description: cluter using a default setting\n",
    "    Input: des_set - cluster data\n",
    "                 K - the number of cluster center\n",
    "    Output: laber  - a np array of the nearest center for each cluster data\n",
    "            center - a np array of the K cluster center\n",
    "    '''\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 0.01)\n",
    "    des_set = np.float32(des_set)\n",
    "    ret, label, center = cv2.kmeans(des_set, K, None, criteria, 3, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    return label, center\n",
    "\n",
    "def get_codebook(all_des, K):\n",
    "    '''\n",
    "    Description: train the codebook from all of the descriptors\n",
    "    Input: all_des - training data for the codebook\n",
    "                 K - the column of the codebook\n",
    "    '''\n",
    "    label, center = get_cluster_center(all_des, K)\n",
    "    return label, center\n",
    "\n",
    "#query and show image\n",
    "conn = ps.connect(host=\"127.0.0.1\", user=\"postgres\", password=\"postgres\", database=\"asoct\")#connect postgresql \n",
    "if conn is not None:\n",
    "    cur = conn.cursor() \n",
    "    command = \"select name,des from cataract_sift limit 100;\"\n",
    "    cur.execute(command) \n",
    "    all_des = np.empty(shape=[0, 128])\n",
    "    image_des_len = []\n",
    "    image_name = []\n",
    "    for name, des in cur.fetchall():\n",
    "        all_des = np.concatenate([all_des, np.array(des)])\n",
    "        image_des_len.append(len(des))\n",
    "        image_name.append(name)\n",
    "    cur.close()\n",
    "conn.close()\n",
    "kplabel, codebook = get_codebook(all_des, K=32)#clusters 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vlad\n",
    "def get_vlad_base_pca(all_des, img_des_len, NNlabel, codebook):\n",
    "    cursor = 0\n",
    "    vlad_base = []\n",
    "    for eachImage in img_des_len:\n",
    "        descrips = all_des[cursor : cursor + eachImage]\n",
    "        centriods_id = NNlabel[cursor : cursor + eachImage]\n",
    "        centriods = codebook[centriods_id]\n",
    "    \n",
    "        vlad = np.zeros(shape=[COLUMNOFCODEBOOK, DESDIM])\n",
    "        for eachDes in range(eachImage):\n",
    "            vlad[centriods_id[eachDes]] = vlad[centriods_id[eachDes]] + descrips[eachDes] - centriods[eachDes]\n",
    "        cursor += eachImage\n",
    "\n",
    "        vlad_base.append(vlad.reshape(COLUMNOFCODEBOOK * DESDIM, -1))\n",
    "    \n",
    "    vlad_base_pca = np.array(vlad_base)\n",
    "    vlad_base_pca = vlad_base_pca.reshape(-1, DESDIM * COLUMNOFCODEBOOK)\n",
    "    sklearn_pca = sklearnPCA(n_components=PCAD)\n",
    "    sklearn_transf = sklearn_pca.fit_transform(vlad_base_pca)\n",
    "    sklearn_transf_norm = sklearn_transf.copy()\n",
    "    for each, each_norm in zip(sklearn_transf, sklearn_transf_norm):\n",
    "        cv2.normalize(each, each_norm, 1.0, 0.0, cv2.NORM_L2)\n",
    "    return sklearn_transf_norm, sklearn_pca\n",
    "\n",
    "def get_vlad_base(all_des, img_des_len, NNlabel,  codebook):\n",
    "    '''\n",
    "    Description: get all images vlad vector \n",
    "    '''\n",
    "    cursor = 0\n",
    "    vlad_base = []\n",
    "    for eachImage in img_des_len:\n",
    "        descrips = all_des[cursor : cursor + eachImage]\n",
    "        centriods_id = NNlabel[cursor : cursor + eachImage]\n",
    "        centriods = codebook[centriods_id]\n",
    "    \n",
    "        vlad = np.zeros(shape=[32, 128])\n",
    "        for eachDes in range(eachImage):\n",
    "            vlad[centriods_id[eachDes]] = vlad[centriods_id[eachDes]] + descrips[eachDes] - centriods[eachDes]\n",
    "        cursor += eachImage\n",
    "    \n",
    "        vlad_norm = vlad.copy()\n",
    "        cv2.normalize(vlad, vlad_norm, 1.0, 0.0, cv2.NORM_L2)\n",
    "        vlad_base.append(vlad_norm.reshape(32 * 128, -1))\n",
    "\n",
    "    return vlad_base\n",
    "\n",
    "vlad_base = get_vlad_base(all_des, image_des_len, kplabel, codebook) #vlad only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Rows = 31998, Cols = 4\n",
      "HR@5=0.148000, MAP@5=0.145400\n",
      "HR@10=0.094000, MAP@10=0.091168\n",
      "HR@15=0.150000, MAP@15=0.147191\n"
     ]
    }
   ],
   "source": [
    "#test and evaluation\n",
    "def sift_extractor(file_path):\n",
    "    '''\n",
    "    Description: extract \\emph{sift} feature from given image\n",
    "    Input: file_path - image path\n",
    "    Output: des - a list of descriptors of all the keypoint from the image\n",
    "    '''\n",
    "    img = cv2.imread(file_path)\n",
    "    gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    _,des = sift.detectAndCompute(gray,None) \n",
    "\n",
    "    return des\n",
    "\n",
    "def get_pic_vlad_pca(pic, des_size, codebook, sklearn_pca):\n",
    "    vlad = np.zeros(shape=[COLUMNOFCODEBOOK, DESDIM])\n",
    "    for eachDes in range(des_size):\n",
    "        des = pic[eachDes]\n",
    "        min_dist = 1000000000.0\n",
    "        ind = 0\n",
    "        for i in range(COLUMNOFCODEBOOK):\n",
    "            dist = cal_vec_dist(des, codebook[i])\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                ind = i\n",
    "        vlad[ind] = vlad[ind] + des - codebook[ind]\n",
    "    \n",
    "    vlad = vlad.reshape(-1, COLUMNOFCODEBOOK * DESDIM)\n",
    "    sklearn_transf = sklearn_pca.transform(vlad)\n",
    "    sklearn_transf_norm = sklearn_transf.copy()\n",
    "    cv2.normalize(sklearn_transf, sklearn_transf_norm, 1.0, 0.0, cv2.NORM_L2)\n",
    "    \n",
    "    return sklearn_transf_norm\n",
    "\n",
    "def cal_vec_dist(vec1, vec2):\n",
    "    '''\n",
    "    Description: calculate the Euclidean Distance of two vectors\n",
    "    '''\n",
    "    return np.linalg.norm(vec1 - vec2)\n",
    "\n",
    "def get_pic_vlad(pic, des_size, codebook):\n",
    "    '''\n",
    "    Description: get the vlad vector of each image\n",
    "    '''\n",
    "    vlad = np.zeros(shape=[32, 128])\n",
    "    for eachDes in range(des_size):\n",
    "        des = pic[eachDes]\n",
    "        min_dist = 1000000000.0\n",
    "        ind = 0\n",
    "        for i in range(32):\n",
    "            dist = cal_vec_dist(des, codebook[i])\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                ind = i\n",
    "        vlad[ind] = vlad[ind] + des - codebook[ind]\n",
    "    \n",
    "    vlad_norm = vlad.copy()\n",
    "    cv2.normalize(vlad, vlad_norm, 1.0, 0.0, cv2.NORM_L2)\n",
    "    vlad_norm = vlad_norm.reshape(32 * 128, -1)\n",
    "    \n",
    "    return vlad_norm\n",
    "\n",
    "image_dir = '/data/fjsdata/ASOCT/Cataract/C_8bit_Crop_New' #the path of images\n",
    "data = pd.read_csv(\"/data/fjsdata/ASOCT/Cataract/CBIR_Cataract.csv\" , sep=',')#load dataset\n",
    "print('Dataset Statistics: Rows = %d, Cols = %d' % (data.shape[0], data.shape[1]))\n",
    "name_list = data['A'].tolist()\n",
    "\n",
    "def hit(name, smname):\n",
    "    if smname not in name_list:#no label\n",
    "        return 0\n",
    "    else:\n",
    "        lv_name = data.loc[data.A==name,'D'].values[0]\n",
    "        lv_smname = data.loc[data.A==smname,'D'].values[0]\n",
    "        if ( lv_name== lv_smname): #same level,hit\n",
    "            return 1\n",
    "        return 0  \n",
    "    \n",
    "for topk in [5,10,15]:\n",
    "    HR =[] #Hit ratio \n",
    "    MAP =[] #mean average precision\n",
    "    for tname in random.sample(name_list,100):\n",
    "        tdes = sift_extractor(os.path.join(image_dir, tname))\n",
    "        tvlad = get_pic_vlad(tdes, len(tdes), codebook)\n",
    "        map_item_score = {}\n",
    "        for eachpic in range(len(image_name)):\n",
    "            dist = cal_vec_dist(tvlad, vlad_base[eachpic])\n",
    "            map_item_score[image_name[eachpic]] = dist\n",
    "        #performance    \n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        rank_map = []\n",
    "        #choose the topk nearest images of the given image \n",
    "        ranklist = heapq.nlargest(topk, map_item_score, key=map_item_score.get)\n",
    "        for smname in ranklist:\n",
    "            ret = hit(tname,smname)\n",
    "            HR.append(ret)\n",
    "            rank_len=rank_len+1\n",
    "            if ret==1: \n",
    "                pos_len = pos_len +1\n",
    "                rank_map.append(pos_len/rank_len) \n",
    "            else: \n",
    "                rank_map.append(0)\n",
    "        MAP.append(np.mean(rank_map))  \n",
    "    print(\"HR@{}={:.6f}, MAP@{}={:.6f}\".format(topk,np.mean(HR),topk,np.mean(MAP)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
