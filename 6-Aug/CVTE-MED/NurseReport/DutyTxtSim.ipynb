{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "希科智慧病房：住院护士交班系统，交班项推荐\n",
    "      —宽表设计：\n",
    "          病人基础信息(patientID, Inhostimes, sex, age, nurselevel, Diagnosis)\n",
    "          病情信息(Signsname,Signsvalue)+交班内容(dutycontent)+日期时间(datetime)\n",
    "      —需求场景：根据病人当前的病情信息推荐相似的交班内容；\n",
    "      —算法设计：文本匹配->时序建模->基于内容推荐； \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape of month 6 is:83137 rows and 11 columns\n",
      "(3451,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "#测试数据_医嘱护理测试数据\n",
    "data = pd.read_csv(\"/data/fjsdata/nursereport/patient_times.csv\", sep=',', header=None, \\\n",
    "                   names=['pId','inNum','sex','age','nrLevel','diagCon1','signName','signValue','diagCon2','dutyCon','datetime'], \\\n",
    "                   low_memory=False, encoding='GBK')\n",
    "print ('Dataset shape of month 6 is:%d rows and %d columns'%(data.shape[0],data.shape[1]))\n",
    "#dutyCon空值和缺失值处理\n",
    "data['dutyCon']=data['dutyCon'].apply(lambda x: 0 if str(x).strip()=='' else x)#空值填充\n",
    "data['dutyCon']=data['dutyCon'].fillna(0)#缺失值填充\n",
    "'''\n",
    "#signName和signVlue 拼接，diagCon1和diagCon2 拼接，signInfo 和 diagCon 拼接成病情描述\n",
    "data['signInfo']=data['signName'].str.cat(data['signValue'],sep=',')\n",
    "data['diagCon']=data['diagCon1'].str.cat(data['diagCon2'],sep=',')\n",
    "data['conCon']=data['diagCon'].str.cat(data['signInfo'],sep=',')\n",
    "#取diagInfo和dutyCon匹配\n",
    "data=data[['pId','conCon','dutyCon','datetime']]\n",
    "txtData = []\n",
    "for pid in list(set(data['pId'].tolist())):\n",
    "    df = data[data['pId']==pid]\n",
    "    #df=df.sort_values(by='datetime')\n",
    "    conCon=''\n",
    "    for index,row in df.iterrows():\n",
    "        if row['dutyCon']==0:#无交班内容\n",
    "            conCon=conCon+','+str(row['conCon']).strip()#拼接\n",
    "        else:#有交班内容\n",
    "            txtData.append(conCon+','+row['dutyCon'])\n",
    "            conCon=''\n",
    "with open('/data/comcode/SentenceSimilarity/data/med.txt', 'w', encoding='gbk') as f:\n",
    "    for item in txtData:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "'''\n",
    "data=data[data['dutyCon']!=0]\n",
    "data=data['dutyCon']\n",
    "print (data.shape)\n",
    "data.to_csv('/data/tmpexec/med.csv',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "Created on 2019年7月2日\n",
    "\n",
    "@author: cvter\n",
    "'''\n",
    "import argparse\n",
    "import jieba\n",
    "from gensim.models import doc2vec\n",
    "import csv\n",
    "\n",
    "def parse_args():#define the paramter of program\n",
    "    parser = argparse.ArgumentParser(description=\"dutyModel.\")\n",
    "    parser.add_argument('--fileName', nargs='?', default='med.csv',help='fileName.')\n",
    "    return parser.parse_args(args=[])\n",
    "\n",
    "def get_stop_words():#load the stopwords \n",
    "    spath = '../data/stopword.txt'\n",
    "    stopwords = [line.strip() for line in open(spath, 'r', encoding='GBK').readlines()]  \n",
    "    return stopwords\n",
    "\n",
    "def get_lineText(textpath): #get the data and tokenize\n",
    "    rows = csv.reader(open(textpath,'r',encoding='utf-8'))\n",
    "    lineText = []\n",
    "    rawText = []\n",
    "    stopwords = get_stop_words()\n",
    "    for r in rows:\n",
    "        rawText.append(r[0])\n",
    "        seg_list = jieba.lcut(r[0].strip()) \n",
    "        txt_list = [' '.join(seg) for seg in seg_list if seg not in stopwords]\n",
    "        lineText.append(txt_list)\n",
    "    return lineText,rawText\n",
    "\n",
    "def train_doc2vec_model(tagged_data):\n",
    "    max_epochs = 100\n",
    "    vec_size = 20\n",
    "    alpha = 0.025\n",
    "    #If dm=1 means ‘distributed memory’ (PV-DM) and dm =0 means ‘distributed bag of words’ (PV-DBOW). \n",
    "    model = doc2vec.Doc2Vec(size=vec_size,alpha=alpha, min_alpha=0.00025,min_count=1,dm =1)\n",
    "    model.build_vocab(tagged_data)\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model.train(tagged_data,total_examples=model.corpus_count,epochs=model.iter)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "    \n",
    "    model.save(\"../data/d2v.model\")\n",
    "    print(\"Doc2Vec Model Saved\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #1.读取参数\n",
    "    args = parse_args()\n",
    "    fileName = args.fileName\n",
    "    #2.加载文本，utf-8格式\n",
    "    lineText,rawText = get_lineText('../data/'+fileName)\n",
    "    print (\"The texts %d has been loaded successfully in the file %s\" % (len(lineText),fileName))\n",
    "    with open('../data/raw.txt','w') as fw:\n",
    "        lists=[line+\"\\n\" for line in rawText]\n",
    "        fw.writelines(lists)\n",
    "    #3.词典生成和训练模型\n",
    "    tagged_data = [doc2vec.TaggedDocument(words=line, tags=[str(i)]) for i, line in enumerate(lineText)]\n",
    "    train_doc2vec_model(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "Created on 2019年7月2日\n",
    "\n",
    "@author: cvter\n",
    "'''\n",
    "import argparse\n",
    "import jieba\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "def parse_args():#define the paramter of program\n",
    "    parser = argparse.ArgumentParser(description=\"dutySim.\")\n",
    "    parser.add_argument('--text', nargs='?', default='患者神志清 , 精神差 , 心电监测示:窦性心率 , 律齐。',help='text.')\n",
    "    parser.add_argument('--topK', nargs='?', default='10',help='topK.')\n",
    "    return parser.parse_args(args=[])\n",
    "\n",
    "def get_stop_words():#load the stopwords \n",
    "    spath = '../data/stopword.txt'\n",
    "    stopwords = [line.strip() for line in open(spath, 'r', encoding='GBK').readlines()]  \n",
    "    return stopwords\n",
    "\n",
    "def get_lineText(text): #get the data and tokenize\n",
    "    stopwords = get_stop_words()\n",
    "    seg_list = jieba.lcut(text.strip()) \n",
    "    lineText = [' '.join(seg) for seg in seg_list if seg not in stopwords]\n",
    "    return lineText\n",
    "\n",
    "def get_most_similar(lineText,topK):\n",
    "    model= doc2vec.Doc2Vec.load(\"../data/d2v.model\")\n",
    "    predVec = model.infer_vector(lineText)\n",
    "    '''\n",
    "    mostSim=model.docvecs.most_similar(0)\n",
    "    sims = model.docvecs.similarity(1,2)#计算两两相似度\n",
    "    docvec =model.docvecs[1]#返回对应的向量\n",
    "    Returns:    Sequence of (doctag/index, similarity).\n",
    "    Return type:    list of ({str, int}, float)\n",
    "    '''\n",
    "    mostSim=model.docvecs.most_similar([predVec], topn=int(topK))\n",
    "\n",
    "    return mostSim\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #1.读取参数\n",
    "    args = parse_args()\n",
    "    text = args.text\n",
    "    topK = args.topK\n",
    "    #2.分词\n",
    "    lineText = get_lineText(text)\n",
    "    #3.匹配相似度最高k项\n",
    "    mostSim = get_most_similar(lineText,topK)\n",
    "    rawText = [line for line in open('../data/raw.txt', 'r').readlines()]\n",
    "    simItems = []\n",
    "    for i, sim in mostSim:\n",
    "        strtxt = rawText[int(i)]\n",
    "        simItems.append([i,sim,strtxt])\n",
    "    with open('../data/sim.txt','w') as fw: #返回最高相似度结果\n",
    "        lists=[str(line)+'\\n' for line in simItems]\n",
    "        fw.writelines(lists)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
